[
  {
    "objectID": "example/session-12-example.html",
    "href": "example/session-12-example.html",
    "title": "Session 12 Code",
    "section": "",
    "text": "Code\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nlibrary(spDataLarge)\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#read-in-packages",
    "href": "example/session-12-example.html#read-in-packages",
    "title": "Session 12 Code",
    "section": "",
    "text": "Code\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nlibrary(spDataLarge)\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#changing-resolution",
    "href": "example/session-12-example.html#changing-resolution",
    "title": "Session 12 Code",
    "section": "Changing resolution:",
    "text": "Changing resolution:\n\n\nCode\nr &lt;- rast()\nr\n\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n\n\n\nCode\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\n\nCode\nra &lt;- aggregate(r, 20)\nra\n\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\n\n\n\nCode\n# rarely used\nrd &lt;- disagg(r, 20)\n\n\n\n|---------|---------|---------|---------|\n=\n                                          \n\n\nCode\nplot(rd)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#crop-and-mask",
    "href": "example/session-12-example.html#crop-and-mask",
    "title": "Session 12 Code",
    "section": "Crop and Mask",
    "text": "Crop and Mask\n\n\nCode\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n\n[1] TRUE\n\n\n\n\nCode\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")\n\n\n\n\nCode\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk2 &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=-1000)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#extend",
    "href": "example/session-12-example.html#extend",
    "title": "Session 12 Code",
    "section": "Extend",
    "text": "Extend\n\n\nCode\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\n\nCode\nplot(srtm.ext)\nplot(st_geometry(zion.buff), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#practice",
    "href": "example/session-12-example.html#practice",
    "title": "Session 12 Code",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\n\nCode\nid &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\nor &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_OR.tif\")\n\n\n\nAggregate\n\n\nCode\nid_agg &lt;- aggregate(id, fact = 30)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nor_agg &lt;- aggregate(or, fact = 30)\n\n\n\n\nCheck for alignment\n\n\nCode\ncrs(id_agg) == crs(or_agg)\n\n\n[1] TRUE\n\n\nCode\norigin(id_agg) == origin(or_agg)\n\n\n[1] FALSE FALSE\n\n\nCode\next(id_agg) == ext(or_agg)\n\n\n[1] FALSE\n\n\n\n\nAlign the origins\nBoth project and resample don’t give the intended results.\n\n\nCode\nid_proj &lt;- project(id_agg, or_agg)\n\nid_resamp &lt;- resample(id_agg, or_agg)\n\npar(mfrow = c(1,2))\nplot(id_proj)\nplot(id_resamp)\n\n\n\n\n\n\n\n\n\nWe need to extend, then resample. Resample is the faster choice because the CRS’s of the the two rasters already match.\n\n\nCode\nid_ext &lt;- extend(id_agg, or_agg)\nor_ext &lt;- extend(or_agg, id_ext)\n\nid_resamp &lt;- resample(id_ext, or_ext)\n\n\n\n\nCode\ncrs(id_resamp) == crs(or_ext)\n\n\n[1] TRUE\n\n\nCode\norigin(id_resamp) == origin(or_ext)\n\n\n[1] TRUE TRUE\n\n\nCode\next(id_resamp) == ext(or_ext)\n\n\n[1] TRUE\n\n\n\n\nMosaic\n\n\nCode\nidor &lt;- mosaic(id_resamp, or_ext)\n\n\n\n\nCrops and Masks\n\n\nCode\nidor_counties &lt;- counties(state = c(\"ID\", \"OR\"))\n\n\nRetrieving data for the year 2022\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\neast_id &lt;- idor_counties %&gt;%\n  filter(NAME %in% c(\"Teton\", \"Jefferson\", \"Madison\"))\nplot(st_geometry(east_id))\n\n\n\n\n\n\n\n\n\nmask without crop leaves a lot of white space.\n\n\nCode\neast_id_proj &lt;- st_transform(east_id, crs(idor))\n\neast_id_fire &lt;- mask(x = idor, mask = east_id_proj)\n\neast_id_fire2 &lt;- crop(idor, east_id_proj, mask=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#read-in-packages-1",
    "href": "example/session-12-example.html#read-in-packages-1",
    "title": "Session 12 Code",
    "section": "Read in packages:",
    "text": "Read in packages:\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nlibrary(tigris)\nlibrary(tidyverse)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#changing-resolution-1",
    "href": "example/session-12-example.html#changing-resolution-1",
    "title": "Session 12 Code",
    "section": "Changing resolution:",
    "text": "Changing resolution:\n\n\nCode\nr &lt;- rast()\nr\n\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n\n\n\nCode\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\n\nCode\nra &lt;- aggregate(r, 20)\nra\n\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\n\n\n\nCode\n# rarely used\nrd &lt;- disagg(r, 20)\n\n\n\n|---------|---------|---------|---------|\n=\n                                          \n\n\nCode\nplot(rd)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#crop-and-mask-1",
    "href": "example/session-12-example.html#crop-and-mask-1",
    "title": "Session 12 Code",
    "section": "Crop and Mask",
    "text": "Crop and Mask\n\n\nCode\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n\n[1] TRUE\n\n\n\n\nCode\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")\n\n\n\n\nCode\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk2 &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=-1000)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#extend-1",
    "href": "example/session-12-example.html#extend-1",
    "title": "Session 12 Code",
    "section": "Extend",
    "text": "Extend\n\n\nCode\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\n\nCode\nplot(srtm.ext)\nplot(st_geometry(zion.buff), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#practice-1",
    "href": "example/session-12-example.html#practice-1",
    "title": "Session 12 Code",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\n\nCode\nid &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\nor &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_OR.tif\")\n\n\n\nAggregate\n\n\nCode\nid_agg &lt;- aggregate(id, fact = 30)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nor_agg &lt;- aggregate(or, fact = 30)\n\n\n\n\nCheck for alignment\n\n\nCode\ncrs(id_agg) == crs(or_agg)\n\n\n[1] TRUE\n\n\nCode\norigin(id_agg) == origin(or_agg)\n\n\n[1] FALSE FALSE\n\n\nCode\next(id_agg) == ext(or_agg)\n\n\n[1] FALSE\n\n\n\n\nAlign the origins\nBoth project and resample don’t give the intended results.\n\n\nCode\nid_proj &lt;- project(id_agg, or_agg)\n\nid_resamp &lt;- resample(id_agg, or_agg)\n\npar(mfrow = c(1,2))\nplot(id_proj)\nplot(id_resamp)\n\n\n\n\n\n\n\n\n\nWe need to extend, then resample. Resample is the faster choice because the CRS’s of the the two rasters already match.\n\n\nCode\nid_ext &lt;- extend(id_agg, or_agg)\nor_ext &lt;- extend(or_agg, id_ext)\n\nid_resamp &lt;- resample(id_ext, or_ext)\n\n\n\n\nCode\ncrs(id_resamp) == crs(or_ext)\n\n\n[1] TRUE\n\n\nCode\norigin(id_resamp) == origin(or_ext)\n\n\n[1] TRUE TRUE\n\n\nCode\next(id_resamp) == ext(or_ext)\n\n\n[1] TRUE\n\n\n\n\nMosaic\n\n\nCode\nidor &lt;- mosaic(id_resamp, or_ext)\n\n\n\n\nCrops and Masks\n\n\nCode\nidor_counties &lt;- counties(state = c(\"ID\", \"OR\"))\n\n\nRetrieving data for the year 2022\n\n\nCode\neast_id &lt;- idor_counties %&gt;%\n  filter(NAME %in% c(\"Teton\", \"Jefferson\", \"Madison\"))\nplot(st_geometry(east_id))\n\n\n\n\n\n\n\n\n\nmask without crop leaves a lot of white space.\n\n\nCode\neast_id_proj &lt;- st_transform(east_id, crs(idor))\n\neast_id_fire &lt;- mask(x = idor, mask = east_id_proj)\n\neast_id_fire2 &lt;- crop(idor, east_id_proj, mask=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "slides/10-slides.html#announcements",
    "href": "slides/10-slides.html#announcements",
    "title": "Operations With Vector Data I",
    "section": "Announcements",
    "text": "Announcements\nDue this week: assignment revision 1"
  },
  {
    "objectID": "slides/10-slides.html#objectives",
    "href": "slides/10-slides.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data"
  },
  {
    "objectID": "slides/10-slides.html#revisiting-predicates-and-measures",
    "href": "slides/10-slides.html#revisiting-predicates-and-measures",
    "title": "Operations With Vector Data I",
    "section": "Revisiting predicates and measures",
    "text": "Revisiting predicates and measures\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nUnary, binary, and n-ary distinguish how many geometries each function accepts and returns"
  },
  {
    "objectID": "slides/10-slides.html#transformations",
    "href": "slides/10-slides.html#transformations",
    "title": "Operations With Vector Data I",
    "section": "Transformations",
    "text": "Transformations\n\n\nTransformations: create new geometries based on input geometries"
  },
  {
    "objectID": "slides/10-slides.html#unary-transformations",
    "href": "slides/10-slides.html#unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Unary Transformations",
    "text": "Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that do no longer cover the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith an arbitrary point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries",
    "href": "slides/10-slides.html#fixing-geometries",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries",
    "text": "Fixing geometries\n\nWhen all(st_is_valid(your.shapefile)) returns FALSE\n\n\n\n\n\nst_make_valid has two methods:\n\noriginal converts rings into noded lines and extracts polygons\nstructured makes rings valid first then merges/subtracts from existing polgyons\nVerify that the output is what you expect!!\n\n\n\n\n\n```{r}\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(0.5,0),c(0.5,0.5),c(0.5,0),c(1,0),c(1,1),c(0,1),c(0,0)))))\nst_is_valid(x)\n```\n\n[1] FALSE"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries-with-st_make_valid",
    "href": "slides/10-slides.html#fixing-geometries-with-st_make_valid",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries with st_make_valid",
    "text": "Fixing geometries with st_make_valid\n\n\n\n\n\n\n\n\n\n\n\n\n\n```{r}\ny &lt;- x %&gt;% st_make_valid()\nst_is_valid(y)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries-with-st_buffer",
    "href": "slides/10-slides.html#fixing-geometries-with-st_buffer",
    "title": "Operations With Vector Data I",
    "section": "Fixing Geometries with st_buffer",
    "text": "Fixing Geometries with st_buffer\n\n\n\nst_buffer enforces valid geometries as an output\nSetting a 0 distance buffer leaves most geometries unchanged\nNot all transformations do this\n\n\n\n```{r}\nz &lt;- x %&gt;% st_buffer(., dist=0)\n\nst_is_valid(z)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-slides.html#changing-crs-with-st_transform",
    "href": "slides/10-slides.html#changing-crs-with-st_transform",
    "title": "Operations With Vector Data I",
    "section": "Changing CRS with st_transform",
    "text": "Changing CRS with st_transform\n\nYou’ve already been using this!!\nDoes not guarantee valid geometries (use check = TRUE if you want this)\nWe’ll try to keep things from getting too complicated"
  },
  {
    "objectID": "slides/10-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "href": "slides/10-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "title": "Operations With Vector Data I",
    "section": "Converting areas to points with st_centroid or st_point_on_surface",
    "text": "Converting areas to points with st_centroid or st_point_on_surface\n\n\n\n\nFor “sampling” other datasets\nTo simplify distance calculations\nTo construct networks\n\n\n\n\nid.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE)\nid.centroid &lt;- st_centroid(id.counties)\nid.pointonsurf &lt;- st_point_on_surface(id.counties)"
  },
  {
    "objectID": "slides/10-slides.html#creating-sampling-areas",
    "href": "slides/10-slides.html#creating-sampling-areas",
    "title": "Operations With Vector Data I",
    "section": "Creating “sampling areas”",
    "text": "Creating “sampling areas”\n\nUncertainty in your point locations\nIncorporate a fixed range around each point\nCombine multiple points into a single polygon\n\n\nhospitals.id &lt;- landmarks.id.csv %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4326"
  },
  {
    "objectID": "slides/10-slides.html#creating-sampling-areas-1",
    "href": "slides/10-slides.html#creating-sampling-areas-1",
    "title": "Operations With Vector Data I",
    "section": "Creating sampling areas",
    "text": "Creating sampling areas\n\nhospital.buf &lt;- hospitals.id %&gt;%\n  st_buffer(., dist=10000)\n\nhospital.mcp &lt;- hospitals.id %&gt;% \n  st_convex_hull(.)"
  },
  {
    "objectID": "slides/10-slides.html#other-unary-transformations",
    "href": "slides/10-slides.html#other-unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Other Unary Transformations",
    "text": "Other Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system\n\n\ntriangulate\nwith Delauney triangulated polygon(s)\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix"
  },
  {
    "objectID": "slides/10-slides.html#binary-transformers-1",
    "href": "slides/10-slides.html#binary-transformers-1",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers\n\n\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n%/%\n\n\ncrop\ncrop an sf object to a specific rectangle"
  },
  {
    "objectID": "slides/10-slides.html#binary-transformers-2",
    "href": "slides/10-slides.html#binary-transformers-2",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers"
  },
  {
    "objectID": "slides/10-slides.html#common-uses-of-binary-transformers",
    "href": "slides/10-slides.html#common-uses-of-binary-transformers",
    "title": "Operations With Vector Data I",
    "section": "Common Uses of Binary Transformers",
    "text": "Common Uses of Binary Transformers\n\nRelating partially overlapping datasets to each other\nReducing the extent of vector objects"
  },
  {
    "objectID": "slides/10-slides.html#n-ary-transformers",
    "href": "slides/10-slides.html#n-ary-transformers",
    "title": "Operations With Vector Data I",
    "section": "N-ary Transformers",
    "text": "N-ary Transformers\n\nSimilar to Binary (except st_crop)\nunion can be applied to a set of geometries to return its geometrical union\nintersection and difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples, etc."
  },
  {
    "objectID": "slides/10-slides.html#centroids-and-distances",
    "href": "slides/10-slides.html#centroids-and-distances",
    "title": "Operations With Vector Data I",
    "section": "Centroids and Distances",
    "text": "Centroids and Distances\nThe function system.time tells you how long a function takes to run:\n\nsystem.time(id.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE))\n\n   user  system elapsed \n   0.98    0.28    1.30 \n\n\nFind the counties that are the furthest distance from each other in Idaho using the polygons, centroids, and point on surface objects we created earlier. Which distance calculation is the fastest?\n(You may want to refer to our session 7 example code.)"
  },
  {
    "objectID": "slides/10-slides.html#intersections-and-buffers",
    "href": "slides/10-slides.html#intersections-and-buffers",
    "title": "Operations With Vector Data I",
    "section": "Intersections and Buffers",
    "text": "Intersections and Buffers\ntigris::primary_secondary_roads() retrieves shapefiles for major roads in each state of the US.\n\nPlot just Ada county and the major roads within.\nMap the portion of major roads that are within 50 km (as the crow flies) of the center of Ada county. (Remember to check the units of your CRS.)\nChallenge: Include county borders in your plot for part 2."
  },
  {
    "objectID": "slides/07-slides.html#objectives",
    "href": "slides/07-slides.html#objectives",
    "title": "Areal Data: Vector Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUnderstand predicates and measures in the context of spatial operations in sf\nDefine valid geometries and approaches for assessing geometries in R\nUse st_* and sf_* to evaluate attributes of geometries and calculate measurements"
  },
  {
    "objectID": "slides/07-slides.html#revisiting-simple-features",
    "href": "slides/07-slides.html#revisiting-simple-features",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\n\n\n\nThe sf package relies on a simple feature data model to represent geometries\n\nhierarchical\nstandardized methods\ncomplementary binary and human-readable encoding\n\n\n\n\n\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above"
  },
  {
    "objectID": "slides/07-slides.html#revisiting-simple-features-1",
    "href": "slides/07-slides.html#revisiting-simple-features-1",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\nYou already know how to access some elements of a simple feature\nst_crs - returns the coordinate reference system\nst_bbox - returns the bounding box for the simple feature"
  },
  {
    "objectID": "slides/07-slides.html#standaridized-methods",
    "href": "slides/07-slides.html#standaridized-methods",
    "title": "Areal Data: Vector Data",
    "section": "Standaridized Methods",
    "text": "Standaridized Methods\n\nWe can categorize sf operations based on what they return and/or how many geometries they accept as input.\n\n\n\n\n\nOutput Categories\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\n\n\n\n\n\nInput Geometries\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries",
    "href": "slides/07-slides.html#remembering-valid-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\n\nA linestring is simple if it does not intersect\n\n\n\nlibrary(sf)\nlibrary(tidyverse)\nls = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(2,1), c(3,4)))\n\nls2 = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(0,2), c(1,1), c(2,0)))"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-1",
    "href": "slides/07-slides.html#remembering-valid-geometries-1",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\nValid polygons\n\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line)\n\nFor multipolygons, adjacent polygons touch only at points\n\nDo not repeat their own path"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-2",
    "href": "slides/07-slides.html#remembering-valid-geometries-2",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np1 = st_as_sfc(\"POLYGON((0 0, 0 10, 10 0, 10 10, 0 0))\")\np2 = st_as_sfc(\"POLYGON((0 0, 0 10, 5 5,  0 0))\")\np3 = st_as_sfc(\"POLYGON((5 5, 10 10, 10 0, 5 5))\")"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-3",
    "href": "slides/07-slides.html#remembering-valid-geometries-3",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np4 = st_as_sfc(c(\"POLYGON((0 0, 0 10, 5 5,  0 0))\", \"POLYGON((5 5, 10 10, 10 0, 5 5))\"))\nplot(p4, col=c( \"#7C4A89\", \"blue\"))"
  },
  {
    "objectID": "slides/07-slides.html#empty-geometries",
    "href": "slides/07-slides.html#empty-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/07-slides.html#using-unary-predicates",
    "href": "slides/07-slides.html#using-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Using Unary Predicates",
    "text": "Using Unary Predicates\n\nUnary predicates accept single geometries (or geometry collections)\nProvide helpful ways to check whether your data is ready to analyze\nUse the st_ prefix and return TRUE/FALSE\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis_simple\nis the geometry self-intersecting (i.e., simple)?\n\n\nis_valid\nis the geometry valid?\n\n\nis_empty\nis the geometry column of an object empty?\n\n\nis_longlat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?"
  },
  {
    "objectID": "slides/07-slides.html#checking-geometries-with-unary-predicates",
    "href": "slides/07-slides.html#checking-geometries-with-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\nBefore conducting costly analyses, it’s worth checking for:\n\n\n\nempty geometries, using any(st_is_empty(x)))\ncorrupt geometries, using any(is.na(st_is_valid(x)))\ninvalid geometries, using any(na.omit(st_is_valid(x)) == FALSE); in case of corrupt and/or invalid geometries,\nin case of invalid geometries, query the reason for invalidity by st_is_valid(x, reason = TRUE)\n\n\nInvalid geometries will require transformation (next week!)"
  },
  {
    "objectID": "slides/07-slides.html#checking-geometries-with-unary-predicates-1",
    "href": "slides/07-slides.html#checking-geometries-with-unary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\n\n\n\n\n\n\n\n\n\n\n\nst_is_simple(ls)\n\n[1] TRUE\n\nst_is_simple(ls2)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_is_valid(p1)\n\n[1] FALSE\n\nst_is_valid(p4)\n\n[1] TRUE TRUE"
  },
  {
    "objectID": "slides/07-slides.html#unary-predicates-and-real-data",
    "href": "slides/07-slides.html#unary-predicates-and-real-data",
    "title": "Areal Data: Vector Data",
    "section": "Unary Predicates and Real Data",
    "text": "Unary Predicates and Real Data\n\n\n\nlibrary(tigris)\nid.cty &lt;- counties(\"ID\", \n                   progress_bar=FALSE)\nst_crs(id.cty)$input\n\n[1] \"NAD83\"\n\nst_is_longlat(id.cty)\n\n[1] TRUE\n\nst_is_valid(id.cty)[1:5]\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nall(st_is_valid(id.cty))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/07-slides.html#binary-predicates-1",
    "href": "slides/07-slides.html#binary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\nAccept exactly two geometries (or collections)\nAlso return logical outcomes\nBased on the Dimensionally Extended 9-Intersection Model (DE-9IM)\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B AND A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\ngiven a mask pattern, return whether A and B adhere to this pattern"
  },
  {
    "objectID": "slides/07-slides.html#binary-predicates-2",
    "href": "slides/07-slides.html#binary-predicates-2",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\n\nid &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"ID\")\nor &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"OR\")\nada.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Ada\")\n\n\n\nst_covers(id, ada.cty)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `covers'\n 1: 1\n\nst_covers(id, ada.cty, sparse=FALSE)\n\n     [,1]\n[1,] TRUE\n\nst_within(ada.cty, or)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: (empty)\n\nst_within(ada.cty, or, sparse=FALSE)\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "slides/07-slides.html#measures-1",
    "href": "slides/07-slides.html#measures-1",
    "title": "Areal Data: Vector Data",
    "section": "Measures",
    "text": "Measures\nUnary Measures\n\nReturn quantities of individual geometries\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n\nUnary Measures\n\nst_area(id)\n\n2.15994e+11 [m^2]\n\nst_area(id.cty[1:5,])\n\nUnits: [m^2]\n[1] 2858212132 3380630278 1459359818 1726660462 1223521586\n\nst_dimension(id.cty[1:5,])\n\n[1] 2 2 2 2 2"
  },
  {
    "objectID": "slides/07-slides.html#binary-measures",
    "href": "slides/07-slides.html#binary-measures",
    "title": "Areal Data: Vector Data",
    "section": "Binary Measures",
    "text": "Binary Measures\n\nst_distance returns the distance between pairs of geometries\n\n\nkootenai.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Kootenai\")\nst_distance(kootenai.cty, ada.cty)\n\nUnits: [m]\n         [,1]\n[1,] 396433.8\n\nst_distance(id.cty)[1:5, 1:5]\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]      0.0 467635.7 277227.0 132998.0      0.0\n[2,] 467635.7      0.0 319706.4 656056.0 514306.9\n[3,] 277227.0 319706.4      0.0 377105.4 336146.8\n[4,] 132998.0 656056.0 377105.4      0.0 133045.5\n[5,]      0.0 514306.9 336146.8 133045.5      0.0"
  },
  {
    "objectID": "slides/07-slides.html#practice",
    "href": "slides/07-slides.html#practice",
    "title": "Areal Data: Vector Data",
    "section": "Practice!",
    "text": "Practice!\n\nCreate a vector object for Owyhee county in Idaho (hint: use filter).\nOwyhee county is in Idaho and borders Oregon. Which two predicates communicate this information? Show your code and output.\nPrint the bounding box information for Oregon and Owyhee county. Which part of this output could clue you in that they border each other?\n\nIf you finish early, try the challenge on the next slide!"
  },
  {
    "objectID": "slides/07-slides.html#challenge",
    "href": "slides/07-slides.html#challenge",
    "title": "Areal Data: Vector Data",
    "section": "Challenge",
    "text": "Challenge\nWhich two counties in Idaho are furthest from each other? You will need to use spatial measures from this class as well as data navigation methods that you may need to look up!"
  },
  {
    "objectID": "slides/06-slides.html#objectives",
    "href": "slides/06-slides.html#objectives",
    "title": "Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the linkage between location, coordinates, coordinate reference systems, and geometry\nAccess and manipulate geometries in R with sf (and terra)\nDefine geometry in the context of vector objects and troubleshoot common problems\nChange the CRS for vectors and rasters (and understand the implications)"
  },
  {
    "objectID": "slides/06-slides.html#getting-more-acquainted-with-r",
    "href": "slides/06-slides.html#getting-more-acquainted-with-r",
    "title": "Coordinates and Geometries",
    "section": "Getting more acquainted with R",
    "text": "Getting more acquainted with R\n\nObjects, classes, functions, oh my…\nIntuition for the tidyverse\nGetting used to pipes (%&gt;% or |&gt;)\nLearning to prototype"
  },
  {
    "objectID": "slides/06-slides.html#kinds-of-errors",
    "href": "slides/06-slides.html#kinds-of-errors",
    "title": "Coordinates and Geometries",
    "section": "2 Kinds of Errors",
    "text": "2 Kinds of Errors\n\nSyntax Errors: Your code won’t actually run\nSemantic Errors: Your code runs without error, but the result is unexpected"
  },
  {
    "objectID": "slides/06-slides.html#asking-better-questions-getting-better-answers",
    "href": "slides/06-slides.html#asking-better-questions-getting-better-answers",
    "title": "Coordinates and Geometries",
    "section": "Asking better questions, getting better answers",
    "text": "Asking better questions, getting better answers\n\nPlaces to get help (Google, Slack, Stack Overflow, Github Issue pages)\nWhat are you trying to do? (the outcome you want/expect)\nWhat isn’t working? (the code and steps you’ve tried so far)\nWhy aren’t common solutions working? (proof that you’ve done your due diligence)"
  },
  {
    "objectID": "slides/06-slides.html#reproducible-examples",
    "href": "slides/06-slides.html#reproducible-examples",
    "title": "Coordinates and Geometries",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\nDon’t require someone to have your data or your computer\nMinimal amount of information and code to reproduce your error\nIncludes both code and your operating environment info\nMore info\nAn example with spatial data"
  },
  {
    "objectID": "slides/06-slides.html#reference-systems",
    "href": "slides/06-slides.html#reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Reference Systems",
    "text": "Reference Systems\n\nTo locate an object or quantity, we need:\n\nA fixed origin (or datum) to measure distances to/from\nA measurement unit (or scale) that defines the units of distance\nDatum + scale = reference system"
  },
  {
    "objectID": "slides/06-slides.html#coordinate-reference-systems",
    "href": "slides/06-slides.html#coordinate-reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nMap the location on an object to earth (geodetic) or flat (projected) surfaces\nCoordinate System - the mathematical rules that specify how coordinates are assigned to points\nDatum - the parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system\nCoordinate Reference Systems - a coordinate system that is related to an object by a datum"
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r",
    "href": "slides/06-slides.html#accessing-crs-with-r",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nsf::st_crs() for vector data\nterra::crs() for raster data\nstored in WKT, epsg, or proj4string (deprecated)\nThe EPSG website is a great reference for getting projection info"
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r-1",
    "href": "slides/06-slides.html#accessing-crs-with-r-1",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\ndir.for.files &lt;- \"C:/Users/carolynkoehn/Documents/HES505_Fall_2024/data/2023/assignment01/\"\nvector.data &lt;- sf::st_read(dsn = paste0(dir.for.files, \"cejst_nw.shp\"), quiet=TRUE)\nsf::st_crs(x = vector.data)$input\n\n[1] \"WGS 84\"\n\nsf::st_crs(x = vector.data)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nsf::st_crs(x = vector.data)$wkt\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    ID[\\\"EPSG\\\",4326]]\""
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r-2",
    "href": "slides/06-slides.html#accessing-crs-with-r-2",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nraster.data &lt;- terra::rast(x = paste0(dir.for.files, \"wildfire_hazard_agg.tif\"))\nterra::crs(raster.data, describe=TRUE, proj=TRUE)\n\n     name authority code area         extent\n1 unnamed      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n                                                                                                 proj\n1 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"
  },
  {
    "objectID": "slides/06-slides.html#what-if-you-dont-know-the-crs",
    "href": "slides/06-slides.html#what-if-you-dont-know-the-crs",
    "title": "Coordinates and Geometries",
    "section": "What if you don’t know the CRS?",
    "text": "What if you don’t know the CRS?\n\n\nSometimes you receive data that is missing the projection\nYou can assign it (with caution)\nYou can guess it using crsuggest::guess_crs()\n\n\n\nlibrary(sf)\nlibrary(mapview)\nlocations &lt;- data.frame(\n  X = c(1200822.97857801, 1205015.51644983, 1202297.44383987, 1205877.68696743, \n        1194763.21511923, 1195463.42403192, 1199836.01037452, 1207081.96500368, \n        1201924.15986897),\n  Y = c(1246476.31475063, 1248612.72571423, 1241479.45996392, 1243898.58428024, \n        1246033.7550009, 1241827.7730307, 1234691.50899912, 1251125.67808482, \n        1252188.4333016),\n  id = 1:9\n)\n\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"X\", \"Y\"))"
  },
  {
    "objectID": "slides/06-slides.html#guessing-crs",
    "href": "slides/06-slides.html#guessing-crs",
    "title": "Coordinates and Geometries",
    "section": "Guessing CRS",
    "text": "Guessing CRS\n\nlibrary(crsuggest)\nguess_crs(locations_sf,\n          target_location = c(80.270721, 13.082680),\n          n_return = 5)\n\n# A tibble: 5 × 2\n  crs_code dist_km\n  &lt;chr&gt;      &lt;dbl&gt;\n1 7785        4.13\n2 24344     806.  \n3 32644     806.  \n4 32244     806.  \n5 32444     806.  \n\nst_crs(locations_sf) &lt;- 7785"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs",
    "href": "slides/06-slides.html#changing-the-crs",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS",
    "text": "Changing the CRS\n\nRequires recomputing coordinates\nCoordinate Conversion - No change to the datum; lossless\nCoordinate Transformation - New datum; relies on models; some error involved"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r",
    "href": "slides/06-slides.html#changing-the-crs-in-r",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nsf::st_transform for vectors\nterra::project for rasters\nProjecting Rasters Causes Distortion"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r-1",
    "href": "slides/06-slides.html#changing-the-crs-in-r-1",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nvector.data.proj &lt;- vector.data %&gt;%\n  sf::st_transform(., crs = 3083)\nst_crs(vector.data.proj)$input\n\n[1] \"EPSG:3083\"\n\nvector.data.proj.rast &lt;- vector.data %&gt;%\n  sf::st_transform(., crs = crs(raster.data))\nst_crs(vector.data.proj.rast)$proj4string\n\n[1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\""
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r-2",
    "href": "slides/06-slides.html#changing-the-crs-in-r-2",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nraster.data.proj &lt;- project(x = raster.data, y = \"EPSG:3083\")\ncrs(raster.data.proj, describe=TRUE, proj=TRUE)\n\n                                     name authority code\n1 NAD83 / Texas Centric Albers Equal Area      EPSG 3083\n                         area                        extent\n1 United States (USA) - Texas -106.66, -93.50, 25.83, 36.50\n                                                                                                            proj\n1 +proj=aea +lat_0=18 +lon_0=-100 +lat_1=27.5 +lat_2=35 +x_0=1500000 +y_0=6000000 +datum=NAD83 +units=m +no_defs\n\nraster.data.proj.vect &lt;- project(x = raster.data, y = vect(vector.data))\ncrs(raster.data.proj.vect, describe=TRUE, proj=TRUE)\n\n    name authority code area         extent                                proj\n1 WGS 84      EPSG 4326 &lt;NA&gt; NA, NA, NA, NA +proj=longlat +datum=WGS84 +no_defs"
  },
  {
    "objectID": "slides/06-slides.html#the-vector-data-model",
    "href": "slides/06-slides.html#the-vector-data-model",
    "title": "Coordinates and Geometries",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nCoordinates define the Vertices (i.e., discrete x-y locations) that comprise the geometry\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons"
  },
  {
    "objectID": "slides/06-slides.html#representing-vector-data-in-r",
    "href": "slides/06-slides.html#representing-vector-data-in-r",
    "title": "Coordinates and Geometries",
    "section": "Representing vector data in R",
    "text": "Representing vector data in R\n\n\n\n\n\nFrom Lovelace et al.\n\n\n\n\n\nsf hierarchy reflects increasing complexity of geometry\n\nst_point, st_linestring, st_polygon for single features\nst_multi* for multiple features of the same type\nst_geometrycollection for multiple feature types\nst_as_sfc creates the geometry list column for many sf operations"
  },
  {
    "objectID": "slides/06-slides.html#points",
    "href": "slides/06-slides.html#points",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nlibrary(sf)\nproj &lt;- st_crs('+proj=longlat +datum=WGS84')\nlong &lt;- c(-116.7, -120.4, -116.7, -113.5, -115.5, -120.8, -119.5, -113.7, -113.7, -110.7)\nlat &lt;- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9, 36.2, 39, 41.6, 36.9)\nst_multipoint(cbind(long, lat)) %&gt;% st_sfc(., crs = proj)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -120.8 ymin: 35.7 xmax: -110.7 ymax: 45.3\nGeodetic CRS:  +proj=longlat +datum=WGS84"
  },
  {
    "objectID": "slides/06-slides.html#points-1",
    "href": "slides/06-slides.html#points-1",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nplot(st_multipoint(cbind(long, lat)) %&gt;% \n                   st_sfc(., crs = proj))"
  },
  {
    "objectID": "slides/06-slides.html#lines",
    "href": "slides/06-slides.html#lines",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nlon &lt;- c(-116.8, -114.2, -112.9, -111.9, -114.2, -115.4, -117.7)\nlat &lt;- c(41.3, 42.9, 42.4, 39.8, 37.6, 38.3, 37.6)\nlonlat &lt;- cbind(lon, lat)\npts &lt;- st_multipoint(lonlat)\n\nsfline &lt;- st_multilinestring(list(pts[1:3,], pts[4:7,]))\nstr(sfline)\n\nList of 2\n $ : num [1:3, 1:2] -116.8 -114.2 -112.9 41.3 42.9 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n $ : num [1:4, 1:2] -111.9 -114.2 -115.4 -117.7 39.8 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n - attr(*, \"class\")= chr [1:3] \"XY\" \"MULTILINESTRING\" \"sfg\""
  },
  {
    "objectID": "slides/06-slides.html#lines-1",
    "href": "slides/06-slides.html#lines-1",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nplot(st_multilinestring(list(pts[1:3,], pts[4:7,])))"
  },
  {
    "objectID": "slides/06-slides.html#polygons",
    "href": "slides/06-slides.html#polygons",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\ncoords = list(outer, hole1, hole2)\npl1 = st_polygon(coords)"
  },
  {
    "objectID": "slides/06-slides.html#polygons-1",
    "href": "slides/06-slides.html#polygons-1",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nplot(pl1)"
  },
  {
    "objectID": "slides/06-slides.html#common-problems-with-vector-data",
    "href": "slides/06-slides.html#common-problems-with-vector-data",
    "title": "Coordinates and Geometries",
    "section": "Common Problems with Vector Data",
    "text": "Common Problems with Vector Data\n\n\n\n\nVectors and scale\nSlivers and overlaps\nUndershoots and overshoots\nSelf-intersections and rings\n\n\n\n\n\n\nTopology Errors - Saylor Acad.\n\n\n\n\nWe’ll use st_is_valid() to check this, but fixing can be tricky"
  },
  {
    "objectID": "slides/06-slides.html#fixing-problematic-topology",
    "href": "slides/06-slides.html#fixing-problematic-topology",
    "title": "Coordinates and Geometries",
    "section": "Fixing Problematic Topology",
    "text": "Fixing Problematic Topology\n\nst_make_valid() for simple cases\nst_buffer with dist=0\nMore complex errors need more complex approaches"
  },
  {
    "objectID": "slides/06-slides.html#a-note-on-vectors",
    "href": "slides/06-slides.html#a-note-on-vectors",
    "title": "Coordinates and Geometries",
    "section": "A Note on Vectors",
    "text": "A Note on Vectors\n\nMoving forward we will rely primarily on the sf package for vector manipulation. Some packages require objects to be a different class. terra, for example, relies on SpatVectors. You can use as() to coerce objects from one type to another (assuming a method exists). You can also explore other packages. Many packages provide access to the ‘spatial’ backbones of R (like geos and gdal), they just differ in how the “verbs” are specified. For sf operations the st_ prefix is typical. For rgeos operations, the g prefix is common."
  },
  {
    "objectID": "slides/06-slides.html#i-want-your-feedback",
    "href": "slides/06-slides.html#i-want-your-feedback",
    "title": "Coordinates and Geometries",
    "section": "I want your feedback!",
    "text": "I want your feedback!\nPlease give me feedback on my live coding approach so I can adjust for this class’s preferences!\nhttps://forms.gle/U2acJcRABiF1TCRF7"
  },
  {
    "objectID": "assignment/03-vectorsolutions.html",
    "href": "assignment/03-vectorsolutions.html",
    "title": "Assignment 3 Solutions: Coordinates and Geometries",
    "section": "",
    "text": "1. Write out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\n\nThis one is probably a little tricky if you haven’t taken the time to check out the attributes of the data (which you should always do). That said, some pretty generic steps would be:\n\n\n1. Load each dataset\n2. Check geometry validity\n3. Align CRS\n4. Run Correlation\n5. Print Results\n\n\nThere are two key steps here, that you’ll repeat for any/all spatial analyses that you do: 1) checking for valid geometries and 2) making sure the data are aligned in a sensible CRS. I can add a code chunk for each now.\n\n\n1. Load each dataset\n\n\n2. Check geometry validity\n\n\n3. Align CRS\n\n\n4. Run Correlation\n\n\n5. Print Results\n\n2. Read in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\n\nHere I’m going to combine the load portion of my pseudocode with the validity since I can do that without creating additional object. I use the str() function to get a sense for what the data looks like and to understand what data classes I’m working with. Then, I use the all() function to make sure that all of the results of st_is_valid() are true. I don’t need to do that with the raster file as the geometry is implicit which means that it has to be topologically valid (this doesn’t mean that the numbers are accurate, it just means that the dataset conforms to the data model R expects). Then I’ll add another code to check the CRS of the different objects.\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\ncdc.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/cdc_nw.shp\")\nstr(cdc.nw)\nall(st_is_valid(cdc.nw))\npm.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/pm_nw.shp\")\nstr(pm.nw)\nall(st_is_valid(pm.nw))\n\nwildfire.haz &lt;- rast(\"data/opt/data/2023/assignment03/wildfire_hazard_agg.tif\")\nstr(wildfire.haz)\n\n\nNow that I’ve gotten the data into my environment, I need to make sure that the CRS are aligned. I’ll demonstrate that with a few different approaches. You can use the logical == or the identical function to check, but remember that these functions are not specific to spatial objects, they evaluate things very literally. So even if the CRS is the same, if st_crs returns the CRS in one format (WKT) and crs returns it in another, you’ll get FALSE even if they are actually the same CRS - pay attention to that. You’ll notice that they aren’t identical; we’ll deal with that in the next question.\n\n\nst_crs(cdc.nw)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nst_crs(pm.nw)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\ncrs(wildfire.haz)\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nidentical(st_crs(cdc.nw), st_crs(pm.nw))\n\n[1] FALSE\n\nst_crs(cdc.nw) == st_crs(pm.nw)\n\n[1] FALSE\n\n\n3. Re-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tfi file. Verify that all the files have the same projection.\n\nNow we’ll use st_transform to get the two shapefiles aligned with the raster (because we generally want to avoid projecting rasters if we can). We can then use the same steps above to see if they’re aligned. Note that I’m using the terra::crs() function to make sure that the output is printed in exactly the same format\n\n\ncdc.nw.proj &lt;- cdc.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\npm.nw.proj &lt;- pm.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\n\nidentical(crs(cdc.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\nidentical(crs(pm.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\n\n4. How does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\n\nNow we just want to look at the bounding box of the data before and after it was projected. We can do this using st_bbox. One of the most obvious changes is that the units for cdc.nw have changed from degrees to meters (as evidenced by the much larger numbers). For the pm.nw object we can see that the raw coordinates indicate a shift to the west; however, because the origin for this crs has also changed, the states still show up in the correct place.\n\n\nst_bbox(cdc.nw)\n\n      xmin       ymin       xmax       ymax \n-124.74918   41.98818 -111.04349   49.00232 \n\nst_bbox(cdc.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2295337  2208890 -1189292  3177425 \n\nst_bbox(pm.nw)\n\n     xmin      ymin      xmax      ymax \n-13898126   5159210 -12361307   6275276 \n\nst_bbox(pm.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2300791  2208891 -1189293  3177426 \n\n\n5. What class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\nThis one was probably a little tricky. First, to check the geometry type, we use st_geometry_type setting by_geometry to FALSE means we get the geometry type for the entire object instead of each observation. We then use a series of filter commands to get the records from Idaho and Ada county. Once we’ve narrowed the data to our correct region, we can filter again to find the row with the minimum value of PM25 (note that we have to set na.rm=TRUE so that we ignore the NA values). Then we just take the number of rows (nrow) of the result of st_coordinates to get the number of coordinates associated with that geometry.\n\n\nst_geometry_type(pm.nw, by_geometry = FALSE)\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nada.pm &lt;- pm.nw %&gt;%\n  filter(STATE_NAME==\"Idaho\" & CNTY_NAME==\"Ada\") %&gt;%\n  filter(PM25 == min(PM25, na.rm = TRUE))\n\nada.pm\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -12935300 ymin: 5329192 xmax: -12910260 ymax: 5402433\nProjected CRS: WGS 84 / Pseudo-Mercator\n# A tibble: 1 × 4\n  STATE_NAME CNTY_NAME  PM25                                            geometry\n* &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;                                  &lt;MULTIPOLYGON [m]&gt;\n1 Idaho      Ada        6.68 (((-12935301 5391002, -12934885 5391290, -12934526…\n\nnrow(st_coordinates(ada.pm))\n\n[1] 1394"
  },
  {
    "objectID": "example/session-14-example.html",
    "href": "example/session-14-example.html",
    "title": "Session 14 code",
    "section": "",
    "text": "Libraries for today:\nCode\nlibrary(tidyverse, quietly = TRUE)\nlibrary(spData)\nlibrary(sf)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#subsetting-data",
    "href": "example/session-14-example.html#subsetting-data",
    "title": "Session 14 code",
    "section": "Subsetting Data:",
    "text": "Subsetting Data:\n\n\nCode\ncolnames(world)\n\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\n\n\nCode\nhead(world)[,1:3] %&gt;% \n  st_drop_geometry()\n\n\n# A tibble: 6 × 3\n  iso_a2 name_long      continent    \n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;        \n1 FJ     Fiji           Oceania      \n2 TZ     Tanzania       Africa       \n3 EH     Western Sahara Africa       \n4 CA     Canada         North America\n5 US     United States  North America\n6 KZ     Kazakhstan     Asia         \n\n\n\n\nCode\nworld %&gt;%\n  dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.) \n\n\n# A tibble: 6 × 2\n  name_long      continent    \n  &lt;chr&gt;          &lt;chr&gt;        \n1 Fiji           Oceania      \n2 Tanzania       Africa       \n3 Western Sahara Africa       \n4 Canada         North America\n5 United States  North America\n6 Kazakhstan     Asia         \n\n\n\n\nCode\nhead(world)[1:3, 1:3] %&gt;% \n  st_drop_geometry()\n\n\n# A tibble: 3 × 3\n  iso_a2 name_long      continent\n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;    \n1 FJ     Fiji           Oceania  \n2 TZ     Tanzania       Africa   \n3 EH     Western Sahara Africa   \n\n\n\n\nCode\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n  select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n\n# A tibble: 6 × 2\n  name_long   continent\n  &lt;chr&gt;       &lt;chr&gt;    \n1 Kazakhstan  Asia     \n2 Uzbekistan  Asia     \n3 Indonesia   Asia     \n4 Timor-Leste Asia     \n5 Israel      Asia     \n6 Lebanon     Asia",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#create-new-columns",
    "href": "example/session-14-example.html#create-new-columns",
    "title": "Session 14 code",
    "section": "Create new columns",
    "text": "Create new columns\n\n\nCode\nworld_dens &lt;- world %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n  select(name_long, continent, pop, gdpPercap ,area_km2) %&gt;%\n  mutate(., dens = pop/area_km2,\n         totGDP = gdpPercap * pop) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#aggregate-summarise",
    "href": "example/session-14-example.html#aggregate-summarise",
    "title": "Session 14 code",
    "section": "Aggregate / Summarise",
    "text": "Aggregate / Summarise\n\n\nCode\nworld %&gt;%\n  st_drop_geometry(.) %&gt;% \n  group_by(continent) %&gt;%\n  summarize(pop = sum(pop, na.rm = TRUE))\n\n\n# A tibble: 8 × 2\n  continent                      pop\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#joins",
    "href": "example/session-14-example.html#joins",
    "title": "Session 14 code",
    "section": "Joins",
    "text": "Joins\n\n\nCode\nhead(coffee_data)\n\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\n\n\nCode\nworld_coffee = left_join(world, coffee_data)\n\n\nJoining with `by = join_by(name_long)`\n\n\nCode\nnrow(world_coffee)\n\n\n[1] 177\n\n\n\n\nCode\nplot(world_coffee[\"coffee_production_2016\"])\n\n\n\n\n\n\n\n\n\n\n\nCode\nworld_coffee_inner = inner_join(world, coffee_data)\n\n\nJoining with `by = join_by(name_long)`\n\n\nCode\nnrow(world_coffee_inner)\n\n\n[1] 45",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#practice",
    "href": "example/session-14-example.html#practice",
    "title": "Session 14 code",
    "section": "Practice:",
    "text": "Practice:\nWhat is the population density for the tracts in the cejst data? Our data sources are:\n\nTotal population in each tract (cejst$TPF)\nArea in \\(m^2\\) of each tract (tigris::tracts(), column ALAND)\n\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment06/cejst_pnw.shp\")\n\nid_tracts &lt;- tigris::tracts(state = \"ID\", year = 2015)\n\n\n\n\nCode\ncejst_id &lt;- cejst %&gt;%\n  filter(SF == \"Idaho\")\n\n\n\n\nCode\nid_tracts &lt;- id_tracts %&gt;%\n  mutate(ALAND_sqmi = ALAND/2589988.11)\nhead(id_tracts)\n\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -117.0628 ymin: 41.99601 xmax: -111.5077 ymax: 46.39597\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE       GEOID    NAME             NAMELSAD MTFCC\n1      16      041  970200 16041970200    9702    Census Tract 9702 G5020\n2      16      041  970100 16041970100    9701    Census Tract 9701 G5020\n3      16      073  950200 16073950200    9502    Census Tract 9502 G5020\n4      16      073  950101 16073950101 9501.01 Census Tract 9501.01 G5020\n5      16      073  950102 16073950102 9501.02 Census Tract 9501.02 G5020\n6      16      069  960700 16069960700    9607    Census Tract 9607 G5020\n  FUNCSTAT       ALAND   AWATER    INTPTLAT     INTPTLON\n1        S   455342589  2412885 +42.0609834 -111.7147361\n2        S  1263363258  9752226 +42.2231666 -111.8485407\n3        S 19603341439 77025612 +42.5728508 -116.1896903\n4        S   117363851  1585581 +43.5924261 -116.9602208\n5        S   132949223  2844915 +43.5247849 -116.8481343\n6        S   770216313  9225885 +46.0956517 -116.8989005\n                        geometry ALAND_sqmi\n1 POLYGON ((-111.935 42.00164...  175.80876\n2 POLYGON ((-112.1263 42.2853...  487.78728\n3 POLYGON ((-117.027 43.54418... 7568.89245\n4 POLYGON ((-117.0268 43.6465...   45.31444\n5 POLYGON ((-116.9284 43.5437...   51.33198\n6 POLYGON ((-117.0628 46.3652...  297.38218\n\n\n\n\nCode\nid_tracts &lt;- st_drop_geometry(id_tracts)\n\n\n\n\nCode\ncejst_id_join &lt;- inner_join(cejst_id, id_tracts,\n                            by = c(\"GEOID10\" = \"GEOID\")) %&gt;%\n  mutate(pop_dens = TPF/ALAND_sqmi)\n\n\n\n\nCode\nplot(cejst_id_join[\"pop_dens\"])\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\nCode\ntm_shape(cejst_id_join) +\n  tm_polygons(col = \"pop_dens\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(cejst_id_join, aes(x=pop_dens, y=IS_PFS)) +\n  geom_point()",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-07-example.html",
    "href": "example/session-07-example.html",
    "title": "Session 7 Live Code",
    "section": "",
    "text": "Read in the libraries we need\n\n\nCode\nlibrary(sf)\nlibrary(tigris)\n\n\n\n\nGet a sf object of ID counties (from the tigris package)\n\n\nCode\nid.cty &lt;- counties(state = \"ID\")\n\n\n\n\nCheck CRS of object\n\n\nCode\nst_crs(id.cty)$input\n\n\n[1] \"NAD83\"\n\n\n\n\nUnary predicates\n\n\nCode\nst_is_longlat(id.cty)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(id.cty)[1:5]\n\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n\nCode\nall(st_is_valid(id.cty))\n\n\n[1] TRUE\n\n\n\n\nGet some data for binary operations\n\n\nCode\nlibrary(tidyverse)\n\nid &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"ID\")\nor &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"OR\")\nada.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Ada\")\n\n\n\n\nTry some predicates\n\n\nCode\nst_covers(id, ada.cty)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `covers'\n 1: 1\n\n\nCode\nst_covers(id, ada.cty, sparse = FALSE)\n\n\n     [,1]\n[1,] TRUE\n\n\n\n\nCode\nst_within(ada.cty, or)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: (empty)\n\n\nCode\nst_within(ada.cty, or, sparse=FALSE)\n\n\n      [,1]\n[1,] FALSE\n\n\n\n\nUnary measures\n\n\nCode\nst_area(id)\n\n\n2.15994e+11 [m^2]\n\n\nCode\nst_area(id.cty)[1:5]\n\n\nUnits: [m^2]\n[1] 2858212132 3380630278 1459359818 1726660462 1223521586\n\n\n\n\nCode\nst_dimension(id.cty)[1:5]\n\n\n[1] 2 2 2 2 2\n\n\n\n\nBinary measure (distance)\n\n\nCode\nkootenai.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Kootenai\")\nst_distance(kootenai.cty, ada.cty)\n\n\nUnits: [m]\n         [,1]\n[1,] 396433.8\n\n\n\n\nCode\nst_distance(id.cty)[1:5, 1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]      0.0 467635.7 277227.0 132998.0      0.0\n[2,] 467635.7      0.0 319706.4 656056.0 514306.9\n[3,] 277227.0 319706.4      0.0 377105.4 336146.8\n[4,] 132998.0 656056.0 377105.4      0.0 133045.5\n[5,]      0.0 514306.9 336146.8 133045.5      0.0\n\n\n\n\nPractice exercise code:\n\n\nCode\n# Part 1\nowyhee.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Owyhee\")\n\n# Part 2\nst_within(owyhee.cty, id)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: 1\n\n\nCode\nst_touches(owyhee.cty, or)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `touches'\n 1: 1\n\n\nCode\n# Part 3\nst_bbox(owyhee.cty)\n\n\n      xmin       ymin       xmax       ymax \n-117.02701   41.99612 -115.03751   43.68080 \n\n\nCode\nst_bbox(or)\n\n\n      xmin       ymin       xmax       ymax \n-124.70354   41.99208 -116.46326   46.29910 \n\n\n\n\nChallenge code:\n\n\nCode\n# Calculate distances between all counties\ncty.dist &lt;- st_distance(id.cty)\n\n# Label rows and columns of matrix\ncolnames(cty.dist) &lt;- id.cty$NAME\nrownames(cty.dist) &lt;- id.cty$NAME\n\n# Find where the maximum value is\nwhich(cty.dist == max(cty.dist), arr.ind = TRUE)\n\n\n         row col\nBoundary  40   4\nFranklin   4  40\n\n\nCode\n# Locate the counties at the row numbers returned by which\n# Needed if you don't label the rows and columns\nid.cty$NAME[c(4, 40)]\n\n\n[1] \"Franklin\" \"Boundary\"\n\n\n\n\nWhiteboard notes",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "lesson/quarto.html",
    "href": "lesson/quarto.html",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "lesson/quarto.html#quarto",
    "href": "lesson/quarto.html#quarto",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "lesson/quarto.html#the-example",
    "href": "lesson/quarto.html#the-example",
    "title": "Quarto and literate programming",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "example/session-06-example.html",
    "href": "example/session-06-example.html",
    "title": "Session 6 Live Code",
    "section": "",
    "text": "This code and a few more examples can also be found on the session 6 slides.\n\nAccess vector data:\n\n\nCode\nvector.data &lt;- sf::st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\n\n\nCheck the CRS:\n\ninput\nproj4string\nwkt\n\n\n\nCode\nsf::st_crs(vector.data)$input\n\n\n[1] \"WGS 84\"\n\n\nCode\nsf::st_crs(vector.data)$proj4string\n\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nCode\nsf::st_crs(vector.data)$wkt\n\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\n\n\nRead in raster data:\n\n\nCode\nraster.data &lt;- terra::rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nCheck raster CRS:\n\n\nCode\nterra::crs(raster.data, describe=TRUE, proj=TRUE)\n\n\n     name authority code area         extent\n1 unnamed      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n                                                                                                 proj\n1 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\n\n\n\n\nGuess the CRS:\n\n\nCode\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\n#library(mapview)\nlocations &lt;- data.frame(\n  X = c(1200822.97857801, 1205015.51644983, 1202297.44383987, 1205877.68696743, \n        1194763.21511923, 1195463.42403192, 1199836.01037452, 1207081.96500368, \n        1201924.15986897),\n  Y = c(1246476.31475063, 1248612.72571423, 1241479.45996392, 1243898.58428024, \n        1246033.7550009, 1241827.7730307, 1234691.50899912, 1251125.67808482, \n        1252188.4333016),\n  id = 1:9\n)\n\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"X\", \"Y\"))\n\n\nWe stopped this example short because of confusion with the mapview library. Feel free to try the code from the slides on your own time!\n\n\nPlot vector data:\n\n\nCode\nplot(st_geometry(vector.data))\n\n\n\n\n\n\n\n\n\n\n\nRe-project a vector:\n\n\nCode\nvector.data.proj &lt;- vector.data %&gt;%\n  st_transform(., crs = 3083)\n\nst_crs(vector.data.proj)$input\n\n\n[1] \"EPSG:3083\"\n\n\nCode\nplot(st_geometry(vector.data.proj))\n\n\n\n\n\n\n\n\n\n\n\nPlot a raster:\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nplot(raster.data)\n\n\n\n\n\n\n\n\n\n\n\nChanging the CRS of a raster:\n\n\nCode\nraster.data.proj &lt;- project(raster.data, \"epsg:3083\")\n\ncrs(raster.data.proj, describe=TRUE)\n\n\n                                     name authority code\n1 NAD83 / Texas Centric Albers Equal Area      EPSG 3083\n                         area                        extent\n1 United States (USA) - Texas -106.66, -93.50, 25.83, 36.50\n\n\nCode\nplot(raster.data.proj)\n\n\n\n\n\n\n\n\n\n\n\nProject based on another dataset:\n\n\nCode\nvector.data.proj.raster &lt;- vector.data %&gt;%\n  st_transform(., crs = crs(raster.data))\n\nst_crs(vector.data.proj.raster)$input\n\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\n\n\nManually create polygon:\n\n\nCode\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\ncoords = list(outer, hole1, hole2)\npl1 = st_polygon(coords)\n\n\n\n\nCheck polygon validity\n\n\nCode\nst_is_valid(vector.data)\n\n\n   [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [57] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [71] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [85] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [99] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [113] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [127] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [141] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [155] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [169] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [183] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [197] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [225] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [239] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [253] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [267] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [281] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [295] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [309] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [323] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [337] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [351] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [365] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [379] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [393] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [407] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [435] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [449] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [463] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [477] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [491] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [505] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [519] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [533] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [547] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [561] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [575] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [589] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [603] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [617] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [631] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [645] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [659] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [673] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [687] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [701] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [715] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [729] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [743] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [757] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [771] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [785] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [799] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [813] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [827] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [841] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [855] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [869] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [883] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [897] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [911] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [925] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [939] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [953] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [967] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [981] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [995] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1009] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1023] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1037] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1051] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1065] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1079] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1093] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1107] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1135] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1149] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1163] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1177] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1191] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1205] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1219] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1233] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1247] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1261] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1275] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1289] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1303] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1317] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1345] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1359] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1373] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1387] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1401] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1415] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1429] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1443] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1457] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1471] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1485] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1499] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1513] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1527] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1555] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1569] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1583] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1597] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1611] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1625] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1639] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1653] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1667] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1681] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1695] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1709] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1723] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1737] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1751] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1765] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1779] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1793] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1807] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1821] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1835] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1849] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1863] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1877] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1891] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1905] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1919] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1933] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1947] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1961] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1975] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1989] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2003] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2017] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2031] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2045] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2059] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2073] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2087] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2101] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2115] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2129] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2143] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2157] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2171] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2185] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2199] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2213] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2227] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2255] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2269] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2283] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2297] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2311] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2325] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2339] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2353] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2367] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2381] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2395] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2409] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2423] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2437] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2465] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2479] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2493] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2507] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2521] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2535] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2549] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2563] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2577] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nCode\ntable(st_is_valid(vector.data))\n\n\n\nTRUE \n2590",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "CRS and Geometries"
    ]
  },
  {
    "objectID": "slides/03-slides.html#todays-plan",
    "href": "slides/03-slides.html#todays-plan",
    "title": "Introduction to Spatial Data",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWays to view the world\nWhat makes data (geo)spatial?\nCoordinate Reference Systems\nGeometries, support, and spatial messiness"
  },
  {
    "objectID": "slides/03-slides.html#as-a-series-of-objects",
    "href": "slides/03-slides.html#as-a-series-of-objects",
    "title": "Introduction to Spatial Data",
    "section": "…As a Series of Objects?",
    "text": "…As a Series of Objects?\n\n\n\n\nThe world is a series of entities located in space.\nUsually distinguishable, discrete, and bounded\nSome spaces can hold multiple entities, others are empty\nObjects are digital representations of entities"
  },
  {
    "objectID": "slides/03-slides.html#as-a-continuous-field",
    "href": "slides/03-slides.html#as-a-continuous-field",
    "title": "Introduction to Spatial Data",
    "section": "…As a Continuous Field",
    "text": "…As a Continuous Field\n\n\n\n\nThe earth is a single entity with properties that vary continuosly through space\nSpatial continuity: Every cell has a value (including “no data” or “not here”)\nSelf-definition: the values define the field\nSpace is tessellated: cells are mutually exclusive"
  },
  {
    "objectID": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "href": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "title": "Introduction to Spatial Data",
    "section": "Spatial data as a stochastic process",
    "text": "Spatial data as a stochastic process\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\nThere is some attribute (\\(Z(\\mathbf{s})\\)) that we observe at a location (\\(s\\)). That location (\\(s\\)) is an element of a domain of data (\\(D\\)), which is a subset of real coordinate numbers (\\(\\mathbb{R}^d\\), \\(d\\) = 2).\nThree types of spatial data are defined by the differences in domain (\\(D\\))."
  },
  {
    "objectID": "slides/03-slides.html#areal-data",
    "href": "slides/03-slides.html#areal-data",
    "title": "Introduction to Spatial Data",
    "section": "Areal Data",
    "text": "Areal Data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\\(D\\) is fixed domain of countable units\nTypically involve some aggregation"
  },
  {
    "objectID": "slides/03-slides.html#geostatistical-data",
    "href": "slides/03-slides.html#geostatistical-data",
    "title": "Introduction to Spatial Data",
    "section": "Geostatistical data",
    "text": "Geostatistical data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\n\nMitzi Morris\n\n\n\n\n\\(D\\) is a fixed subset of \\(\\mathbb{R}^d\\)\n\\(Z(\\mathbf{s})\\) could be observed at any location within \\(D\\).\nModels predict unobserved locations"
  },
  {
    "objectID": "slides/03-slides.html#point-patterns",
    "href": "slides/03-slides.html#point-patterns",
    "title": "Introduction to Spatial Data",
    "section": "Point patterns",
    "text": "Point patterns\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\\(D\\) is random; where \\(\\mathbf{s}\\) depicts the location of events\n\n\n\n\nBen-Said, M. Ecol Process 10, 56 (2021)."
  },
  {
    "objectID": "slides/03-slides.html#what-is-a-data-model",
    "href": "slides/03-slides.html#what-is-a-data-model",
    "title": "Introduction to Spatial Data",
    "section": "What is a data model?",
    "text": "What is a data model?\n\n\nData: a collection of discrete values that describe phenomena\nYour brain stores millions of pieces of data\nComputers are not your brain\n\nNeed to organize data systematically\nBe able to display and access efficiently\nNeed to be able to store and access repeatedly\n\nData models solve this problem"
  },
  {
    "objectID": "slides/03-slides.html#types-of-spatial-data-models",
    "href": "slides/03-slides.html#types-of-spatial-data-models",
    "title": "Introduction to Spatial Data",
    "section": "2 Types of Spatial Data Models",
    "text": "2 Types of Spatial Data Models\n\nRaster: grid-cell tessellation of an area. Each raster describes the value of a single phenomenon. More next week…\nVector: (many) attributes associated with locations defined by coordinates"
  },
  {
    "objectID": "slides/03-slides.html#the-vector-data-model",
    "href": "slides/03-slides.html#the-vector-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nVertices (i.e., discrete x-y locations) define the shape of the vector\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons\n\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#vectors-in-action",
    "href": "slides/03-slides.html#vectors-in-action",
    "title": "Introduction to Spatial Data",
    "section": "Vectors in Action",
    "text": "Vectors in Action\n\n\nUseful for locations with discrete, well-defined boundaries\nVery precise (not necessarily accurate)\n\n\n\nImage Source: QGIS User’s manual"
  },
  {
    "objectID": "slides/03-slides.html#vector-challenge",
    "href": "slides/03-slides.html#vector-challenge",
    "title": "Introduction to Spatial Data",
    "section": "Vector Challenge!",
    "text": "Vector Challenge!\nThe plot below includes examples of two of the three types of vector objects. Which ones are they?\n\nData Carpentry: Geospatial Concepts"
  },
  {
    "objectID": "slides/03-slides.html#the-raster-data-model",
    "href": "slides/03-slides.html#the-raster-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Raster Data Model",
    "text": "The Raster Data Model\n\n\n\n\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data",
    "href": "slides/03-slides.html#types-of-raster-data",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular: constant cell size; axes aligned with Easting and Northing\nRotated: constant cell size; axes not aligned with Easting and Northing\nSheared: constant cell size; axes not perpendicular\nRectilinear: cell size varies along a dimension\nCurvilinear: cell size and orientation dependent on the other dimension"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data-1",
    "href": "slides/03-slides.html#types-of-raster-data-1",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\nContinuous: numeric data representing a measurement (e.g., elevation, precipitation)\nCategorical: integer data representing factors (e.g., land use, land cover)"
  },
  {
    "objectID": "slides/03-slides.html#location-vs.-place",
    "href": "slides/03-slides.html#location-vs.-place",
    "title": "Introduction to Spatial Data",
    "section": "Location vs. Place",
    "text": "Location vs. Place\n\n\n\n\n\nPlace: an area having unique physical and human characteristics interconnected with other places\nLocation: the actual position on the earth’s surface\nSense of Place: the emotions someone attaches to an area based on experiences\nPlace is location plus meaning\n\n\n\n\n\n\nnominal: (potentially contested) place names\nabsolute: the physical location on the earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#describing-absolute-locations",
    "href": "slides/03-slides.html#describing-absolute-locations",
    "title": "Introduction to Spatial Data",
    "section": "Describing Absolute Locations",
    "text": "Describing Absolute Locations\n\nCoordinates: 2 or more measurements that specify location relative to a reference system\n\n\n\n\n\nCartesian coordinate system\norigin (O) = the point at which both measurement systems intersect\nAdaptable to multiple dimensions (e.g. z for altitude)\n\n\n\n\n\n\nCartesian Coordinate System"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe",
    "href": "slides/03-slides.html#locations-on-a-globe",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\n\n\nLatitude and Longitude"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe-1",
    "href": "slides/03-slides.html#locations-on-a-globe-1",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\nGlobal Reference Systems (GRS)\nGraticule: the grid formed by the intersection of longitude and latitude\nThe graticule is based on an ellipsoid model of earth’s surface and contained in the datum"
  },
  {
    "objectID": "slides/03-slides.html#global-reference-systems",
    "href": "slides/03-slides.html#global-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems\n\nThe datum describes which ellipsoid to use and the precise relations between locations on earth’s surface and Cartesian coordinates\n\n\nGeodetic datums (e.g., WGS84): distance from earth’s center of gravity\nLocal data (e.g., NAD83): better models for local variation in earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#global-reference-systems-1",
    "href": "slides/03-slides.html#global-reference-systems-1",
    "title": "Introduction to Spatial Data",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-extent",
    "href": "slides/03-slides.html#describing-location-extent",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: extent",
    "text": "Describing location: extent\n\n\nHow much of the world does the data cover?\nFor rasters, these are the corners of the lattice\nFor vectors, we call this the bounding box"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-resolution",
    "href": "slides/03-slides.html#describing-location-resolution",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: resolution",
    "text": "Describing location: resolution\n\n\n\n\nResolution: the accuracy that the location and shape of a map’s features can be depicted\nMinimum Mapping Unit: The minimum size and dimensions that can be reliably represented at a given map scale.\nMap scale vs. scale of analysis"
  },
  {
    "objectID": "slides/03-slides.html#projections",
    "href": "slides/03-slides.html#projections",
    "title": "Introduction to Spatial Data",
    "section": "Projections",
    "text": "Projections\n\n\n\n\nBut maps, screens, and publications are…\nProjections describe how the data should be translated to a flat surface\nRely on ‘developable surfaces’\nDescribed by the Coordinate Reference System (CRS)\n\n\n\n\n\n\nDevelopable Surfaces\n\n\n\n\nProjection necessarily induces some form of distortion (tearing, compression, or shearing)"
  },
  {
    "objectID": "slides/03-slides.html#coordinate-reference-systems",
    "href": "slides/03-slides.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nSome projections minimize distortion of angle, area, or distance\nOthers attempt to avoid extreme distortion of any kind\nIncludes: Datum, ellipsoid, units, and other information (e.g., False Easting, Central Meridian) to further map the projection to the GCS\nNot all projections have/require all of the parameters"
  },
  {
    "objectID": "slides/03-slides.html#the-orange-peel-analogy",
    "href": "slides/03-slides.html#the-orange-peel-analogy",
    "title": "Introduction to Spatial Data",
    "section": "The Orange Peel Analogy",
    "text": "The Orange Peel Analogy\n\n\nA datum is the choice of fruit to use. Is the earth an orange, a lemon, a lime, a grapefruit?\n\n\nA projection is how you peel your orange and then flatten the peel.\n\n\n\nSource: Data Carpentry: Geospatial Concepts"
  },
  {
    "objectID": "slides/03-slides.html#choosing-projections",
    "href": "slides/03-slides.html#choosing-projections",
    "title": "Introduction to Spatial Data",
    "section": "Choosing Projections",
    "text": "Choosing Projections\n\n\n\n\n\nEqual-area for thematic maps\nConformal for presentations\nMercator or equidistant for navigation and distance"
  },
  {
    "objectID": "slides/03-slides.html#geometries",
    "href": "slides/03-slides.html#geometries",
    "title": "Introduction to Spatial Data",
    "section": "Geometries",
    "text": "Geometries\n\n\n\nVectors store and aggregate the locations of a feature into a geometry\nMost vector operations require simple, valid geometries\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#valid-geometries",
    "href": "slides/03-slides.html#valid-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Valid Geometries",
    "text": "Valid Geometries\n\nA linestring is simple if it does not intersect\nValid polygons:\n\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line) - For multipolygons, adjacent polygons touch only at points\nDo not repeat their own path"
  },
  {
    "objectID": "slides/03-slides.html#empty-geometries",
    "href": "slides/03-slides.html#empty-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/03-slides.html#support",
    "href": "slides/03-slides.html#support",
    "title": "Introduction to Spatial Data",
    "section": "Support",
    "text": "Support\n\nSupport is the area to which an attribute applies.\n\n\n\nFor vectors, the attribute-geometry relationship can be:\nconstant = applies to every point in the geometry (lines and polygons are just lots of points)\nidentity = a value unique to a geometry\naggregate = a single value that integrates data across the geometry\nRasters can have point (attribute refers to the cell center) or cell (attribute refers to an area similar to the pixel) support"
  },
  {
    "objectID": "slides/03-slides.html#types-of-support-for-vectors",
    "href": "slides/03-slides.html#types-of-support-for-vectors",
    "title": "Introduction to Spatial Data",
    "section": "Types of support for vectors",
    "text": "Types of support for vectors\n\n\nGive an example of:\n\nconstant support\nidentity support\naggregate support"
  },
  {
    "objectID": "slides/03-slides.html#spatial-messiness",
    "href": "slides/03-slides.html#spatial-messiness",
    "title": "Introduction to Spatial Data",
    "section": "Spatial Messiness",
    "text": "Spatial Messiness\n\nQuantitative geography requires that our data are aligned\nAchieving alignment is part of reproducible workflows\nMaking principled decisions about projections, resolution, extent, etc"
  },
  {
    "objectID": "example/session-10-example.html",
    "href": "example/session-10-example.html",
    "title": "Session 10 Code",
    "section": "",
    "text": "Load libraries\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\n\n\n\nCentroids vs Point on Surface\n\n\nCode\nid.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE)\nid.centroid &lt;- st_centroid(id.counties)\nid.pointonsurf &lt;- st_point_on_surface(id.counties)\n\n\n\n\nCode\nplot(st_geometry(id.counties))\nplot(st_geometry(id.centroid), col=\"blue\", add=TRUE)\nplot(st_geometry(id.pointonsurf), col=\"red\", add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nPractice Example 1: Distance on Points and Polygons\n\n\nCode\nsystem.time(poly_dist &lt;- st_distance(id.counties))\n\n\n   user  system elapsed \n   4.32    0.03    4.36 \n\n\nCode\nsystem.time(cent_dist &lt;- st_distance(id.centroid))\n\n\n   user  system elapsed \n   0.00    0.00    0.01 \n\n\nCode\nsystem.time(pos_dist &lt;- st_distance(id.pointonsurf))\n\n\n   user  system elapsed \n   0.02    0.00    0.00 \n\n\n\n\nPractice Example 2: Intersections and Buffers\n\n\nCode\n# get roads data\nroads &lt;- tigris::primary_secondary_roads(\"ID\", progress_bar=FALSE)\n\n# get a polygon of Ada county\nada.cty &lt;- filter(id.counties, NAME == \"Ada\")\n\n# find all road sections within Ada county\nada.roads &lt;- st_intersection(roads, ada.cty)\n\n\n\n\nCode\n# plot result\nplot(st_geometry(ada.cty))\nplot(st_geometry(ada.roads), col=\"purple\", add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the units of the CRS\nst_crs(id.centroid)\n\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nCode\n# create a 50km buffer around the centroid of Ada county\nada.cent &lt;- filter(id.centroid, NAME == \"Ada\")\nada.buff &lt;- st_buffer(ada.cent, dist = 50000)\n\n\n\n\nCode\n# get roads within buffer zone\nroads.buff &lt;- st_intersection(roads, ada.buff)\n\n\n\n\nCode\n# plot result\nplot(st_geometry(roads.buff))\n\n\n\n\n\n\n\n\n\nIf we want to plot county boundaries, st_intersection won’t work. We’ll only get parts of each county polygon:\n\n\nCode\ncty.buff &lt;- st_intersection(id.counties, ada.buff)\nplot(st_geometry(cty.buff))\n\n\n\n\n\n\n\n\n\nBack to predicates!\n\n\nCode\n# find counties that intersect with buffer\ncty.50 &lt;- st_intersects(id.counties, ada.buff, sparse = FALSE)\n# convert to vector so filter is happy\ncty.50 &lt;- as.vector(cty.50)\n\n# get counties that intersect buffer\nmap.counties &lt;- filter(id.counties, cty.50)\n\n\n\n\nCode\n# plot result\nplot(map.counties$geometry)\nplot(st_geometry(roads.buff), col=\"purple\", add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations I"
    ]
  },
  {
    "objectID": "slides/13-slides.html#objectives",
    "href": "slides/13-slides.html#objectives",
    "title": "Raster Data: II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "slides/13-slides.html#why-use-moving-windows",
    "href": "slides/13-slides.html#why-use-moving-windows",
    "title": "Raster Data: II",
    "section": "Why use moving windows?",
    "text": "Why use moving windows?\n\nTo create new data that reflects “neighborhood” data\nTo smooth out values\nTo detect (and fill) holes or edges\nChange the thematic scale of your data (without changing resolution)"
  },
  {
    "objectID": "slides/13-slides.html#what-is-a-moving-window",
    "href": "slides/13-slides.html#what-is-a-moving-window",
    "title": "Raster Data: II",
    "section": "What is a moving window?",
    "text": "What is a moving window?"
  },
  {
    "objectID": "slides/13-slides.html#implementing-moving-windows-in-r",
    "href": "slides/13-slides.html#implementing-moving-windows-in-r",
    "title": "Raster Data: II",
    "section": "Implementing Moving Windows in R",
    "text": "Implementing Moving Windows in R\n\nUse the focal function in terra\n\nfocal(x, w=3, fun=\"sum\", ..., na.policy=\"all\", fillvalue=NA,          expand=FALSE, silent=TRUE, filename=\"\", overwrite=FALSE, wopt=list())"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters",
    "href": "slides/13-slides.html#focal-for-continuous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(spData)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-1",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-1",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-2",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-2",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-3",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-3",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continous-rasters",
    "href": "slides/13-slides.html#focal-for-continous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continous Rasters",
    "text": "focal for Continous Rasters\n\ncan alter the size and shape of window by providing a weights matrix for w\nCan create different custom functions for fun (see the help file)\nna.policy for filling holes or avoiding them"
  },
  {
    "objectID": "slides/13-slides.html#reclassification-1",
    "href": "slides/13-slides.html#reclassification-1",
    "title": "Raster Data: II",
    "section": "Reclassification",
    "text": "Reclassification\n\nCreate new data based on the presence of a particular class(es) of interest\nCombine classes in a categorical map\nUseful as inputs for overlay analyses"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-rasters-in-r",
    "href": "slides/13-slides.html#reclassifying-rasters-in-r",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-rasters-in-r-1",
    "href": "slides/13-slides.html#reclassifying-rasters-in-r-1",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-categorical-rasters",
    "href": "slides/13-slides.html#reclassifying-categorical-rasters",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters\n\nNeed a classification matrix\nUse classify\n\n\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-categorical-rasters-1",
    "href": "slides/13-slides.html#reclassifying-categorical-rasters-1",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters"
  },
  {
    "objectID": "slides/13-slides.html#raster-math",
    "href": "slides/13-slides.html#raster-math",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nPerforms cell-wise calculations on 1 (or more) SpatRasters\nGenerally works the same as matrix operations\nAll layers must be aligned"
  },
  {
    "objectID": "slides/13-slides.html#raster-math-1",
    "href": "slides/13-slides.html#raster-math-1",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nr &lt;- rast(ncol=5, nrow=5)\nvalues(r) &lt;- 1:ncell(r)\nr2 &lt;- r*2\nr3 &lt;- t(r)\nr4 &lt;- r + r2"
  },
  {
    "objectID": "slides/13-slides.html#cell-wise-operations",
    "href": "slides/13-slides.html#cell-wise-operations",
    "title": "Raster Data: II",
    "section": "Cell-wise operations",
    "text": "Cell-wise operations\n\nterra has a special set of apply functions\napp, lapp, tapp\napp applies a function to the values of each cell\nlapp applies a function using the layer as the value\ntapp applies the function to a subset of layers"
  },
  {
    "objectID": "slides/13-slides.html#context-specific-functions",
    "href": "slides/13-slides.html#context-specific-functions",
    "title": "Raster Data: II",
    "section": "Context-specific Functions",
    "text": "Context-specific Functions\n\ndistance and relatives are based on relationships between cells\nterrain allows calculation of slope, ruggedness, aspect using elevation rasters\nshade calculates hillshade based on terrain"
  },
  {
    "objectID": "slides/13-slides.html#practice",
    "href": "slides/13-slides.html#practice",
    "title": "Raster Data: II",
    "section": "Practice",
    "text": "Practice\nYou are tasked with the following (admittedly silly) analysis by the “Wildfire for Insurance Agents” group. They would like a map of wildfire risk in Idaho with these categories: “No worries,” “A little bit risky,” “Moderate risk,” and “Very risky,” and “Don’t move here.” They don’t want any stakeholders to feel that their properties stand out unfairly, so they don’t want pixels that are different from the pixels around them. Choose a county to present this analysis to and make a map for that audience as well as the state map."
  },
  {
    "objectID": "slides/13-slides.html#objectives-1",
    "href": "slides/13-slides.html#objectives-1",
    "title": "Raster Data: II",
    "section": "Objectives",
    "text": "Objectives\n\nYou should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "slides/01-slides.html#todays-plan",
    "href": "slides/01-slides.html#todays-plan",
    "title": "Getting Started",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nIntroductions\nWhy (not) R?\nCourse logistics and resources\nTesting out RStudio, git, and GitHub Classroom"
  },
  {
    "objectID": "slides/01-slides.html#about-me",
    "href": "slides/01-slides.html#about-me",
    "title": "Getting Started",
    "section": "About Me",
    "text": "About Me\n\n\n\n\nWhat I do\nMy path to this point\nWhy I teach this course"
  },
  {
    "objectID": "slides/01-slides.html#what-about-you",
    "href": "slides/01-slides.html#what-about-you",
    "title": "Getting Started",
    "section": "What about you?",
    "text": "What about you?\n\n\n\nYour preferred pronouns\nWhere are you from?\nWhat do you like most about Boise?\nWhat do you miss most about “home”?\nWhat is your research?"
  },
  {
    "objectID": "slides/01-slides.html#why-r",
    "href": "slides/01-slides.html#why-r",
    "title": "Getting Started",
    "section": "Why R?",
    "text": "Why R?\n\n\n\n\nOpen Source\nHuge user community\nIntegrated analysis pipelines\nReproducible workflows\n\n\n\n\nCodePlot\n\n\n\nlibrary(maps)\nlibrary(socviz)\nlibrary(tidyverse)\nparty_colors &lt;- c(\"#2E74C0\", \"#CB454A\") \nus_states &lt;- map_data(\"state\")\nelection$region &lt;- tolower(election$state)\nus_states_elec &lt;- left_join(us_states, election)\np0 &lt;- ggplot(data = us_states_elec,\n             mapping = aes(x = long, y = lat,\n                           group = group, \n                           fill = party))\np1 &lt;- p0 + geom_polygon(color = \"gray90\", \n                        size = 0.1) +\n    coord_map(projection = \"albers\", \n              lat0 = 39, lat1 = 45) \np2 &lt;- p1 + scale_fill_manual(values = party_colors) +\n    labs(title = \"Election Results 2016\", \n         fill = NULL)"
  },
  {
    "objectID": "slides/01-slides.html#why-not-r-1",
    "href": "slides/01-slides.html#why-not-r-1",
    "title": "Getting Started",
    "section": "Why not R?",
    "text": "Why not R?\n\n## ---\n## Error: could not find function \"performance\"\n## ---\n##  [1] \"Error in if (str_count(string = f[[j]], \n##  pattern = \\\"\\\\\\\\S+\\\") == 1) \n##  { : \\n  argument is of length zero\"   \n## ---\n## Error in eval(expr, envir, enclos) : object 'x' not found\n## ---\n## Error in file(file, \"rt\") : cannot open the connection\n## ---\n\n\n\n\nCoding can be hard…\nMemory challenges\n\n\n\nSpeed\nDecision fatigue"
  },
  {
    "objectID": "slides/01-slides.html#getting-help",
    "href": "slides/01-slides.html#getting-help",
    "title": "Getting Started",
    "section": "Getting Help",
    "text": "Getting Help\n\n\n\nGoogle it!!\n\nUse the exact error message\nInclude the package name\ninclude “R” in the search\n\n\n\n\nStack Overflow\n\nReproducible examples\n\nPackage “issue” pages\nr_spatial slack channel\nCommon errors\n\n\n\nAsk Me"
  },
  {
    "objectID": "slides/01-slides.html#logistics",
    "href": "slides/01-slides.html#logistics",
    "title": "Getting Started",
    "section": "Logistics",
    "text": "Logistics\n\n\nMeet on Mondays and Wednesdays\n~55 min lecture, 20 min practice\n4 major sections\nReadings"
  },
  {
    "objectID": "slides/01-slides.html#course-webpage",
    "href": "slides/01-slides.html#course-webpage",
    "title": "Getting Started",
    "section": "Course Webpage",
    "text": "Course Webpage\nhttps://isdrfall24.classes.spaseslab.com/\n\n\nSyllabus\nSchedule\nLectures\nAssignments\nResources"
  },
  {
    "objectID": "slides/01-slides.html#assignments",
    "href": "slides/01-slides.html#assignments",
    "title": "Getting Started",
    "section": "Assignments",
    "text": "Assignments\n\nCheck out the syllabus for more on grading!\n\n\n\n\n\n\nSelf-reflections (2x)\n\nYour goals for the course\nEvaluation criteria\n\nCoding exercises (10x)\n\nProblem solving\nReproducible workflows\nMuscle memory\n\n\n\n\nCode Revisions (3x)\n\nDigging deeper\nCommon issues\nMore extensive feedback\n\nFinal project (1st draft, final draft)\n\nPractice a full analysis workflow\nIntegrate analysis & visuals to tell a story"
  },
  {
    "objectID": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "href": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "title": "Getting Started",
    "section": "Orientation to RStudio and our RStudio server",
    "text": "Orientation to RStudio and our RStudio server"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git",
    "href": "slides/01-slides.html#introduce-yourself-to-git",
    "title": "Getting Started",
    "section": "Introduce yourself to Git",
    "text": "Introduce yourself to Git\n\nLots of ways, but one easy way is:\n\n\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\") #your info here\n\n\nGenerate a PAT token if you don’t have one (make sure you save it somewhere)\n\n\nusethis::create_github_token()"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "href": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "title": "Getting Started",
    "section": "Introduce yourself to Git (cont’d)",
    "text": "Introduce yourself to Git (cont’d)\n\nStore your credentials for use (times out after 1 hr)\n\n\ngitcreds::gitcreds_set()\n\n\nVerify\n\n\ngitcreds::gitcreds_get()"
  },
  {
    "objectID": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "href": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "title": "Getting Started",
    "section": "Joining the assignment and cloning the repo",
    "text": "Joining the assignment and cloning the repo\n\nClick this link\nBring the project into RStudio\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\nPaste the link from the “Clone Repository” button into the “Repository URL” space"
  },
  {
    "objectID": "slides/01-slides.html#the-git-workflow",
    "href": "slides/01-slides.html#the-git-workflow",
    "title": "Getting Started",
    "section": "The git workflow",
    "text": "The git workflow\n\nMake sure to pull every time you start working on a project\nMake some changes to code\nSave those changes\nCommit your changes\nPush your work to the remote!"
  },
  {
    "objectID": "slides/01-slides.html#checking-in",
    "href": "slides/01-slides.html#checking-in",
    "title": "Getting Started",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis?\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "assignment/04-mapssolutions.html",
    "href": "assignment/04-mapssolutions.html",
    "title": "Assignment 4 Solutions: Predicates and Measures",
    "section": "",
    "text": "1. Load the cejst_nw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code). \n\nRemember that predicates return logical (i.e. TRUE or FALSE) answers so we are looking for functions with st_is_* to look for valid or empty geometries. We wrap those in the all() or any() function calls so that we get a single TRUE or FALSE for the entire geometry collection rather than returning the value for each individual observation. While those can be useful for figuring out if the entire dataset meets our criteria (i.e., all are valid or any have empty geometries), identifying which records have empty geometries takes an additional step. We use which() to return the row index of each record that returns a TRUE for st_is_empty() and then subset the original data using the [] notation keeping only the rows with empty geometries and all other columns.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\n\ncejst.nw &lt;- read_sf(\"opt/data/data/assignment04/cejst_nw.shp\")\nall(st_is_valid(cejst.nw))\nany(st_is_empty(cejst.nw))\nwhich(st_is_empty(cejst.nw))\n\ncejst.nw[which(st_is_empty(cejst.nw)),]\n\n 2. Load the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset… \n\nHere we are interested in distance which is a measure (not a predicate or transformer), but to get there we need to take a few extra steps. First, we read in the csv file and convert it to coordinates (using st_as_sf, a transformer). Then we use dplyr::filter to retain only the hospitals in the dataset. Finally, because this is a lat/long dataset, we assume a geodetic projection of WGS84 and assign it to the filtered object. Once we’ve gotten all that squared away, it’s just a matter of using the st_distance function to return the distance matrix for all objects in the dataset.\n\n\nhospitals.id &lt;- read_csv(\"opt/data/data/assignment04/landmarks_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4326\n\ndist.hospital &lt;- st_distance(hospitals.id)\n\ndist.hospital[1:5, 1:5]\n\n 3. Filter the cejst_nw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\n\nThis one should be relatively straightforward. We start with another call to dplyr::filter to get down to just the tracts in Ada County (note the use of the & to combine two logical calls). Then we use a second filter to return the row with the max value for agricultural loss. Note that we have to use the na.rm=TRUE argument to avoid having the NA values force the function to return NA.\n\n\nada.cejst &lt;- cejst.nw %&gt;% \n  filter(., SF == \"Idaho\" & CF == \"Ada County\") \n\nada.max.EALR &lt;- ada.cejst %&gt;%  \n  filter(., EALR_PFS == max(EALR_PFS, na.rm = TRUE))\n  \nada.max.EALR[, c(\"SF\", \"CF\", \"EALR_PFS\")]\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1998 ymin: 43.11297 xmax: -115.9742 ymax: 43.59134\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 4\n  SF    CF         EALR_PFS                                             geometry\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Idaho Ada County      0.7 (((-116.1371 43.55217, -116.137 43.55222, -116.1369…\n\n\n 4. Finally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\nWe can access the helpfile for adjacent by using ?terra::adjacent (I won’t do that here because I don’t want to print the entire helpfile). From that we can see that the cells argument is the place to specify which cells we are interested in. also see that the directions argument allows us to specify whether we want “rook”, “bishop”, or “queen” neighbors. Finally, we see that if we want to exclude the focal cell itself, we have to set include to FALSE. By plotting the map with the cell numbers, we can see that cells 1 and 5 are on th top row of the raster and thus do not have any neighbors for for the upper 3 categories whereas cell 55 has all 8 neighbors. If you choose cells that are in the center of the raster, you get all neighbors\n\n\nr &lt;- rast(nrows=10, ncols=10)\ncellnum &lt;- cells(r)\nr[] &lt;- cellnum\nplot(r)\n\n\n\n\n\n\n\nadjacent(r, cells=c(1, 5, 55), directions=\"queen\", include=FALSE)\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n0   NaN  NaN  NaN   10    2   20   11   12\n4   NaN  NaN  NaN    4    6   14   15   16\n54   44   45   46   54   56   64   65   66\n\nadjacent(r, cells=c(51, 52, 55), directions=\"queen\", include=FALSE)\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n50   50   41   42   60   52   70   61   62\n51   41   42   43   51   53   61   62   63\n54   44   45   46   54   56   64   65   66"
  },
  {
    "objectID": "assignment/01-introsolutions.html",
    "href": "assignment/01-introsolutions.html",
    "title": "Assignment 1 Solutions: Introductory material",
    "section": "",
    "text": "How does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\n\nMy research is based almost entirely on geographic analysis, between estimating ecosystem services and studying agricultural change through time in different Idaho locations. One issue I face is that I integrate ecosystem services data collected and estimated at multiple scales into one analysis. I have to be careful I don’t fall into any pitfalls or fallacies when I combine multiple sources of data. In other cases, I have county level data and have to acknowledge the measurement bias that comes with data collection along those policial boundaries.\n\nWhat are the primary components that describe spatial data?\n\nI would say that the primary components are the coordinate reference system (because it helps us understand where we actually are on Earth), the extent of the data (because that helps me know what scale we’re working with and the size of the computational problem), the resolution (same reason as extent), the geometry, and spatial support. I don’t think about this last one often enough, but it really is the key to honest interpretation of the spatial data that you have.\n\nWhat is the coordinate reference system and why is it important\n\nThe CRS consists of the information necessary to locate points in 2 or 3 dimensional space. Coordinates are only meaningful in the context of a CRS (i.e., (2,2) could describe any number of places in the world - we need to know the origin and the datum to actually know where that is). The CRS becomes particularly important when we need to align datasets that were not collected in the same CRS originally or when we need to transfer locations from the globe to a flat surface (e.g., map, screen, etc).\n\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\n\nHere’s a fun article on projections that shows what I’m talking about!\n\nRead in the cejst_nw.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\nI can read in the data using st_read or read_sf\n\nlibrary(sf)\n\ncejst.sf &lt;- read_sf(\"/opt/data/data/assignment01/cejst_nw.shp\")\ncejst.st &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\nYou can inspect the differences between the resulting object classes by calling class\n\n```{r}\n#| message: false\n\nclass(cejst.sf)\nclass(cejst.st)\n```\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n[1] \"sf\"         \"data.frame\"\n\n\nYou’ll notice that using st_read assigns the object to an sf and data.frame class meaning that functions defined for those two classes will work. Alternatively, read_sf assigns the object to sf, tbl_df, tbl, and data.frame classes meaning that a much broader set of functions can be run on the cejst.sf object.\nBecause the data are in wide format, we can assume that there is only 1 observation for each location (because sf requires that there is a geometry entry for every observation (even if it’s empty)). Probably the easiest way to get the number of observations is:\n\n```{r}\nnrow(cejst.sf)\n```\n\n[1] 2590\n\n\nSimilarly, if we wanted to know how many attributes are collected for each observation we could use ncol:\n\n```{r}\nncol(cejst.sf)\n```\n\n[1] 124\n\n\nNote that these are really only approximate estimates. There’s usually a lot of extra ID-style columns in spatial data such that the number of columns with useful information is less than the total number of columns, but we won’t worry about that for now."
  },
  {
    "objectID": "example/session-11-example.html",
    "href": "example/session-11-example.html",
    "title": "Session 11 Code",
    "section": "",
    "text": "Code for questions 1 and 2 can be found in Session 11, in both the slides and the Panopto recording.",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations II"
    ]
  },
  {
    "objectID": "example/session-11-example.html#question-3",
    "href": "example/session-11-example.html#question-3",
    "title": "Session 11 Code",
    "section": "Question 3",
    "text": "Question 3\n\nWhat do we need to know?\n\n\n\nPseudocode\n\n\n\nStep 1: Get cdc data and project if necessary\nWe did this in the previous questions – the dataset is cdc.idaho.\n\n\nStep 2: Get hospital data and project if necessary\nWe did this in the previous questions – the dataset is hospital.sf.proj.\n\n\nStep 3: Buffer service areas around hospitals\nWe did this in the previous questions – the dataset is hospital.buf.\n\n\nStep 4: Find intersections of service areas\nWe did this in the previous questions – the dataset is hospital.int.overlaps. However, we re-projected it, so now we need to project cdc.idaho to the same projection. A plot shows that they are aligned.\n\n\nCode\ncdc.idaho.proj &lt;- st_transform(cdc.idaho, crs=st_crs(hospital.int.overlaps))\n\nplot(st_geometry(cdc.idaho.proj), col=\"purple\")\nplot(st_geometry(hospital.int.overlaps), add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Find tracts that overlap those intersections\n\n\nCode\noverlap.tracts.matrix &lt;- st_intersects(cdc.idaho.proj, hospital.int.overlaps, sparse = FALSE)\n\noverlap.tracts.matrix[1:12, 1:5]\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]\n [1,] FALSE FALSE FALSE FALSE FALSE\n [2,] FALSE FALSE FALSE FALSE FALSE\n [3,] FALSE FALSE FALSE FALSE FALSE\n [4,] FALSE FALSE FALSE FALSE FALSE\n [5,] FALSE FALSE  TRUE FALSE FALSE\n [6,] FALSE FALSE  TRUE FALSE FALSE\n [7,] FALSE FALSE  TRUE FALSE FALSE\n [8,] FALSE FALSE  TRUE FALSE FALSE\n [9,] FALSE FALSE  TRUE FALSE FALSE\n[10,] FALSE FALSE  TRUE FALSE FALSE\n[11,] FALSE FALSE  TRUE FALSE FALSE\n[12,] FALSE FALSE  TRUE FALSE FALSE\n\n\nThis creates a logical matrix where each row corresponds to a tract, and the cells in the matrix show whether it overlaps with each overlap area (TRUE) or whether it does not (FALSE). The cool thing about logicals is that they also count as numbers (TRUE = 1, FALSE = 0). By finding the rowSums, we can see which tracts overlap with 2+ hospital areas (rowSum &gt;= 1) and which don’t (rowSum = 0).\n\n\nCode\noverlap.tracts.filter &lt;- rowSums(overlap.tracts.matrix)\n\noverlap.tracts &lt;- cdc.idaho.proj[overlap.tracts.filter&gt;=1, ]\n\nplot(st_geometry(overlap.tracts), col=\"cornflowerblue\")\nplot(st_geometry(hospital.int.overlaps), add=TRUE, col=\"orange\")\n\n\n\n\n\n\n\n\n\nFor an alternate tidyverse integration, see this Stack Overflow question.\n\n\nStep 6: Find tracts outside of service buffers\nWe’ll use the same process as step 5, but this time we’ll keep the rowSums that are equal to 0.\n\n\nCode\nnohosp.tracts.matrix &lt;- st_intersects(cdc.idaho.proj, hospital.buf, sparse = FALSE)\nnohosp.tracts.matrix[1:12, 1:5]\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]\n [1,] FALSE FALSE FALSE FALSE FALSE\n [2,] FALSE FALSE FALSE FALSE FALSE\n [3,] FALSE FALSE FALSE FALSE FALSE\n [4,] FALSE FALSE FALSE FALSE FALSE\n [5,] FALSE FALSE  TRUE FALSE FALSE\n [6,] FALSE FALSE  TRUE FALSE FALSE\n [7,] FALSE FALSE  TRUE FALSE FALSE\n [8,] FALSE FALSE  TRUE FALSE FALSE\n [9,] FALSE FALSE  TRUE FALSE FALSE\n[10,] FALSE FALSE  TRUE FALSE FALSE\n[11,] FALSE FALSE  TRUE FALSE FALSE\n[12,] FALSE FALSE  TRUE FALSE FALSE\n\n\nCode\nnohosp.tracts.filter &lt;- rowSums(nohosp.tracts.matrix)\n\nnohosp.tracts &lt;- cdc.idaho.proj[nohosp.tracts.filter==0, ]\n\nplot(st_geometry(nohosp.tracts), col=\"firebrick\")\nplot(st_geometry(hospital.buf), add=TRUE, col=\"orange\")\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Calculate average chronic heart disease rate for both groups of tracts\n\n\nCode\navg.nohosp.rate &lt;- mean(nohosp.tracts$chd_crudep)\n\navg.overlaps.rate &lt;- mean(overlap.tracts$chd_crudep)\n\n\n\n\nStep 8: Find the difference\n\n\nCode\navg.overlaps.rate - avg.nohosp.rate\n\n\n[1] 0.36875\n\n\nThe rates of chronic heart disease are on average higher in tracts with multiple hospitals than those with no hospitals. Maybe there’s less access to a diagnosis, or heart disease is more fatal to people with less hospital access…?",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations II"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html",
    "href": "lesson/vector_intro.html",
    "title": "Intro to Vector Data",
    "section": "",
    "text": "Today we’ll build on the introductory discussion we were having about vector operations and the sf package. We’ll build a few vectors from scratch and then move on to explore a broader suite of common vector operations implemented by the sf package.",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "href": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "title": "Intro to Vector Data",
    "section": "A reminder about vector geometries in R",
    "text": "A reminder about vector geometries in R\nYou’ll recall that the sf package organizes the different types of vectors (e.g., points, lines, polygons) in to a hierarchical structure organized by complexity of geometries contained within an R object. For example, a single point will be a POINT, several points will be a MULTIPOINT, and an object containing points, polygons, and lines will be a GEOMETRYCOLLECTION. We need to be aware of what types of geometries and objects we have becasue some operations are restricted to particular types of objects or geometries as indicated by errors that read:\nError in UseMethod(\"st_crs&lt;-\") :    no applicable method for 'st_crs&lt;-' applied to an object of class \"c('XY', 'POINT', 'sfg')\"\nwhich indicates that the function (st_crs) does not have a method defined for the type of object it’s being applied to. Note that the function inside UseMethod will be replaced by whichever function you’re attempting to apply to your object and the object of class component will vary based on the function and the object class.\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\nAs is, these geometries are built on vertices with coordinates that are based on the Cartesian plane and thus are “spatial”, but not georeferenced or geographic. In order to convert these sf geometries to a geogrphic object (i.e., one with a CRS and whose location depicts and actual spot on the earth’s surface), we use st_sfc() to create a simple feature geography list column (see ?st_sfc for an example of this workflow).",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Conventions in sf and the tidyverse",
    "text": "Conventions in sf and the tidyverse\nOne of the benefits of the sf package is that it is designed to interface with the tidyverse suite of packages. One of the appealing parts of working with tidyverse packages is that they share an underlying philosophy, data structure, and grammar. This can make life a lot easier as you move from getting your data into R, constructing a set of covariates (including those derived from spatial data), analyzing, and plotting (or mapping) those data. People have strong opinions about the tidyverse, but I find it to be an (eventually) useful way for people to gain some intuition for working in R. One of the grammatical conventions used in the tidyverse suite of packages is the use _ in function calls (this is known as snake case should you ever need to know that at a dinner party). The _ is typically used to separate the verb in a function call from its predicate. For example, bind_rows() in the dplyr package “binds” (the verb) rows (the predicate) wheras bind_cols() binds columns. For the sf package it’s slightly different in that most of the functions begin with a st_ or sf_ prefix to indicate that the function is designed to work on spatial objects followed by a word (or words) describing what the operation does (e.g., st_centroid() returns a MULTIPOINT object with each point located at the centroid of a polygon). We can classify these functions based on what they are expected to return:\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\nWe can also distinguish these functions based on how many geometries that operate on:\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries\n\nWe’ll focus on the unary operators for now, but the binary and n-ary operators will become more important as we move to develop databases for spatial analysis.\n\nUnary predicates\nUnary predicates are helpful ‘checks’ to make sure the object you are working with has the properties you might expect. Are the geometries valid? Is the data projected? Because we are asking a set of TRUE/FALSE questions, these functions are specified as st_is_:\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nsimple\nis the geometry self-intersecting (i.e., simple)?\n\n\nvalid\nis the geometry valid?\n\n\nempty\nis the geometry column of an object empty?\n\n\nlonglat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?\n\n\n\n\n\nCode\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\n\nReading layer `nc' from data source \n  `C:\\Users\\carolynkoehn\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.3\\x86_64-w64-mingw32\\sf\\1.0-16\\ad57b543f7c3fca05213ba78ff63df9b\\sf\\shape\\nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nCode\nst_is_longlat(nc)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(nc)\n\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n\nUnary measures\nMeasures return a quantity that describes the geometry\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\ndistance is a binary measure that returns the distance between pairs of geometries either within a single object or between features in multiple objects\n\n\nCode\nhead(st_area(nc))\n\n\nUnits: [m^2]\n[1] 1137107793  610916077 1423145355  694378925 1520366979  967504822\n\n\nCode\nst_distance(nc)[1:5,1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]      [,4]      [,5]\n[1,]      0.0      0.0  25591.8 439493.26 299049.94\n[2,]      0.0      0.0      0.0 408416.68 268284.09\n[3,]  25591.8      0.0      0.0 366648.94 226461.23\n[4,] 439493.3 408416.7 366648.9      0.00  67066.43\n[5,] 299049.9 268284.1 226461.2  67066.43      0.00\n\n\n\n\nUnary transformers\nUnary transformations work on a per object basis and return a new geometry for each geometry. These are a few of the most common, we’ll encounter a few more as the semester continues.\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is this larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (last week)\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n\n\n\nCode\nplot(st_geometry(nc))\nplot(st_geometry(st_centroid(nc)), add=TRUE, col='red')\n\n\nWarning: st_centroid assumes attributes are constant over geometries",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Using sf and the tidyverse",
    "text": "Using sf and the tidyverse\nAs I mentioned, one of the benefits of using the sf package is that commands from the other tidyverse package have defined methods for spatial objects. The dplyr package has a ton of helpful functions for maniputlating data in R. For example, we might select a single row from a shapefile based on the value of its attributes by using the dplyr::filter() command:\n\n\nCode\ndurham.cty &lt;- nc %&gt;% \n  filter(., NAME == \"Durham\")\n## We can also use the bracket approach\ndurham.cty2 &lt;- nc[nc$NAME == \"Durham\",]\n\nplot(st_geometry(nc))\nplot(st_geometry(durham.cty), add=TRUE, col=\"blue\")\n\n\n\n\n\n\n\n\n\nOr perhaps we only want a few of the columns in the dataset (because shapefiles always have lots of extra stuff). We can use dplyr::select() to choose columns by name:\n\n\nCode\nnc.select &lt;- nc %&gt;% \n  select(., c(\"CNTY_ID\", \"NAME\", \"FIPS\"))\nplot(nc.select)\n\n\n\n\n\n\n\n\n\nNotice that the geometries are sticky, this will be important later",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "slides/02-slides.html#checking-in",
    "href": "slides/02-slides.html#checking-in",
    "title": "Why Geographic Analysis",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "slides/02-slides.html#todays-plan",
    "href": "slides/02-slides.html#todays-plan",
    "title": "Why Geographic Analysis",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWhat can we do with geographic information?\nConceptual challenges\nAnalytical challenges\nCritiques of quantitative geography"
  },
  {
    "objectID": "slides/02-slides.html#what-is-geography",
    "href": "slides/02-slides.html#what-is-geography",
    "title": "Why Geographic Analysis",
    "section": "What is geography",
    "text": "What is geography\n\nGeo: land, earth, terrain\nGraph: writing, discourse\nTuan: Space (extent) and Place (location)\nAnalysis of the effects of extent and location on events or features"
  },
  {
    "objectID": "slides/02-slides.html#five-themes-in-geography",
    "href": "slides/02-slides.html#five-themes-in-geography",
    "title": "Why Geographic Analysis",
    "section": "Five Themes in Geography",
    "text": "Five Themes in Geography\n\n\n\nLocation\nPlace\nRegion\nMovement\nHuman-Environment Interaction\n\n\n\n\n\nWGBH Educational Foundation"
  },
  {
    "objectID": "slides/02-slides.html#location",
    "href": "slides/02-slides.html#location",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#location-1",
    "href": "slides/02-slides.html#location-1",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#place",
    "href": "slides/02-slides.html#place",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForest cover map by Robert Simmons via Wikimedia Commons; temp map from GISgeography.com"
  },
  {
    "objectID": "slides/02-slides.html#place-1",
    "href": "slides/02-slides.html#place-1",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?"
  },
  {
    "objectID": "slides/02-slides.html#region",
    "href": "slides/02-slides.html#region",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#region-1",
    "href": "slides/02-slides.html#region-1",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#movement",
    "href": "slides/02-slides.html#movement",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape."
  },
  {
    "objectID": "slides/02-slides.html#movement-1",
    "href": "slides/02-slides.html#movement-1",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth maps courtesy of High Country News"
  },
  {
    "objectID": "slides/02-slides.html#human-environment-interactions",
    "href": "slides/02-slides.html#human-environment-interactions",
    "title": "Why Geographic Analysis",
    "section": "Human-Environment Interactions",
    "text": "Human-Environment Interactions\nHow do people relate to and change the physical world to meet their needs?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmoke map courtesy of Capital Public Radio; Nightlights courtesy of NASA Earth Observatory."
  },
  {
    "objectID": "slides/02-slides.html#description",
    "href": "slides/02-slides.html#description",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nCoordinates\nDistances\nNeighbors\nSummary statistics\n\n\n\n\n\n\ncourtesy of innovative GIS"
  },
  {
    "objectID": "slides/02-slides.html#description-1",
    "href": "slides/02-slides.html#description-1",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nRange Maps\nHotspots\nIndices"
  },
  {
    "objectID": "slides/02-slides.html#explanation-and-inference",
    "href": "slides/02-slides.html#explanation-and-inference",
    "title": "Why Geographic Analysis",
    "section": "Explanation and Inference",
    "text": "Explanation and Inference\n\nCognitive Description: collection ordering and classification of data\nCause and Effect: design-based or model-based testing of the factors that give rise to geographic distributions\nSystems Analysis: describes the entire complex set of interactions that structure an activity"
  },
  {
    "objectID": "slides/02-slides.html#prediction",
    "href": "slides/02-slides.html#prediction",
    "title": "Why Geographic Analysis",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\nExtend description or explanation into unmeasured space\nStationarity: the rules governing a process do not drift over space-time"
  },
  {
    "objectID": "slides/02-slides.html#scale",
    "href": "slides/02-slides.html#scale",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nWhat do we even mean?\n\n\n\n\n\n\nGrain: the smallest unit of measurement\nExtent: the areal coverage of the measurement\n\n\n\n\nFrom Manson 2008"
  },
  {
    "objectID": "slides/02-slides.html#scale-1",
    "href": "slides/02-slides.html#scale-1",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nEven if it exists, how do we know we are measuring at the right scale?"
  },
  {
    "objectID": "slides/02-slides.html#fallacies",
    "href": "slides/02-slides.html#fallacies",
    "title": "Why Geographic Analysis",
    "section": "Fallacies",
    "text": "Fallacies\n\n\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals"
  },
  {
    "objectID": "slides/02-slides.html#measurement-error-and-mismatch",
    "href": "slides/02-slides.html#measurement-error-and-mismatch",
    "title": "Why Geographic Analysis",
    "section": "Measurement Error and Mismatch",
    "text": "Measurement Error and Mismatch"
  },
  {
    "objectID": "slides/02-slides.html#spatial-autocorrelation",
    "href": "slides/02-slides.html#spatial-autocorrelation",
    "title": "Why Geographic Analysis",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#stationarity",
    "href": "slides/02-slides.html#stationarity",
    "title": "Why Geographic Analysis",
    "section": "Stationarity",
    "text": "Stationarity\nThe rules governing a process do not drift over space-time\n\n\n\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "href": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "title": "Why Geographic Analysis",
    "section": "Not all geography needs to be quantitative",
    "text": "Not all geography needs to be quantitative\n\nAbstraction removes the interesting part\nWhat “is” may require assumptions we don’t want to accept\nWholly dependent on the military-industrial complex"
  },
  {
    "objectID": "content/26-content.html",
    "href": "content/26-content.html",
    "title": "Movement and Networks II",
    "section": "",
    "text": "Now that we’ve chatted briefly about what a network is and how we represent it for analysis, it’s time to take some first steps toward building spatial networks in R. The backbone of most network analysis is igraph, a package that is available for a number of programming languages. There are also a growing number of spatial network packages being developed for R. We’ll introduce a few of those today."
  },
  {
    "objectID": "content/26-content.html#resources",
    "href": "content/26-content.html#resources",
    "title": "Movement and Networks II",
    "section": "Resources",
    "text": "Resources\n\n Network Data in R and Spatial Networks from the Online Companion to Network Science in Archaelogy introduces a variety of approaches for handling and visualizing network data with R using examples from archaeology.\n CMRnet: An R package to derive networks of social interactions and movement from mark-recapture data by (Silk et al. 2021) describes a package for developing networks from common wildlife sampling techniques.\n The sfnetworks package github page provides a variety of vignettes for manipulating spatial network data within the sf framework for spatial objects."
  },
  {
    "objectID": "content/26-content.html#objectives",
    "href": "content/26-content.html#objectives",
    "title": "Movement and Networks II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nGenerate an adjacency matrix for network analysis\nCalculate network density, centrality, and other common measures\nGenerate landscape connectivity models using terra and gDistance\n\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/24-content.html",
    "href": "content/24-content.html",
    "title": "Statistical Modelling III",
    "section": "",
    "text": "In our last class on multivariate analysis, we’ll take on one of the more underappreciated elements of modeling: understanding if your model is good enough for prediction or inference. We’ll spend a bit of time differentiating the uses of models as a means of understanding what it means to be a “good” model."
  },
  {
    "objectID": "content/24-content.html#resources",
    "href": "content/24-content.html#resources",
    "title": "Statistical Modelling III",
    "section": "Resources",
    "text": "Resources\n\n A practical guide to selecting models for exploration, inference, and prediction in ecology by (Tredennick et al. 2021) highlights the importance of understanding model performance before making inference on predictor effects.\n Model selection using information criteria, but is the “best” model any good? by (Mac Nally et al. 2018) highlights the importance of understanding model performance before making inference on predictor effects.\n Standards for distribution models in biodiversity assessments by (Araújo et al. 2019) highlights the importance of understanding model performance before making inference on predictor effects."
  },
  {
    "objectID": "content/24-content.html#objectives",
    "href": "content/24-content.html#objectives",
    "title": "Statistical Modelling III",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate three different reasons for modeling and how they link to assessments of fit\nDescribe and implement several test statistics for assessing model fit\nDescribe and implement several assessments of classification\nDescribe and implement resampling techniques to estimate predictive performance\n\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/22-content.html",
    "href": "content/22-content.html",
    "title": "Statistical Modelling I",
    "section": "",
    "text": "Now that we’ve spent some time building dataframes and assessing the spatial correlation (or covariation) for different data, we can move beyond just describing the nature of the data we have or interpolating based on simple predictions. We’ll introduce two fairly simple spatial analysis approaches - overlays and logistic regression - and talk about some of the key assumptions and extensions of these approaches."
  },
  {
    "objectID": "content/22-content.html#resources",
    "href": "content/22-content.html#resources",
    "title": "Statistical Modelling I",
    "section": "Resources",
    "text": "Resources\n\n Overlay analysis provides an overview of the logic of overlay analysis.\n Predicting site location with simple additive raster sensitivity analysis using R from Ben Markwick has a complete example of using a weights of evidence approach to overlays.\n Logistic regression: a brief primer by (Stoltzfus 2011) is a nice introduction to logistic regression.\n Is my species distribution model fit for purpose? Matching data and models to applications by (Guillera-Arroita et al. 2015) is an excellent, concise description of the relations between data collection, statistical models, and inference.\n Predicting species distributions for conservation decisions by (Guisan et al. 2013) is a foundational paper describing some of the challenges with making conservation decisions based on the outcomes of species distribution models."
  },
  {
    "objectID": "content/22-content.html#objectives",
    "href": "content/22-content.html#objectives",
    "title": "Statistical Modelling I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nIdentify nearest neighbors based on distance\nDescribe and implement overlay analyses\nExtend overlay analysis to statistical modeling\nGenerate spatial predictions from statistical models\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/20-content.html",
    "href": "content/20-content.html",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Last class we started to explore ways to leverage spatial autocorrelation as a means of using interpolation to generate values at unobserved locations. We’ll continue that discussion using variograms and kriging. We then move to a discussion of areal data and the need to identify “neighbors” as a means of understanding how to weight observations when the actual point location of the observation may be unknown or impossible to assign."
  },
  {
    "objectID": "content/20-content.html#resources",
    "href": "content/20-content.html#resources",
    "title": "Proximity and Areal Data",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights."
  },
  {
    "objectID": "content/20-content.html#objectives",
    "href": "content/20-content.html#objectives",
    "title": "Proximity and Areal Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe and implement statistical approaches to interpolation\nDescribe the case for identifying neighbors with areal data\nImplement contiguity-based neighborhood detection approaches\nImplement graph-based neighborhood detection approaches\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Recording"
  },
  {
    "objectID": "content/18-content.html",
    "href": "content/18-content.html",
    "title": "Combining Data and Point Patterns",
    "section": "",
    "text": "Today we’ll finish up our example of combining data for analysis and introduce point process models as a first version of spatial analysis. We’ll need a few new packages here, but many of the key data management processes will remain the same."
  },
  {
    "objectID": "content/18-content.html#resources",
    "href": "content/18-content.html#resources",
    "title": "Combining Data and Point Patterns",
    "section": "Resources",
    "text": "Resources\n\n The Chapters 17 and 18 on Spatial Point Processes and the spatstat package in Paula Moraga’s book Spatial Statistics for Data Science: Theory and Practice with R.\n Rings, circles, and null-models for point pattern analysis in ecology by (Wiegand and A. Moloney 2004) provides an introduction to metrics for spatial clustering with applications in ecology.\n Improving the usability of spatial point process methodology: an interdisciplinary dialogue between statistics and ecology by Janine Illian (a major contributor to modern point pattern analyses) and David Burslem (a Scottish plant ecologist) (Illian and Burslem 2017) is a fairly modern take on the challenges associated with point process modeling in ecology.\n Chapter 11: Point Pattern Analysis in Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provides a nice (and free) introduction to some of these introductory point process methods."
  },
  {
    "objectID": "content/18-content.html#objectives",
    "href": "content/18-content.html#objectives",
    "title": "Combining Data and Point Patterns",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nComplete the creation of a dataset for analysis using vector and raster data\nDefine a point process and their utility for ecological applications\nDefine first and second-order Complete Spatial Randomness\nUse several common functions to explore point patterns"
  },
  {
    "objectID": "content/18-content.html#slides",
    "href": "content/18-content.html#slides",
    "title": "Combining Data and Point Patterns",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Building Spatial Databases with Attributes",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R.\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#resources",
    "href": "content/14-content.html#resources",
    "title": "Building Spatial Databases with Attributes",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R.\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#objectives",
    "href": "content/14-content.html#objectives",
    "title": "Building Spatial Databases with Attributes",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nBegin building a database for spatial analysis",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "Building Spatial Databases with Attributes",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Operations with Raster Data I",
    "section": "",
    "text": "Now that we’ve learned about predicates and measures with raster data, it’s time to learn more about some of the transformations that we can conduct with terra. We’ll start with some of the basic transformations that operate on the entire dataset then move to some of the important cell-wise operations.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Operations with Raster Data I",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#objectives",
    "href": "content/12-content.html#objectives",
    "title": "Operations with Raster Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Operations with Raster Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Operations With Vector Data I",
    "section": "",
    "text": "Now that we’ve spent some time getting used to the syntax of the sf package and used it to assess some of the characteristics of vector objects (e.g., through predicates and measures), we’ll move into transformations. Transformations allow you to actually manipulate the geometries of a vector object (without necessarily changing the attributes themselves) and are a powerful tool for geting disparate data into some logical alignment. That said, transforming geometries can be complicated and often has some unanticipated consequences. That’s why we spent a little bit of time learning the mapping syntax as a means for you to be able to check yourself.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Operations With Vector Data I",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages).",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#objectives",
    "href": "content/10-content.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Operations With Vector Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Areal Data: Rasters",
    "section": "",
    "text": "Now that we’ve learned a bit about how to assess some of the important quantities of vector-based spatial data, we’ll try to apply a bit of the same logic to raster data. We’ll be using the terra package for the majority of raster options in this course primarily because it of its speed. That said, it is not a tidyverse package and so some of the intuition we used to organize the sf functions will be a little harder to extend here. I’ll do my best to help you make the links!!",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Areal Data: Rasters",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 from Paula Moraga’s Spatial Statistics for Data Science: Theory and Practice with R provides a quick intro to using terra for raster and vector data.\n The terra reference page provides a brief overview of all of the functions and their categories. We’ll only focus on the SpatRaster methods.\n Raster Data Manipulation from the Spatial Data Science with R and terra ebook provides some nice examples of terra functions in the context of spatial workflows.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#objectives",
    "href": "content/08-content.html#objectives",
    "title": "Areal Data: Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAccess the elements that define a raster\nBuild rasters from scratch using matrix operations and terra\nEvaluate logical conditions with raster data\nCalculate different measures of raster data",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Areal Data: Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "",
    "text": "Now that you have a bit of the fundamentals of geographic data and have had a chance to start using R, it’s time to get into more complicated workflows. To do that, you’ll have to have a bit more of a foundation in coordinates, coordinate reference systems, and geometries and how to access those in R. We’ll start there today and move into functions that change or relate geometries in the next few classes.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#objectives",
    "href": "content/06-content.html#objectives",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine coordinate, coordinate system, datum, and coordinate reference system\nAccess coordinate and geometry information for simple features in R\nUnderstand the rules for simple feature geometries\nAccess and transform the coordinate reference system for vector and raster data in R",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Introduction to Spatial Data in R",
    "section": "",
    "text": "Now that we’ve covered some of the conceptual bases of spatial data and geographic analysis, it’s time to get started working with actual data in R. Today’s readings and lecture are focused on the basics of getting your data into the R environment and familiarizing yourself with the different components that make up spatial data objects. We’ll do fancier things in the weeks to come!",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Introduction to Spatial Data in R",
    "section": "Readings",
    "text": "Readings\n Chapter 2 in Geocomputation with R (Lovelace et al. 2019) provides and overview of using sf for vector datasets and terra for raster data.\n Chapter 2 from Moraga (2023) explores similar topics, but provides more explanation about projections, coordinates, etc.\n Simple Features for R provides a more in-depth, technical discussion of how the sf package organizes spatial data and attributes.\n Ch 2.1-2.5 from Spatial Data Science with R and terra describes the basic functionality of terra for both vector and raster datasets. For reasons we’ll discuss in class, we will rarely use terra for vector data.",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#objectives",
    "href": "content/04-content.html#objectives",
    "title": "Introduction to Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRead spatial data into your R environment.\nDescribe the various aspects of spatial data files and objects.\nGenerate simple summaries describing the spatial data object.\nDetermine the projection, extent, and resolution of spatial data objects.",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Introduction to Spatial Data in R",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Why Geographic Analysis",
    "section": "",
    "text": "Before we get inundated with technical details and syntax, I think it’s important to remind ourselves why we need geographical analysis. We’ll spend some time on the various conceptualizations of place and space, how those things show up in geographic data, and their implications for the kinds of science we can do when we’re using geographic data. This session is meant to provide a bit of philosophical foundation for you to keep in mind as you work through various parts of the analytic pipeline.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Why Geographic Analysis",
    "section": "Readings",
    "text": "Readings\nThe following readings are intended to give you some sense of the discussion surrounding the role of spatial data in understanding the world. They are a mix of old favorites and relatively recent reviews. You don’t need to read all of them or memorize them, but they are worth a skim. I bet you’ll find something interesting.\n Conservation biogeography: assessment and prospects by Whitaker et al. (2005) provides an overview of the of geography in understanding ecosystem function and shaping conservation strategies.\n Economic geography, politics, and policy by Rickard (2020) provides a review of the role of geography in understanding responses to globalization.\n Revolutionary and counter revolutionary theory in geography and the problem of ghetto formation by David Harvey (1972) offers a scathing critique of quantitative geography (though Harvey was one of the founders of the field). See (Barnes 2009) for a relatively recent attempt to reconcile these views.\n Does scale exist? An epistemological scale continuum for complex human–environment systems by Steven Manson (2008) is one of my favorite summaries of the various definitions and confusion surrounding scale as a concept invoked in many disciplines.\n Spatial Scaling in Ecology by John Wiens (1989) describes the fundamental challenges of scale in Ecology.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#objectives",
    "href": "content/02-content.html#objectives",
    "title": "Why Geographic Analysis",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine what we mean by description, explanation, and prediction in geography.\nDescribe critiques and limitations of quantitative geographical analysis\nDefine ‘scale’ and its implications for geographic analysis\nPlace your research in the broader context of geographic analysis",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Why Geographic Analysis",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/32-content.html#objectives",
    "href": "content/32-content.html#objectives",
    "title": "Conclusion",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/30-content.html#objectives",
    "href": "content/30-content.html#objectives",
    "title": "Data Visualization and Maps II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/17-content.html",
    "href": "content/17-content.html",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "",
    "text": "Note that the last 3 sections needed a bit of reorganization. I’ve moved to session 18 to keep the webpage aligned with the schedule."
  },
  {
    "objectID": "content/17-content.html#resources",
    "href": "content/17-content.html#resources",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "content/17-content.html#objectives",
    "href": "content/17-content.html#objectives",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:"
  },
  {
    "objectID": "content/17-content.html#slides",
    "href": "content/17-content.html#slides",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Carolyn Koehn\n   4034-1 Environmental Research Building\n   carolynkoehn@u.boisestate.edu \n   Schedule an appointment\n\n\n\n\n\n   Mondays and Wednesdays\n   August 19–December 6, 2024\n   1:30-2:45 PM\n   Riverfront Hall Room 102B\n   Slack"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nSpatial data are ubiquitous and form the basis for many of our inquiries into social, ecological, and evolutionary processes. As such, developing the skills necessary for incorporating spatial data into reproducible statistical workflows is critical. In this course, we will introduce the core components of manipulating spatial data within the R statistical environment including managing vector and raster data, projections, extraction of data values, interpolation, and plotting. Students will also learn to prototype and benchmark different workflows to aid in applying the appropriate tools to their research questions."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course Objectives",
    "text": "Course Objectives\nStudents completing this course should be able to:\n\nArticulate the opportunities and challenges posed by geographic analysis.\nSelect the appropriate R packages and functions for manipulating different types of spatial data\nDesign statistical analyses that integrate geospatial and tabular data\n\nConstruct appropriate data visualizations for conveying geospatial data\nDevelop reproducible workflows for manipulating, visualizing, and analyzing spatial data."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "Expectations",
    "text": "Expectations\nBe nice. Be honest. Try hard.\nThe beauty of working with open source software is the community of users working on problems just like yours (and nothing like yours). Like any community, this one functions best when its members are kind, genuine, and make good-faith efforts to solve their problems along the way (more on this below).\nYou can (and should) expect me to:\n\nCreate a space where you can ask questions without fear of embarrassment or retribution\nProvide feedback on your work within 1 week of submission\nRespond to email and slack messages within 48 hours\nMake every attempt to answer your questions (when I can) or point you toward resources that may help\n\nIn turn, I expect you to:\n\nTreat all of us with respect and compassion\nMake an honest effort to work through the assignments\nDemonstrate that you have tried to solve your coding errors before asking me\nCommunicate with me when the course isn’t working for you"
  },
  {
    "objectID": "syllabus.html#prerequisite-knowledge-and-skills",
    "href": "syllabus.html#prerequisite-knowledge-and-skills",
    "title": "Syllabus",
    "section": "Prerequisite Knowledge and Skills",
    "text": "Prerequisite Knowledge and Skills\nYou can succeed in this class.\nSome familiarity with the R statistical environment is helpful, but not necessary. My goal is to foster an environment where we are all learning from each other and sharing the tips and tricks that help us along the way. Learning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. I find it helpful to remember the following:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. Even experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n— Hadley Wickham\n\n\nIf you want to start learning a few of the basics, the Resources tab has some background information to get you started. Note that this is not an exhaustive list - the number of new R tutorials available on the internet seems to be growing exponentially.\n\nGetting Help With R problems\nI am happy to help you work through your R coding challenges, but there are a lot of you and only one of me. Moreover, I may not always know exactly how to fix your problem any better than you do. In order to make sure that I am not the primary obstacle to your ability to complete the class assignments, I’m asking that you use the following steps prior to emailing/Slacking me with your coding questions.\n\n\n\n\n\n\nTip\n\n\n\nWhen you send me a question, please let me know what you searched, why the solutions you found don’t work for you, and what output you are expecting**\n\n\nWe’ll spend a bit of time on asking better questions and getting better answers so don’t worry if you aren’t quite sure how this all works.\n\nGoogle it! Searching for help with R on Google can sometimes be tricky. Google is generally smart enough to figure out what you mean when you search for “r reproject polygons”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats reproject polygons”). Also, since most of your R work will deal with the RSpatial packages, it’s often easier to just search for the package name and operation rather than the letter “r” (e.g. “sf reproject polygons”). I often paste the specific error message I get along with the spatial package I’m using to try and help Google find my solutions.\nAsk your colleagues We have an r_spatial chatroom at Slack where anyone in this class can ask questions and anyone can answer. Ask questions about code or class materials. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too. As a bonus, Slack allows you to format code to make it easy for all of us to copy and paste your code and distinguish it from the rest of your question.\nUse the forums Two of the most important sources for help with R-coding are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). If you aren’t able to find an answer to your question from the thousands of existing questions, you can post your own. You’ll need to create a reproducible example so others can figure out what you’re trying to do and what error you’re receiving, but you’d be amazed how helpful the community can be.\nAsk me! Sign up for a time to meet with me during student hours at https://calendly.com/carolynkoehn-u. I’ll want to know what searches you’ve tried (so I don’t chase down answers that you’ve already seen) and what approaches you’ve tried and why they haven’t worked. Remember, I’m here to help (but not write your code for you)."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nR and RStudio\nR is free, but it can sometimes be a pain to install and configure especially when dealing with spatial packages (we’ll talk more about why this is during class). To make life easier, I have set up an online RStudio server service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer and we should be able to avoid a number of the machine-specific issues that pop-up when 20 students have 20 different computers, operating systems (OS), etc. If you haven’t installed R on your local machine and would like some help getting that set up, there’ a useful set of instructions for installing R, RStudio, and all the tidyverse packages here.\n\n\nGit and Github Classroom\nAll assignments will be managed using GitHub classroom. This will allow each you to have your own repositories for each assignment and make it easier for me to comment on and help with your code. To use this, you should sign up for the GitHub Student Developers Pack as soon as possible and send me your github username. Once I have that, I can add you to the course and make sure that you have access to all of the necessary data and example code.\n\n\nReadings\nThe goal of this course is primarily to get you started with spatial workflows in R. That said, maps (and the spatial data that produce them) are extremely powerful and their use comes with risks and responsibilities. Although most of this course will focus on getting the code right, I’ll mix in a few readings each week to help tie the technical details of our code back to the broader contexts of spatial analysis or to illustrate new applications of the methods you are learning."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThis course is organized in 4 sections:\n\nGetting Started: What is spatial analysis and how do we do it in R?\nSpatial Data Operations in R: Prepping geospatial data for use in R\nStatistical Workflows for Spatial Data: Putting spatial data to work!\nVisualizing Spatial Data: Everyone loves a map…\n\nThe schedule page provides an overview of what to expect each week.\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the course progresses."
  },
  {
    "objectID": "syllabus.html#assignments-and-grades",
    "href": "syllabus.html#assignments-and-grades",
    "title": "Syllabus",
    "section": "Assignments and Grades",
    "text": "Assignments and Grades\nI teach this course because I believe that a) we can learn a lot about social and ecological processes by studying where they happen, b) integrating spatial analysis directly into statistical workflows makes those analyses more robust and reproducible, and c) overcoming coding challenges can provide a profound sense of accomplishment. That said, I recognize that there are many reasons that you are taking this course and that my objectives may differ from yours. In order to make sure that you get what you need out of this class, we’ll be using a mix of approaches for determining your grade in this course.\nSelf-assessment (12.5 pts x 2):  During the first week of the course, I’m going to ask you to reflect on what you want out of this course (concepts, skills, practice, etc.). This assessment will help me do a better job of aligning the content of the course to your specific needs. Grading for the self-assessment is described on the assignments page.\nExercises (5pts x 10): There are ten homework assignments. These exercises are designed to reinforce the material we cover in lecture, give you practice designing and implementing your own workflows, and build habits that promote reproducibility in science. They also allow me get a sense for your engagement in the course. Exercises are due at 11:59PM on their due date (generally Thursdays). I will post the “key” within 3 days of the due date and will not accept submissions after the key is posted. If you turn the assignment in on-time with the required number of commits, you’ll receive full credit.\nAssignment Revisions (25pts x 3): We will have three “assignment revisions” due during the course. These provide an opportunity for me to check in and see how things are going. You’ll be able to update your responses to the homeworks based on the keys and reflect on what you’ve learned throughout the course of the assignments. You’ll also be able to provide additional feedback on how the course is going for you. Rather than assign arbitrary points to each assignment, I’m going to grade your assignment revisions using the following ‘levels’ (inspired by Sarah K. Johnson’s description of her graduate data analysis course at Tufts):\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your Rmarkdown document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before you get to the next assessment. Failure to resubmit will result in no credit for the assessment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. You’re welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\nFinal Project (50pts):  The final project asks you to conduct an entire spatial analysis from layout to results. Grades on the final project are based on your objectives and your self-assessment of whether or not you achieved those objectives. Your first draft of the final project will be due December 5. I’ll make comments based on the same categories for the homework revision and you’ll have time to revise your submission prior to the final deadline of December 12.\nYou can find descriptions for all the assignments on the assignments page.\n\nGrades\nWe’ll use a form of contract grading to determine your grades in the course. Contract grading allows us to have a conversation about what you want out of the course, what you expect to put into it, and what I think you need to be successful in deploying the skills we learn here. Based on your goals for course, we’ll sign a contract that instantiates your objectives into the grade you’ll receive for the course. Complete the assignments and meet your objectives and you’ll get the grade you chose.\nThe expectations for the grades are:\n\nA You complete all of the self-assessments and at least 8 of the exercises. All of the assignment revisions achieve the “Good to Go” level. Your final project achieves the “Good to Go” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nB You complete all of the self-assessments and at least 8 of the exercises. At least one of the assignment revisions achieves the “Good to Go” level with the remainder achieving “Resubmit if you like”. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nC You complete all of the self-assessments and at least 6 of the exercises. All of your assignment revisions achieve the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nD You complete all of the self-assessments and at least 4 of the exercises. At least one of your assignment revisions achieves the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\n\n\n\nAttendance and incomplete assignments\nAttendance is an important part of this course. You are allowed to miss 2 classes without providing any justification (stuff happens). Beyond that, each additional absence will result in a 0.5 grading reduction (i.e., an A becomes and A-). Similarly, completing the assignments to a satisfactory level is vital to ensure you have a firm grip on the code and concepts. Hence, each assignment that fails to achieve a “Resubmit If You Like” will result in 0.5 grading reduction.\n\n\nLate work\nI would highly recommend staying caught up as much as possible, but if you need to turn something (other than the exercises and final project) in late, that’s fine—there’s no penalty."
  },
  {
    "objectID": "syllabus.html#student-wellbeing",
    "href": "syllabus.html#student-wellbeing",
    "title": "Syllabus",
    "section": "Student Wellbeing",
    "text": "Student Wellbeing\nIf you are struggling for any reason (COVID, relationship, family, or life’s stresses) and believe these may impact your performance in the course, I encourage you to contact the Dean of Students at (208) 426-1527 or email deanofstundents@boisestate.edu for support. If you notice a significant change in your mood, sleep, feelings of hopelessness or a lack of self worth, consider connecting immediately with Counseling Services (1529 Belmont Street, Norco Building) at (208) 426-1459 or email healthservices@boisestate.edu.\n\nLearning during a pandemic\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise."
  },
  {
    "objectID": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "href": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "title": "Syllabus",
    "section": "This course was designed with you in mind",
    "text": "This course was designed with you in mind\nI developed this course to provide a welcoming environment and effective, equitable learning experience for all students. If you encounter barriers in this course, please bring them to my attention so that I may work to address them.\n\nThis class’s community is inclusive.\nStudents in this class represent a rich variety of backgrounds and perspectives. The Human-Environment Systems group is committed to providing an atmosphere for learning that respects diversity and creates inclusive environments in our courses. While working together to build this community, we ask all members to: * share their unique experiences, values, and beliefs, if comfortable doing so.\n\nlisten deeply to one another.\nhonor the uniqueness of their peers.\nappreciate the opportunity we have to learn from each other in this community.\nuse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the campus community.\nrecognize opportunities to invite a community member to exhibit more inclusive, equitable speech or behavior—and then also invite them into further conversation. We also expect community members to respond with gratitude and to take a moment of reflection when they receive such an invitation, rather than react immediately from defensiveness.\nkeep confidential any discussions that the community has of a personal (or professional) nature, unless the speaker has given explicit permission to share what they have said.\nrespect the right of students to be addressed and referred to by the names and pronouns that correspond to their gender identities, including the use of non-binary pronouns.\n\n\n\nWe use each other’s preferred names and pronouns.\nI will ask you to let me know your preferred or adopted name and gender pronoun(s), and I will make those changes to my own records and address you that way in all cases.\nTo change to a preferred name so that it displays on all BSU sites, including Canvas and our course roster, contact the Registrar’s Office at (208) 426-4249. Note that only a legal name change can alter your name on BSU official and legal documents (e.g., your transcript).\n\n\nThis course is accessible to students with disabilities.\nI recognize that navigating your education and life can often be more difficult if you have disabilities. I want you to achieve at your highest capacity in this class. If you have a disability, I need to know if you encounter inequitable opportunities in my course related to:\n\naccessing and understanding course materials engaging with course materials and other students in the course\ndemonstrating your skills and knowledge on assignments and exams.\n\nIf you have a documented disability, you may be eligible for accommodations in all of your courses. To learn more, make an appointment with the university’s Educational Access Center.\n\n\nFor students responsible for children\nI recognize the unique challenges that can arise for students who are also parents or guardians of children. Any student needing to temporarily bring children or another dependent to class is welcome to do so to stay engaged with the class."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the principle that asks students to engage with their academic work to the fullest and to behave honestly, transparently, and ethically in every assignment and every interaction with a peer, professor, or research participant. When a strong culture of academic integrity is fostered by students and faculty in an academic program, students learn more, build positive relationships and collaborations, and can feel more confident in the value of their degrees.\nIn order to cultivate fairness and credibility, everyone must participate in upholding academic integrity. Students in this class are responsible for asking for help or clarification when it’s needed, speaking up when they see unethical behavior taking place, and understanding and adhering to the Student Code of Conduct, including the section on academic misconduct. Boise State and I take academic misconduct very seriously. It’s important to know that when a student engages in academic misconduct, I will report the incident to the Office of the Dean of Students. I also have the right to assign sanctions, which could include requirements to revise or redo work, complete educational assignments to learn about academic integrity, and grade penalties ranging from lower credit on an assignment to failing this class1. Students should learn more by reviewing the Student Code of Conduct."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo seriously, just don’t cheat or plagiarize!↩︎"
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Authoring in Rmarkdown and Quarto",
    "section": "",
    "text": "[Rmarkdown] and [Quarto] are two powerful ways to combine writing (or presentations) and analysis into a single reproducible workflow. Although they can be a little more cumbersome to use than traditional word processors (e.g., Word or Pages) or presentation software (e.g., PowerPoint or Keynotes), they have the benefit of allowing you to keep all of the pieces of your manuscripts or presentations in one place. Change some data or analysis? The whole manuscript or presentation should update without forcing you to try and find all of the places where that change might alter your writing or slides. It may take a little getting used to, but the fact that both Rmarkdown and Quarto can utilize the power of LaTeX typesetting means that you’ll ultimately be able to produce publication quality equations, tables, and figures all in one place.\nThis webpage and all of my slides were built with Quarto. Having gone through the process of learning how make that work, I’m convinced that having a working knowledge of one or both of these is useful. As such, you’ll be using Rmarkdown or Quarto (your choice) to complete your assignments and render them to html.",
    "crumbs": [
      "Resources",
      "Guides",
      "Authoring in Rmarkdown and Quarto"
    ]
  },
  {
    "objectID": "resource/lastyear.html",
    "href": "resource/lastyear.html",
    "title": "Past years’ classes",
    "section": "",
    "text": "2021 was the first time this class was taught in its current format. Matt Williamson built a number of worked examples to try to clarify how different parts of a spatial workflow come together. Although the examples may diverge a bit from the examples we do this year, the page does have a number of potentially useful pieces. Check out the examples to access these.",
    "crumbs": [
      "Resources",
      "Overview",
      "Past years' classes"
    ]
  },
  {
    "objectID": "resource/git.html",
    "href": "resource/git.html",
    "title": "Helpful git links",
    "section": "",
    "text": "Getting in the habit of using version control can be challenging, especially if you are collaborating with others. The challenge gets worse when some of those collaborators are not familiar with the importance of version control. Here are a few links to try and make your (and their) transition a little smoother.",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "href": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "title": "Helpful git links",
    "section": "Installing Git and making it play nice with R",
    "text": "Installing Git and making it play nice with R\nHappy git with R is Jenny Bryan’s extremely helpful introduction to git and incorporating it into your R workflow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "resource/git.html#getting-the-hang-of-git",
    "href": "resource/git.html#getting-the-hang-of-git",
    "title": "Helpful git links",
    "section": "Getting the hang of git",
    "text": "Getting the hang of git\nUnderstanding the logic of git: provides a relatively accessible explanation of the various operations in git and links that to commonly used syntax.\nOh Sh@t, Git?!?: A less technical, more irreverant introduction to git workflows and fixing the inevitable challenges of version control. (G-rated version available at Dang it, Git?!?).",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "lesson/index.html",
    "href": "lesson/index.html",
    "title": "Lessons",
    "section": "",
    "text": "This section has some worked examples demonstrating the use of different packages and giving some ‘roadmaps’ for completing different spatial operations. These are mostly my opinions, your mileage may vary, but I’ll try to justify why I do things the way that I do so that you can make an informed choice when you decide to deviate from that path.",
    "crumbs": [
      "Lessons",
      "Overview",
      "Lessons"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings and slides",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture. On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).",
    "crumbs": [
      "Content",
      "Overview",
      "Readings and slides"
    ]
  },
  {
    "objectID": "assignment/self-eval2.html",
    "href": "assignment/self-eval2.html",
    "title": "Self-reflection 2",
    "section": "",
    "text": "This is the second of three self-reflections that you’ll complete during the course. We’ll revisit your objectives and check-in on what I can do to help you get the most out of the second half of the course. This is our mid-semester “adjustment”. As such, I’m asking that you complete this in a timely fashion and turn it in by Oct 21. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval2.html#instructions",
    "href": "assignment/self-eval2.html#instructions",
    "title": "Self-reflection 2",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-2-xxx.qmd and give it a title (like M Williamson Self-Reflection 2). Make sure that you select the html output option.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-2-xx.qmd, and self-reflection-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "The main goals of this class is class is to get you comfortable with the manipulation, analysis, and visualization of spatial data using the R computing environment. These assignments are designed to help you practice those skills and reflect on your progress. All of the assignments have their own repository in our GitHub classroom, so you’ll submit them there. I’ll transfer grades over to Canvas so you’ll have an idea where you’re at in the course, but we won’t use Canvas for much else.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#self-reflections",
    "href": "assignment/index.html#self-reflections",
    "title": "Assignments",
    "section": "Self-reflections",
    "text": "Self-reflections\nThis course is collaborative. I’m hoping to provide a broad suite of information that can help you analyze spatial data for your graduate research and beyond. That said, you know more than I do about your personal and professional objectives. During the course of the semester, I’ll ask you to submit 2 self-reflections. The first should help me get to know you and get us on the same page with respect to your goals for the course. The last provides you with an opportunity to reflect on your progress in the course relative to your own objectives and give me feedback on how I can improve the course. These self-reflections are the foundation of how you’ll be ‘graded’ in this course, so they are mandatory and need to be submitted by the due date",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nI’ve created ten assignments to practice outlining a workflow, writing R code, and troubleshooting errors. The assignments rely on these datatasets and are designed to provide “bite-sized” practice to help cement the topics covered in the (generally 2) lectures that each applies. That said, they may take some and I’d encourage you to start on them early so that we have time to work on any questions you have. Moreover, long coding days are the worst so working in bite size chunks can help keep things fun! Finally, working incrementally will also to encourage you to get in the habit of using git to keep track of your progress.\nI’ll be grading these according to:\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your quarto document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before the next assignment. Failure to resubmit will result in no credit for the assignment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. You are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\n\n\n\n\n\n\nTip\n\n\n\nLate Work: There is no such thing as ‘late work’ with these assignments. Life happens, sometimes things take longer to finish than you expect. If you turn it in, I’ll give you feedback. That said, the assignments build on each other so it’s probably best to avoid falling too far behind.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your knowledge of spatial analysis workflows through a final project that requires you to integrate a variety of spatial datasets, analyze the data with respect to a question of interest, and create visuals that help you interpret the data.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/data.html",
    "href": "assignment/data.html",
    "title": "Assignment Datasets",
    "section": "",
    "text": "In order to reduce the fatigue of learning new datasets for each assignment, we’ll try to keep things limited to a handful of point, vector, and raster datasets. I’m going to simplify them a bit for the sake of loading times, but you can take a look at the webpages for each dataset to get a sense for what the originals look like."
  },
  {
    "objectID": "assignment/data.html#fire-occurrence-data",
    "href": "assignment/data.html#fire-occurrence-data",
    "title": "Assignment Datasets",
    "section": "Fire Occurrence Data",
    "text": "Fire Occurrence Data\nThe US Forest Service maintains a geospatial archive of historic and ongoing fire occurrence ignition locations. It’s a massive dataset, but you can play around with a web-viewer to see the sorts of things they report."
  },
  {
    "objectID": "assignment/data.html#wildfire-risk-to-communities",
    "href": "assignment/data.html#wildfire-risk-to-communities",
    "title": "Assignment Datasets",
    "section": "Wildfire Risk to Communities",
    "text": "Wildfire Risk to Communities\nThis is a recent product that is being used to determine where priorities for fire mitigation and restoration should be located. They’ve been the tool underlying the most recent round of Community Wildfire Defense Grants."
  },
  {
    "objectID": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "href": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "title": "Assignment Datasets",
    "section": "Climate and Economic Justice Screening Tool",
    "text": "Climate and Economic Justice Screening Tool\nThe CEJST combines a number of interesting datasets to identify locations where environmental justice concerns are likely to be highest. We’ll generally be using the underlying data (not the classification itself), but it’s worth checking out just to get a sense for what’s there.\nIn general, I’ll do my best to keep us using these datasets and give you a heads up and description if we need to deviate (e.g., for the networks unit)."
  },
  {
    "objectID": "assignment/12-nets.html",
    "href": "assignment/12-nets.html",
    "title": "Assignment 10: Movement and Networks",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/12-nets.html#instructions",
    "href": "assignment/12-nets.html#instructions",
    "title": "Assignment 10: Movement and Networks",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/10-secondrevision.html",
    "href": "assignment/10-secondrevision.html",
    "title": "Assignment 10: Revisiting your code (part 2)",
    "section": "",
    "text": "This is your second opportunity to reconsider your answers to the middle four assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/08-combinations.html",
    "href": "assignment/08-combinations.html",
    "title": "Assignment 7: Combining Data Types",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/08-combinations.html#instructions",
    "href": "assignment/08-combinations.html#instructions",
    "title": "Assignment 7: Combining Data Types",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/06-vectorops.html",
    "href": "assignment/06-vectorops.html",
    "title": "Assignment 6: Vector Operations",
    "section": "",
    "text": "Now that you’ve been introduced to predicates, measures, and transformers in the sf package. You should be able complete a relatively simple workflow for a spatial analysis. We’ll build on this again with raster data (using terra) next week and then integrate both data models the week after that. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/06-vectorops.html#instructions",
    "href": "assignment/06-vectorops.html#instructions",
    "title": "Assignment 6: Vector Operations",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-6-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-6-xxx.qmd and give it a title (like M Williamson Assignment 6). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-6-xx.qmd, and assignment-6-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/06-vectorops.html#the-assignment",
    "href": "assignment/06-vectorops.html#the-assignment",
    "title": "Assignment 6: Vector Operations",
    "section": "The Assignment",
    "text": "The Assignment\nWe want to begin to assess the role of distance from schools in determining the education outcomes for Idahoans. We’ll use the landmarks_pnw.csv and cejst_pnw.shp datasets as the basis for this assignment. You’ll need to load the csv and convert it to an sf object. We want to compare the percentage of individuals age 25 or over with less than a high school degree (HSEF in the cejst dataset) for of counties within 50km of a school (MTFCC == K2543) to those that are more than 50km. You’ll need to follow many of the same operations in the video example from class. Your assignment is:\n\nWrite out the pseudocode for your analysis\nTranslate the pseudocode into code chunks and create the necessary code (You’ll need to use things like st_distance, st_buffer, st_sym_difference)\nMake a map for both the percentage of individuals with less than a high school degree in counties within 50km and beyond 50km (i.e. make 2 maps)\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/04-maps.html",
    "href": "assignment/04-maps.html",
    "title": "Assignment 4: Predicates and Measures",
    "section": "",
    "text": "This is the fourth assignment of the semester for HES 505.\nNow that you’ve learned a bit about predicates and measures it’s time to practice on some vector and raster data. By the end of this assignment, you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#instructions",
    "href": "assignment/04-maps.html#instructions",
    "title": "Assignment 4: Predicates and Measures",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-4-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-4-xxx.qmd and give it a title (like M Williamson Assignment 4). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-4-xx.qmd, and assignment-4-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#the-data",
    "href": "assignment/04-maps.html#the-data",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Data",
    "text": "The Data\nWe will be using the landmarks data table and the shapefile from the Climate and Economic Justice Screening Tool from previous assignments. I’ve moved the versions for this assignment into the /opt/data/data/assignment04/ folder to make life easier.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#the-assignment",
    "href": "assignment/04-maps.html#the-assignment",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Assignment",
    "text": "The Assignment\n\nLoad the cejst_nw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code).\nLoad the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset…\nFilter the cejst_nw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\nFinally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/02-introspatialsolutions.html",
    "href": "assignment/02-introspatialsolutions.html",
    "title": "Assignment 2 Solutions: Intro to Spatial Data",
    "section": "",
    "text": "Find a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative analysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n1. Create a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\n\nIn order to do this, you’ll need to start a new Quarto document (File -&gt; New File -&gt; Quarto Document). Once you’ve done that Rstudio will open up a Quarto document with the yaml header already in place and a bunch of example text and code. Delete that. Then use Markdown syntax to specify headings (# for top level headings, ## for second level headings, etc.) So in this case, once you’ve gotten all of the example stuff deleted, you can use # Introduction to create a section header with the correct title. Adding citations requires you to create a separate, .bib file that lives in the same directory as your document and has your citation info in BIBTEX format. You can find more on that here.\n\n2. Create a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in class (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\n\nNow I’ll add # Methods as my next header and start to write out the steps I’d like the analysis to follow. There are lots of ways you might do this, depending on whether you want the pseudocode to be part of your final product. For now we’ll keep it simple and just use numbered steps like:\n\n\nLoad data\nFilter the correct rows\nSelect the right variables\nModel y as a function of x1, x2, x3…\nPlot\n\n3. Add in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\n\nAdding in code blocks and giving is accomplished by setting up your code fence (```), identifying the language you want to use ({r}), and then setting code options with the hash-pipe (#|). You can see my example here\n\n4. Based on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\n\nIn order to ensure that the code prints, you need to add the #| echo: true execution option. In order to prevent it from running, you want to set #| eval: false. You can see this in my example.\n\n5. Add a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose. \n\nYou can add images in markdown by using the ![]() syntax where the file location is pasted in the parentheses and any caption is placed in the brackets."
  },
  {
    "objectID": "assignment/01-intro.html",
    "href": "assignment/01-intro.html",
    "title": "Assignment 1: Introductory material",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of geographic thought, core technical details of working with spatial data, and introduce R as a tool for end-to-end spatial workflows. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/01-intro.html#instructions",
    "href": "assignment/01-intro.html#instructions",
    "title": "Assignment 1: Introductory material",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/01-intro.html#questions-for-the-assignment",
    "href": "assignment/01-intro.html#questions-for-the-assignment",
    "title": "Assignment 1: Introductory material",
    "section": "Questions for the Assignment",
    "text": "Questions for the Assignment\n\nHow does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\nWhat are the primary components that describe spatial data?\nWhat is the coordinate reference system and why is it important\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html",
    "href": "assignment/02-introspatial.html",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "",
    "text": "This is the second assignment of the semester for HES 505.\nFor the rest of the course, I’ll be asking you to use pseudocode to plan your analysis steps before you start using any functions (or writing your own). Pseudocode allows you to think about the important steps of your process and identify your desired results before your start down the path of coding. You can think of pseudocode as an outline for syntax, much like the one you might use for writing an manuscript or report. Quarto documents are designed to let you both outline your report and plan your analysis all in the same place! This assingment is meant to give you some practice setting up your outlines before you start coding. By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html#instructions",
    "href": "assignment/02-introspatial.html#instructions",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-2-xxx.qmd and give it a title (like M Williamson Assignment 2). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-2-xx.qmd, and assignment-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html#the-assignment",
    "href": "assignment/02-introspatial.html#the-assignment",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "The Assignment",
    "text": "The Assignment\nFind a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative analysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n\nCreate a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\nCreate a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in clase (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\nAdd in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\nBased on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\nAdd a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/03-vector.html",
    "href": "assignment/03-vector.html",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "",
    "text": "This is the third assignment of the semester for HES 505. The last few lectures have focused on coordinates and geometries. In this assignment, we’ll use the different functions for accessing and transforming the crs of different spatial objects. We’ll also use a little of the tidyverse to subset the data and access some of the geometry information for one of the observations in our dataset. You’ll need to use both the lectures and the recorded examples (or check out the tidyverse tutorials linked in the lectures). This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#instructions",
    "href": "assignment/03-vector.html#instructions",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-3-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-3-xxx.qmd and give it a title (like M Williamson Assignment 3). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-3-xx.qmd, and assignment-3-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#the-data",
    "href": "assignment/03-vector.html#the-data",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Data",
    "text": "The Data\nFor this assignment, you’ll be looking at 3 different datasets. One from the Center for Disease Control’s PLACES data describing the distribution of chronic health risks, one from the EPA describing exposure to PM2.5 (an important air pollutant), and one describing wildfire risk. You might imagine that as we become increasingly concerned with the environmental justice concerns associated with fire, we might be concerned about whether more smoke increases the risk of chronic respiratory diseases. We won’t totally answer that question this week, but you’ll start to develop the workflow necessary to move towards that type of analysis. All of the data are on the server in the opt/data/data/assignment03/ folder.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#the-assignment",
    "href": "assignment/03-vector.html#the-assignment",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Assignment",
    "text": "The Assignment\n\nWrite out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\nRead in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\nRe-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tif file. Verify that all the files have the same projection.\nHow does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\nWhat class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html",
    "href": "assignment/05-firstrevision.html",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "",
    "text": "Now that you’ve had some practice with R and the format of the course, it’s time to pause and take a moment to check in on what you’ve learned. Because we haven’t had a ton of coding yet, this review is a little more conceptual (rather than focusing on particular pieces you may have done incorrectly or inefficiently). My soluionts (or suggestions) for how I’d approach the first four assignments are posted (at the end of each assignment page). Your task here is to review those solutions and your own code and answer a few questions to demonstrate what you’ve learned so far and where I need to be more clear. You’ll still be using Quarto to complete this homework.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html#instructions",
    "href": "assignment/05-firstrevision.html#instructions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar1_xxx.qmd and give it a title (like M Williamson Assignment Revision 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar1_xx.qmd, and ar1_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html#questions",
    "href": "assignment/05-firstrevision.html#questions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Questions",
    "text": "Questions\n\nHow do you introduce yourself to git? (look at some of the initial lectures for the course) How often do you need to introduce yourself when you are on the RStudio server? What is the difference between a “commit” and a “push”?\nHow do you check the Coordinate Reference System of a vector file using the sf package? What about for a raster file using the terra package? Why is the CRS important?\nWhat is the difference between a predicate and a measure?\nAs you look back on your first few assignments, what has been the biggest challenge? Do you feel like you know how to solve it? How can I help?\nAs you look back on the first few weeks of lectures, what is the one thing that you wish I would do differently (I’ll do my best!)\nTell me about your final project - what topic are you hoping to explore? What data are going to use? (Remember, this assignment works better if you’re using data that isn’t part of your thesis) If there are things I can do to help you find data or a topic, please let me know here.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html",
    "href": "assignment/07-rasterops.html",
    "title": "Assignment 7: Building spatial databases",
    "section": "",
    "text": "It’s time to put together the various vector, raster, and tabular code we’ve learned in order to build a dataframe that you can use for a subsequent statistical analysis. For this assignment we’ll be using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters from 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder, a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Your goal is to create a dataframe that includes the total cost of all disasters that have occurred within a National Forest Boundary along with several social and ecological variables that might help explain the difference in dollars expended to contain the hazard (typically fire). By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html#instructions",
    "href": "assignment/07-rasterops.html#instructions",
    "title": "Assignment 7: Building spatial databases",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-7-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-7-xxx.qmd and give it a title (like M Williamson Assignment 7). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-7-xx.qmd, and assignment-7-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html#the-assignment",
    "href": "assignment/07-rasterops.html#the-assignment",
    "title": "Assignment 7: Building spatial databases",
    "section": "The Assignment",
    "text": "The Assignment\n\nYou’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets.\nValidate your geometries and make sure all of your data is in the same CRS.\nSmooth the wildfire hazard and land use datasets using a 5x5 moving window; use the mean for the continuous dataset and the mode for the categorical dataset.\nEstimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\nNext join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\nMake a set of maps that shows the Forest-level values for all of your selected variables.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/09-pointpatterns.html",
    "href": "assignment/09-pointpatterns.html",
    "title": "Assignment 8: Spatial Autocorrelation and Interpolation",
    "section": "",
    "text": "Exploring and exploiting spatial autocorrelation in our data is an important tool for interpolating missing values in geographic data. It’s also an important diagnostic to check before additional statistical analyses. You’ll get to try some of that out here. We’ll the cost of natural disasters dataset fromm 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder and focusing on the data describing the structures lost to natural disaster (STR_DESTROYED_TOTAL). For this assignment, we’ll use this data to:"
  },
  {
    "objectID": "assignment/09-pointpatterns.html#instructions",
    "href": "assignment/09-pointpatterns.html#instructions",
    "title": "Assignment 8: Spatial Autocorrelation and Interpolation",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-8-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-8-xxx.qmd and give it a title (like M Williamson Assignment 8). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-8-xx.qmd, and assignment-8-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/11-statmod.html",
    "href": "assignment/11-statmod.html",
    "title": "Assignment 9: Statistical Analyses",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/11-statmod.html#instructions",
    "href": "assignment/11-statmod.html#instructions",
    "title": "Assignment 9: Statistical Analyses",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/13-thirdrevision.html",
    "href": "assignment/13-thirdrevision.html",
    "title": "Assignment 13: Revisiting your code (part 3)",
    "section": "",
    "text": "This is your final opportunity to reconsider your answers to the last few assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/final-proj.html",
    "href": "assignment/final-proj.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation techniques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#overview",
    "href": "assignment/final-proj.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation techniques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#requirements",
    "href": "assignment/final-proj.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\nDatasets. The ability to manipulate and integrate a variety of data types, resolutions, and formats is a key component of this course. Your analysis should incorporate at least 5 datasets. The ultimate compostion of your database is up to you, but I’d like you to include 1 tabular dataset, 1 vector dataset, and 1 raster dataset. You should choose the other 2 (or more) to give you practice with the data types that are most relevant to your objectives and/or research.\nAnalyses. You’ve learned several classes of analyses (e.g., overlays, point-pattern, multivariate regression, and statistical learning). Apply at least one (preferably the one most tied to your own objectives and research) of these analyses to address your question. In the course of doing so, you’ll need to justify your choice, assess whether your data meets appropriate assumptions, and evaluate the implications of key assumptions you make. For example, if you’re conducting an overlay analysis, how does your choice of threshold affect the ultimate result? If you’ve fit a statistical model based on summary statistics (e.g, mean, median), how well does the model fit? How does the model change if you use different slices of the data?\nVisualizations. You should produce a minimum of 3 visualizations to accompany your analysis. One of these should be a publication quality location map. The others are up to you, but should a) help you tell the story of your analysis and b) help you meet your objectives for the course and your own research. These can be additional maps of results, figures that summarize your data or results in non-spatial ways, or interactive graphics that allow you to explore parts of your analysis.\nReporting You can generate a ‘manuscript’ style document (using Quarto) or a flexdashboard (using Quarto and shiny) as the final product. Your report should include:\n\nA brief (1-2 paragraphs) description of your question and why you’re interested in it.\nA Methods section with subsections describing the data sources, any processing steps you took and why, and your process for the analysis. Show your code and provide annotation to describe what you are attempting do with the various steps.\nA Results section that includes tabular results as well as any relevant visualizations that describe your data and analysis.\nA Discussion of your results that puts your results in the context of your question, considers alternative analysis strategies and why they may or may not be better than the approach you chose, describes additional data that might be important for your question, and considers the role of extent and resolution in your analysis.",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#assessment",
    "href": "assignment/final-proj.html#assessment",
    "title": "Final Project",
    "section": "Assessment",
    "text": "Assessment\nYou’ll submit a draft of the final report on December 5. I’ll give you feedback based on your project and on your objectives for the course. You’ll then have a chance to address my feedback before turning in your final draft on December 12. Your final self-assessment will ask you to reflect on your objectives for the course and evaluate the degree to which your final project demonstrates that you achieved your objectives. Thus, when you are designing your project, make sure that you have your initial objectives in mind.\n\n\n\n\n\n\nTip\n\n\n\nA note on grades: You will be responsible for assessing how well your assignment demonstrates that you achieved your objectives. I reserve the right to change the grade you’ve given yourself, but will provide clear justification for why I’m doing that. Without a completed self-assessment there is no grade for your final project, so please make sure you complete it.",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/self-eval1.html",
    "href": "assignment/self-eval1.html",
    "title": "Self-reflection 1",
    "section": "",
    "text": "This is the first of three self-reflections that you’ll complete during the course. These provide an opportunity for me to learn more about you, check-in on how the course is going, and make sure that getting what you need from the course materials. This first reflection also helps us set the standards for your assessment in the course. As such, I’m asking that you complete this in a timely fashion and turn it in by Aug. 23. You’ll need to accept this link to access the questions.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Self-reflection 1"
    ]
  },
  {
    "objectID": "assignment/self-eval1.html#instructions",
    "href": "assignment/self-eval1.html#instructions",
    "title": "Self-reflection 1",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Self-reflection 1"
    ]
  },
  {
    "objectID": "assignment/self-eval3.html",
    "href": "assignment/self-eval3.html",
    "title": "Self-reflection 3",
    "section": "",
    "text": "This is the final self-reflection for the course and provides a way of evaluating your final project relative to your course learning objectives. This final self-reflection is critical for assigning your grades on the final project and the course, in general. As such, I’m asking that you complete this in a timely fashion and turn it in by Dec. 16. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval3.html#instructions",
    "href": "assignment/self-eval3.html#instructions",
    "title": "Self-reflection 3",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section has the code and description from the live-coding exercises that we’ll do in class.",
    "crumbs": [
      "Examples",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Fun datasets",
    "section": "",
    "text": "So much data, so little time… Here are some links to help you get started finding data for your geospatial projects",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#spatial-data-repositories",
    "href": "resource/data.html#spatial-data-repositories",
    "title": "Fun datasets",
    "section": "Spatial Data Repositories",
    "text": "Spatial Data Repositories\n\nDataBasin: Lots of spatial data related to conservation issues across the US.\nThe AdaptWest portal has tons of spatial data on climate change and its potential impacts.\nUS Protected Areas Database: PAD-US is America’s official national inventory of U.S. terrestrial and marine protected areas that are dedicated to the preservation of biological diversity and to other natural, recreation and cultural uses, managed for these purposes through legal or other effective means. PAD-US also includes the best available aggregation of federal land and marine areas provided directly by managing agencies, coordinated through the Federal Geographic Data Committee (FGDC) Federal Lands Working Group.\nUSGS Gap Analysis Project: A variety of datasets depicting land cover and species distributions.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#general-data-repositories",
    "href": "resource/data.html#general-data-repositories",
    "title": "Fun datasets",
    "section": "General Data Repositories",
    "text": "General Data Repositories\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#political-science-and-economics-datasets",
    "href": "resource/data.html#political-science-and-economics-datasets",
    "title": "Fun datasets",
    "section": "Political science and economics datasets",
    "text": "Political science and economics datasets\nThere’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\nInside AirBnB a Creative Commons-licensed dataset with a ton of spatially referenced info on AirBnBs in cities across the globe.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#the-30daymapchallenge",
    "href": "resource/data.html#the-30daymapchallenge",
    "title": "Fun datasets",
    "section": "The #30daymapchallenge",
    "text": "The #30daymapchallenge\nThe #30daymapchallenge is a social mapping/cartography/data visualization challenge designed to encourage experimentation with different types of datasets and mapping approaches. Searching the hashtag on social media (especially Twitter) will bring up a bunch of cool examples. Here are a few repositories to help you get started:\n\nThe Official #30DayMapChallenge Repo has an archive of past challenges and a description of what this is all about.\nBob Rudis’ 2019 bookdown project Contains both code and useful information for generating the visualizations along with sources for data.\nAlexandra Kapp’s 2020 repository makes use of some of the newer animation and interactive visualization techniques.\nThe R-Spatial list of 2020 challenge repositories",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to R and coding, potentially interesting data, and cool visualizations. Let me know when you find fun things to include here!",
    "crumbs": [
      "Resources",
      "Overview",
      "Resources"
    ]
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. \n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Getting started with R Spatial"
    ]
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. \n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Getting started with R Spatial"
    ]
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.2.1) and download it.\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nThe tidyverse consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nYou can install packages manually in RStudio, but this can be a bit fragile, especially for some of the spatial packages. Instead of using the RStudio GUI we’ll just install thins at the prompt. To install the tidyverse pacakge (and all of its associated dependencies) run the following: install.packages(\"tidyverse\").",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Intro to Spatial Data in R\n        ",
    "section": "",
    "text": "Intro to Spatial Data in R\n        \n        \n            Use R to load, visualize, and analyze spatial data\n        \n        \n            HES 505 • Fall 2024Human-Environment SystemsBoise State University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n   Carolyn Koehn\n   4034-1 Environmental Research Building\n   carolynkoehn@u.boisestate.edu \n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   August 19–December 6, 2024\n   1:30-2:45 PM\n   Riverfront Hall Room 102B\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 48 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "content/29-content.html#objectives",
    "href": "content/29-content.html#objectives",
    "title": "Data Visualization and Maps I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/31-content.html#objectives",
    "href": "content/31-content.html#objectives",
    "title": "Introduction to Interactive Maps",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Getting Started",
    "section": "",
    "text": "Today we’ll focus on getting oriented to the course and the tools we’ll be using throughout the semester. Readings are designed to help understand some of the ‘rules’ of R syntax and develop an understanding for manipulating different types of data in R. I’ve added a few on open science and reproducibility because I think they help make the case for learning to build code-based workflows.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Getting Started",
    "section": "Readings",
    "text": "Readings\n\nThe syllabus, content, examples, and assignments pages for this class\n Chapter 1 - 6 in Venables et al., An Introduction to R (Venables et al. 2009) - for a quick refresher on data types in R (it’s only 30 pages)\n Chapters 1-2 in Douglas et al., An Introduction to R - provides another intro to R that’s been updated and is an open-source book.\n Happy Git and GitHub for the useR - all you really need to know to be a proficient user of git for version control and reproducible workflows.\n Open science, reproducibility, and transparency in ecology by Powers and Hampton - discusses the importance of open science for ecologists.\n Practical Reproducibility in Geography and Geosciences by Nilst and Pebesma - describes the importance of reproducibility for geospatial analysis.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#objectives",
    "href": "content/01-content.html#objectives",
    "title": "Getting Started",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should:\n\nBe able to articulate the organization of the course, the approach to grading, and the requirements for the final project\nBe able to access the RStudio Server and Github classroom\nBe able to clone the first self-reflection and know the process for submitting assignments",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Getting Started",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Introduction to Spatial Data",
    "section": "",
    "text": "Now that you have a little more background in the breadth of philosophies, methods, and questions that geography encompasses, it’s time to start familiarizing yourself with the nature of spatial data. This lecture will be a little more conceptual, but is designed to help you make some sense for how R thinks about spatial data (or at least how the package developers have been thinking about it).",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "Introduction to Spatial Data",
    "section": "Readings",
    "text": "Readings\n Types of Spatial Data from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R provides a nice overview of the types of spatial data from the perspective of a statistical analyst.\n Attributes and Support from Pebesma and Bivand’s Spatial Data Science with Applications in R gives more info and examples on the nature of the relationship between geometries and support.\n Scale and Projections from Mapping, Society, and Technology by Laura Matson and Melinda Kernik gives a nice overview of the challenges associated with representing location on Earth’s surface.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#objectives",
    "href": "content/03-content.html#objectives",
    "title": "Introduction to Spatial Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nContrast the different “views” of spatial data and their incorporation in GIS.\nIdentify key elements that make data “spatial”.\nArticulate the importance of coordinate reference systems.\nRecognize the relationship between geometries and support.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Introduction to Spatial Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "",
    "text": "Today we’ll focus on some of the tools for reproducible workflows using R. We’ll introduce Quarto as a means of authoring different kinds of documents. We’ll talk about literate programming and leaving breadcrumbs for yourself (and others). Finally, we’ll begin to work through the ideas of workflow planning",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Readings",
    "text": "Readings\n\n Authoring in Quarto - an intro to Quarto for developing different kinds of documents. Lots of other resources linked here!!\n Pseudocode: what it is and how to write it - A nice blogpost by Sara Metawalli the sketches out the logic of pseudocode and why it can be helpful.\n The Whole Game - from Wickham et al., R for Data Science (Wickham and Grolemund 2016). Focus on the sections that begin with “Workflow” to get a sense for how we’ll start putting the pieces together.\n Scripts, algorithms, and functions - chapter 11 in in Lovelace et al., Geocomputation with R (Lovelace et al. 2019) introduces some concepts behind geospatial programming. A few of these pieces will make more sense in the next few weeks, but the general advice on constructing code and planning analyses is useful now.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#objectives",
    "href": "content/05-content.html#objectives",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDevelop basic docs with Quarto\nUnderstand the basics of creating readable code\nUse pseudocode to sketch out a computational problem",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Areal Data: Vectors",
    "section": "",
    "text": "Now that you have started working with the various components of coordinates and coordinate reference systems, it’s time to start learning the fundamental aspects of working with vector data in sf and R. The syntax is a little confusing at first, but once you’ve gotten a sense for the logic behind it you should be able to start piecing together the functions necessary to implement the pseudocode you write for an analysis. We’ll spend more time on vector manipulation in the coming weeks so you’ll get plenty of practice with the ideas we introduce today.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Areal Data: Vectors",
    "section": "Readings",
    "text": "Readings\nSame as last class really, but hopefully you’ll begin to understand them better…\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#objectives",
    "href": "content/07-content.html#objectives",
    "title": "Areal Data: Vectors",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nImplement approaches for checking and repairing geometries in R\nUnderstand predicates and measures in the context of spatial operations in sf\nUse st_* to evaluate attributes of geometries and calculate measurements",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Areal Data: Vectors",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Introduction to Mapping Geographic Data",
    "section": "",
    "text": "Now that we’re getting into actual operations on spatial data and beginning to actually modify the geometries and attributes of spatial data, it’ll be important for you to be able to visualize the results. At this point, we’ll be focusing on rough visualization as a way of “gut-checking” the outcomes of your code. We’ll focus more on creating informative, aesthetically pleasing, publication quality visualizations in section 4 of this course.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Readings",
    "text": "Readings\n\n Ch.3 Tmap in a nutshell from “Elegant and informative maps with tmap” by Martijn Tennekes and Jakub Nowosad provides a great “quick start” for using the tmap package for visualizing spatial data.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax.\n Making maps in R by Emily Burchfield illustrates some quick mapping syntax with base plot, ggplot, and tmap. For now, just focus on the base plot and tmap sections as we’ll take on the ggplot stuff later in the course.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#objectives",
    "href": "content/09-content.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Operations With Vector Data II",
    "section": "",
    "text": "Now that you have the complete picture of predicates, measures, and transformers; it’s time to use them on some actual data. This lecture is meant to be the “practical” application of the ideas you’ve learned in our previous discussions of vector data and give you enough tools to begin to subset your data to the records and attributes of interest, calculate new spatial metrics, and generate new geometries based on existing data.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Operations With Vector Data II",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages).",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#objectives",
    "href": "content/11-content.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Operations With Vector Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Operations with Raster Data II",
    "section": "",
    "text": "Now that we’ve done some “global” transformations of raster data using terra, we’ll look at some of the options for cell-wise transformations. Rather than manipulating the extent, resolution, or CRS of the raster data; we’ll actually be using functions to change the values of the cells themselves.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Operations with Raster Data II",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#objectives",
    "href": "content/13-content.html#objectives",
    "title": "Operations with Raster Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Operations with Raster Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Building Databases with Location",
    "section": "",
    "text": "Today we’ll continue our development of attributes (or covariates) in our spatial databases. We’ll look at developing attributes that describe various geographic properties along with joining and subsetting based on locations.",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/15-content.html#resources",
    "href": "content/15-content.html#resources",
    "title": "Building Databases with Location",
    "section": "Resources",
    "text": "Resources\n\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R.\n Attributes and Support of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/15-content.html#objectives",
    "href": "content/15-content.html#objectives",
    "title": "Building Databases with Location",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nGenerate new features using geographic data\nUse topological subsetting to reduce features based on geography\nUse spatial joins to add attributes based on location",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/19-content.html",
    "href": "content/19-content.html",
    "title": "Interpolation",
    "section": "",
    "text": "Point patterns give us the foundation for beginning geostatistical analyses. In geostatistical analyses, we have observations or a spatial process from a limited sample of locations, but would like to be able to infer the values of that process across the entire study region (or at least an area larger than we initially sampled). Interpolation provides one simple way of doing this that relies on the notion that we can learn something about the process simply from our measurements and the location those measurements were taken. We can extend these approaches by adding additional covariates and model structures, but we’ll start simple for now."
  },
  {
    "objectID": "content/19-content.html#resources",
    "href": "content/19-content.html#resources",
    "title": "Interpolation",
    "section": "Resources",
    "text": "Resources\n\n Chapter 2: Scale in (Fletcher and Fortin 2018) provides a thorough introduction to the ecologist’s conceptualization of scale with R examples.\n This article by Steven Manson (Manson 2008) provides a more comprehensive view of conceptualizations of scale.\n The Hypothesis Testing and Autocorrelation chapters of Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provide concrete examples of attempts to find process from spatial patterns.\n Chapter 12: Spatial Interpolation in Spatial Data Science by Edzer Pebesma and Roger Bivand provides examples of different types of kriging and interpolation using sf and stars."
  },
  {
    "objectID": "content/19-content.html#objectives",
    "href": "content/19-content.html#objectives",
    "title": "Interpolation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDistinguish deterministic and stochastic processes\nDefine autocorrelation and describe its estimation\nArticulate the benefits and drawbacks of autocorrelation\nLeverage point patterns and autocorrelation to interpolate missing data\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/21-content.html",
    "href": "content/21-content.html",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "Now that we’ve learned about the power of spatial autocorrelation for interpolation from point data, it’s time to explore methods for spatial autocorrelation with areal data. We’ll have to define neighbors because distance is a little more ambiguous here and then look at some global and local measures of autocorrelation."
  },
  {
    "objectID": "content/21-content.html#resources",
    "href": "content/21-content.html#resources",
    "title": "Spatial Autocorrelation",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights.\n\n Spatial Autocorrelation in R provides some easy code for working through neighbors with areal data and calculating spatial autocorrelation measures."
  },
  {
    "objectID": "content/21-content.html#objectives",
    "href": "content/21-content.html#objectives",
    "title": "Spatial Autocorrelation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse the spdep package to identify the neighbors of a given polygon based on proximity, distance, and minimum number\nUnderstand the underlying mechanics of Moran’s I and calculate it for various neighbors\nDistinguish between global and local measures of spatial autocorrelation\nVisualize neighbors and clusters\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/23-content.html",
    "href": "content/23-content.html",
    "title": "Statistical Modelling II",
    "section": "",
    "text": "Last class we spent some time extending the idea of Favorability to build a foundation for treating overlay analysis as a logistic regression. Although logistic regression has a number of properties that make it desirable for inference, a number of recently developed statistical learning approaches have greatly improved our ability to take advantage a wide variety of available data and generate spatially explicit predictions. These methods may make interpretation and inference more challenging, but can improve the predictive ability of your models. We’ll explore some of those today."
  },
  {
    "objectID": "content/23-content.html#resources",
    "href": "content/23-content.html#resources",
    "title": "Statistical Modelling II",
    "section": "Resources",
    "text": "Resources\n\n An Introduction to Statistical Learning by (James et al. 2021) is a comprehensive introduction to a number of statistical learning techniques with examples in R. Although these examples are not necessarily spatial, the chapters provide a lot of the background necessary for understanding what the models are doing.\n A statistical explanation of MaxEnt for ecologists by (Elith et al. 2011) provides a relatively accessible description of the details of MaxEnt species distribution modeling.\n Random forests for Classification in Ecology by (Cutler et al. 2007) provides an introduction to the utility of Random Forests for ecologists."
  },
  {
    "objectID": "content/23-content.html#objectives",
    "href": "content/23-content.html#objectives",
    "title": "Statistical Modelling II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate the differences between statistical learning classifiers and logistic regression\nDescribe several classification trees and their relationship to Random Forests\nDescribe MaxEnt models for presence-only data\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/25-content.html",
    "href": "content/25-content.html",
    "title": "Movement and Networks I",
    "section": "",
    "text": "Today we’re going to finish up some of the model evaluation and prediction pieces from last week and then take a brief detour into the use of spatial information in networks. To do that, we’ll need to introduce some basic concepts of networks before getting too far down the road of the syntax."
  },
  {
    "objectID": "content/25-content.html#resources",
    "href": "content/25-content.html#resources",
    "title": "Movement and Networks I",
    "section": "Resources",
    "text": "Resources\n\n Landscape connectivity: A graph-theoretic perspective by (Urban and Keitt 2001) introduces the notion of using networks and graphs to understand ecological connectivity.\n Connectivity for conservation: a framework to classify network measures by (Rayfield et al. 2011) helps simplify the (often overwhelming nature) of terminology and network metrics.\n Graphs and network science: and introduction provides a little simpler, but less spatially informed introduction to the tasks of network analysis."
  },
  {
    "objectID": "content/25-content.html#objectives",
    "href": "content/25-content.html#objectives",
    "title": "Movement and Networks I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse cross-validation to evaluate your models\nDefine a network and it’s key components\nIdentify major questions that we can address with networks\n\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the week. Read and watch these before our in-person class.\nLesson (): This page contains additional annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here, but it will be helpful as you work on your assignments.\nExample (): This page the scripts that we work on in class as a reminder of some of the live-coding exercises. These are provided as a reference to help you link your notes to the syntax we use in class.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nAugust 19(Session 1)\n\n\nIntroduction to the course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 21(Session 2)\n\n\nWhy Geographic Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 23\n\n\n Self-Evaluation 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 26(Session 3)\n\n\nIntroduction to Spatial Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 28(Session 4)\n\n\nIntroduction to Spatial Data with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 29\n\n\n Homework 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 2\n\n\nNo Class(Labor Day)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 4(Session 5)\n\n\nLiterate Programming, Quarto, Workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 5\n\n\n Homework 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Operations in R\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nSeptember 9(Session 6)\n\n\nAreal Data: Coordinates and Geometries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 11(Session 7)\n\n\nAreal Data: Vector Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 12\n\n\n Homework 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 16(Session 8)\n\n\nAreal Data: Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18(Session 9)\n\n\nMapping Geographic Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 19\n\n\n Homework 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 23(Session 10)\n\n\nOperations With Vectors I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 25(Session 11)\n\n\nOperations With Vectors II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 26\n\n\n Assignment Revision 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 30(Session 12)\n\n\nOperations With Rasters I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2(Session 13)\n\n\nOperations With Rasters II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 3\n\n\n Homework 6  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 7(Session 14)\n\n\nBuilding Spatial Databases with Attributes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 9(Session 15)\n\n\nBuilding Spatial Databases with Location\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 14(Session 16)\n\n\nCombining Vectors and Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 15\n\n\n Homework 7  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Workflows for Spatial Data\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nOctober 16(Session 17)\n\n\nPoint Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 21(Session 18)\n\n\nInterpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 22\n\n\n Assignment Revision 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 23(Session 19)\n\n\nProximity and Areal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 28(Session 20)\n\n\nSpatial Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 29\n\n\n Homework 8  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 30(Session 21)\n\n\nStatistical Modelling I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 4(Session 22)\n\n\nStatistical Modelling II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 6(Session 23)\n\n\nStatistical Modelling III\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 7\n\n\n Homework 9  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 11(Session 24)\n\n\nMovement and Networks I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 13(Session 25)\n\n\nMovement and Networks II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 14\n\n\n Homework 10  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Spatial Data\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nNovember 18(Session 26)\n\n\nData Visualization and Maps I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 20(Session 27)\n\n\nData Visualization and Maps II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 21\n\n\n Assignment Revision 3  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 25\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 27\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 2(Session 28)\n\n\nIntroduction to Interactive Maps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrapup\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nDecember 4(Session 29)\n\n\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 5\n\n\n Final Project Draft  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 11\n\n\nFinal Project Workday(optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 12\n\n\n Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 13\n\n\n Final Self-Evaluation Due  (submit by 11:59 PM)"
  },
  {
    "objectID": "slides/05-slides.html#for-today",
    "href": "slides/05-slides.html#for-today",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "For today",
    "text": "For today\n\nIntroduce literate programming\nDescribe pseudocode and its utility for designing an analysis\nIntroduce Quarto as a means of documenting your work\nPractice workflow"
  },
  {
    "objectID": "slides/05-slides.html#why-do-we-need-reproducibility",
    "href": "slides/05-slides.html#why-do-we-need-reproducibility",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why Do We Need Reproducibility?",
    "text": "Why Do We Need Reproducibility?\n\n\n\nNoise!!\nConfirmation bias\nHindsight bias\n\n\n\n\n\nMunafo et al. 2017. Nat Hum Beh."
  },
  {
    "objectID": "slides/05-slides.html#reproducibility-and-your-code",
    "href": "slides/05-slides.html#reproducibility-and-your-code",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducibility and your code",
    "text": "Reproducibility and your code\n\nScripts: may make your code reproducible (but not your analysis)\nCommenting and formatting can help!\n\n\n```{r}\n#| eval: false\n#|\n## load the packages necessary\nlibrary(tidyverse)\n## read in the data\nlandmarks.csv &lt;- read_csv(\"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2023/assignment01/landmarks_ID.csv\")\n\n## How many in each feature class\ntable(landmarks.csv$MTFCC)\n```"
  },
  {
    "objectID": "slides/05-slides.html#reproducible-scripts",
    "href": "slides/05-slides.html#reproducible-scripts",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducible scripts",
    "text": "Reproducible scripts\n\nComments explain what the code is doing\nOperations are ordered logically\nOnly relevant commands are presented\nUseful object and function names\nScript runs without errors (on your machine and someone else’s)"
  },
  {
    "objectID": "slides/05-slides.html#toward-efficient-reproducible-analyses",
    "href": "slides/05-slides.html#toward-efficient-reproducible-analyses",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Toward Efficient Reproducible Analyses",
    "text": "Toward Efficient Reproducible Analyses\n\nScripts can document what you did, but not why you did it!\nScripts separate your analysis products from your report/manuscript"
  },
  {
    "objectID": "slides/05-slides.html#what-is-literate-programming",
    "href": "slides/05-slides.html#what-is-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\n\n— Donald Knuth, CSLI, 1984"
  },
  {
    "objectID": "slides/05-slides.html#what-is-literate-programming-1",
    "href": "slides/05-slides.html#what-is-literate-programming-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!\nMultiple “scales” possible"
  },
  {
    "objectID": "slides/05-slides.html#why-literate-programming",
    "href": "slides/05-slides.html#why-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why literate programming?",
    "text": "Why literate programming?\n\nYour analysis scripts are computer software\nIntegrate math, figures, code, and narrative in one place\nExplaining something helps you learn it"
  },
  {
    "objectID": "slides/05-slides.html#planning-an-analysis",
    "href": "slides/05-slides.html#planning-an-analysis",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Planning an analysis",
    "text": "Planning an analysis\n\n\n\nOutline your project\nWrite pseudocode\nIdentify potential packages\nBorrow (and attribute) code from others (including yourself!)"
  },
  {
    "objectID": "slides/05-slides.html#pseudocode-and-literate-programming",
    "href": "slides/05-slides.html#pseudocode-and-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode and literate programming",
    "text": "Pseudocode and literate programming\n\nAn informal way of writing the ‘logic’ of your program\nBalance between readability and precision\nAvoid syntactic drift"
  },
  {
    "objectID": "slides/05-slides.html#writing-pseudocode",
    "href": "slides/05-slides.html#writing-pseudocode",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Writing pseudocode",
    "text": "Writing pseudocode\n\n\n\nFocus on statements\nMathematical operations\nConditionals\nIteration\nExceptions"
  },
  {
    "objectID": "slides/05-slides.html#pseudocode-1",
    "href": "slides/05-slides.html#pseudocode-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nStart function\nInput information\nLogical test: if TRUE\n  (what to do if TRUE)\nelse\n  (what to do if FALSE)\nEnd function"
  },
  {
    "objectID": "slides/05-slides.html#what-is-quarto",
    "href": "slides/05-slides.html#what-is-quarto",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nA multi-language platform for developing reproducible documents\nA ‘lab notebook’ for your analyses\nAllows transparent, reproducible scientific reports and presentations"
  },
  {
    "objectID": "slides/05-slides.html#key-components",
    "href": "slides/05-slides.html#key-components",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Key components",
    "text": "Key components\n\nMetadata and global options: YAML\nText, figures, and tables: Markdown and LaTeX\nCode: knitr (or jupyter if you’re into that sort of thing)"
  },
  {
    "objectID": "slides/05-slides.html#yaml---yet-another-markup-language",
    "href": "slides/05-slides.html#yaml---yet-another-markup-language",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "YAML - Yet Another Markup Language",
    "text": "YAML - Yet Another Markup Language\n\nAllows you to set (or change) output format\nProvide options that apply to the entire document\nSpacing matters!"
  },
  {
    "objectID": "slides/05-slides.html#formatting-text",
    "href": "slides/05-slides.html#formatting-text",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Formatting Text",
    "text": "Formatting Text\n\nBasic formatting via Markdown\nFancier options using Divs and spans via Pandoc\nFenced Divs start and end with ::: (can be any number &gt;3 but must match)"
  },
  {
    "objectID": "slides/05-slides.html#adding-code-chunks",
    "href": "slides/05-slides.html#adding-code-chunks",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Adding Code Chunks",
    "text": "Adding Code Chunks\n\nUse 3x ``` on each end\nInclude the engine {r} (or python or Julia)\nInclude options beneath the “fence” using a hashpipe (#|)"
  },
  {
    "objectID": "slides/05-slides.html#additional-considerations",
    "href": "slides/05-slides.html#additional-considerations",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Additional considerations",
    "text": "Additional considerations\n\nFile locations and Quarto\nCaching for slow operations\nModularizing code and functional programming"
  },
  {
    "objectID": "lesson/dataclasses.html",
    "href": "lesson/dataclasses.html",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Data Classes"
    ]
  },
  {
    "objectID": "lesson/dataclasses.html#data-types-and-structures",
    "href": "lesson/dataclasses.html#data-types-and-structures",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Data Classes"
    ]
  },
  {
    "objectID": "example/session-5-example.html",
    "href": "example/session-5-example.html",
    "title": "Session 5 Live Code",
    "section": "",
    "text": "This is the html output of the Quarto options we tested in class today.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#code-chunk-output-options",
    "href": "example/session-5-example.html#code-chunk-output-options",
    "title": "Session 5 Live Code",
    "section": "Code chunk output options",
    "text": "Code chunk output options\nCommon code chunk options (all true by default):\n\ninclude\neval\necho\nwarning\nmessage\n\n\n\n\nThis table shows what occurs when these options are set to false.\n\n\nWe used the following code to test code chunk options to generate this table. We also added code chunk labels.\n\n\nCode\n```{r}\n#| output: false\n#| label: \"read libraries\"\n\nlibrary(tidyverse)\nlibrary(sf)\n```\n\n\nI’m getting data from this source. In literate programming, we would add more detail!\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\n\n\nCode\n```{r}\n#| warning: false\n\nggplot(data = cejst, aes(x = AGE_10, y = AGE_MIDDLE)) +\n  geom_point()\n```\n\n\n\n\n\n\n\n\n\nYou can find more information on code chunk options here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#inline-code",
    "href": "example/session-5-example.html#inline-code",
    "title": "Session 5 Live Code",
    "section": "Inline code",
    "text": "Inline code\nWe implemented inline code by with the formula: `, r, a single space, some code, `.\nThe mean proportion of children 10 and under in the Northwest is 0.1140576.\nYou can find more information on inline code here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#heading-styles",
    "href": "example/session-5-example.html#heading-styles",
    "title": "Session 5 Live Code",
    "section": "Heading Styles",
    "text": "Heading Styles\n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n##### Heading 5\n###### Heading 6\nrenders as:",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#heading-2",
    "href": "example/session-5-example.html#heading-2",
    "title": "Session 5 Live Code",
    "section": "Heading 2",
    "text": "Heading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#text-formatting",
    "href": "example/session-5-example.html#text-formatting",
    "title": "Session 5 Live Code",
    "section": "Text formatting",
    "text": "Text formatting\n**bold** renders as bold\n_italics_ renders as italics",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#lists",
    "href": "example/session-5-example.html#lists",
    "title": "Session 5 Live Code",
    "section": "Lists",
    "text": "Lists\n\nBullet List\n- bullet 1\n- bullet 2\n- another sub-bullet\n- bullet 3\nrenders as:\n\nbullet 1\n\nbullet 2\n\nanother sub-bullet\n\n\nbullet 3\n\n\n\nNumbered List\n1. number 1\n2. number 2\nrenders as:\n\nnumber 1\nnumber 2",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#commenting",
    "href": "example/session-5-example.html#commenting",
    "title": "Session 5 Live Code",
    "section": "Commenting",
    "text": "Commenting\nTo comment out text outside code chunks, use &lt;!-- at the beginning of a comment and --&gt; at the end.\nMore information about text formatting in Quarto can be found here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "assignment/02-example.html",
    "href": "assignment/02-example.html",
    "title": "Quarto Example",
    "section": "",
    "text": "Introduction\nI want to reproduce the figure from (Dash Nelson 2016) depicting commute networks in the United States based on US Census data.\n\n\nMethods\nSome pseudocode:\n\n1. Retrieve ACS commute data\n2. Identify the source and destination networks\n3. Calculate the edge density for each source-destination pair\n4. Thin to a manageable number of nodes based on edge densities\n5. plot\n\nas code chunks\n\n```{r}\n#| eval: false\n#| label: getacs\n\n1. Retrieve ACS commute data\n```\n\n\n```{r}\n#| eval: false\n#| label: buildnet\n\n2. Identify the source and destination networks\n```\n\n\n```{r}\n#| eval: false\n#| label: edgedens\n\n3. Calculate the edge density for each source-destination pair\n```\n\n\n```{r}\n#| eval: false\n#| label: thinnodes\n\n4. Thin to a manageable number of nodes based on edge densities\n```\n\n\n```{r}\n#| eval: false\n#| label: buildplot\n\n5. Build plot\n```\n\n\n\nResults\n\n\n\nCommute Networks from Dash Nelson and Rae 2016\n\n\n\n\n\n\n\nReferences\n\nDash Nelson, A., Garrett AND Rae. 2016. An economic geography of the united states: From commutes to megaregions. PLOS ONE 11:1–23."
  },
  {
    "objectID": "example/session-08-example.html",
    "href": "example/session-08-example.html",
    "title": "Session 8 Code",
    "section": "",
    "text": "Playing with resolution:\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nr &lt;- rast(xmin=-4, xmax=9,\n          ncols=10)\nres(r)\n\n\n[1] 1.3 1.0\n\n\n\n\nCode\nr2 &lt;- rast(xmin=-4, xmax=9,\n           resolution = c(1.3, 1))\nncol(r2)\n\n\n[1] 10\n\n\n\n\nCode\nr3 &lt;- rast(xmin=-4, xmax=5,\n           ncols=10)\nres(r3)\n\n\n[1] 0.9 1.0\n\n\n\n\nCode\nempty_rast &lt;- rast()\n\n\n\n\nPractice questions\n\n\nCode\n# Question 1\nrr &lt;- rast(xmin=-5, xmax=5, ymin=-5, ymax=5, res=2)\n\n# By looking at the help file, we realized that res overrides nrows/ncols\n# This code uses the default global extent\nrr2 &lt;- rast(nrows=5, ncols=5, res=2)\n\n\n\n\nCode\n# Question 2\nvalues(rr) &lt;- runif(25)\n\n# The \"values were recycled\" error clues us in that we don't have 25 cells\nvalues(rr2) &lt;- runif(25)\n\n\nWarning: [setValues] values were recycled\n\n\n\n\nCode\n# Question 3\norigin(rr)\n\n\n[1] 1 1\n\n\n\n\nCode\n# Question 4\nvalues(rr)[adjacent(rr, cells=12)] &lt;- NA\nplot(rr)\n\n\n\n\n\n\n\n\n\nCode\n# You can do this in two steps \n# by making an object to hold the adjacent cell numbers\nadj &lt;- adjacent(rr, cells=12)\nvalues(rr)[adj] &lt;- NA\n\n\nFor more information on subsetting, see the Software Carpentries R for Reproducible Data Analysis: Subsetting Data lesson.\n\n\nCode\n# Question 5\nplot(distance(rr))\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\n\n\n\n\n\n\n\n\n\nExtra practice\nTo run examples of functions, go to the help file and scroll down to the “Examples” section. You can either copy-paste the code into your file or click the link that says “run examples.”\n\n\nCode\n# Question 2\n\nfire_rast &lt;- rast(\"/opt/data/data/assignment03/wildfire_hazard_agg.tif\")\nplot(distance(fire_rast))",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Predicates and Measures"
    ]
  },
  {
    "objectID": "slides/04-slides.html#objectives",
    "href": "slides/04-slides.html#objectives",
    "title": "Reading Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\n\nRevisit the components of spatial data\nDescribe some of the key considerations for thinking about spatial data\nIntroduce the two primary R packages for spatial workflows\nLearn to read and explore spatial objects in R"
  },
  {
    "objectID": "slides/04-slides.html#questions-from-monday",
    "href": "slides/04-slides.html#questions-from-monday",
    "title": "Reading Spatial Data in R",
    "section": "Questions from Monday",
    "text": "Questions from Monday\n\nWhy do we need a projection for calculations on a computer?\nWhat does it mean that a raster’s geometry is implicit?"
  },
  {
    "objectID": "slides/04-slides.html#reviewing-spatial-data",
    "href": "slides/04-slides.html#reviewing-spatial-data",
    "title": "Reading Spatial Data in R",
    "section": "Reviewing Spatial Data",
    "text": "Reviewing Spatial Data\nLet’s Kahoot!\nhttps://create.kahoot.it/share/isdr-session-4/888711f4-50a3-4732-a707-cbf68d9ae9dc"
  },
  {
    "objectID": "slides/04-slides.html#data-types-and-r-packages",
    "href": "slides/04-slides.html#data-types-and-r-packages",
    "title": "Reading Spatial Data in R",
    "section": "Data Types and R Packages",
    "text": "Data Types and R Packages\n\n\nData Types\n\nVector Data\n\nPoint features\nLine features\nArea features (polygons)\n\nRaster Data\n\nSpatially continuous field\nBased on pixels (not points)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "href": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: spreadsheets",
    "text": "Reading in Spatial Data: spreadsheets\n\n\nMost basic form of spatial data\nNeed x (longitude) and y (latitude) as columns\nNeed to know your CRS\nread_*** necessary to bring in the data\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\nfile.to.read &lt;- read_csv(file = \"path/to/your/file\", \n                         col_names = TRUE, col_types = NULL, \n                         na =na = c(\"\", \"NA\"))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, \n                       coords = c(\"longitude\", \"latitude\"), \n                       crs=4326)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\nALL FILES NEED TO BE IN THE SAME FOLDER\n\n\n\n\n\n.shp is the shapefile itself\n.prj contains the CRS information\n.dbf contains the attributes\n.shx contains the indices for matching attributes to geometries\nother extensions contain metadata\n\n\n\n\n\nst_read and read_sf in the sf package will read shapefiles into R\nread_sf leaves character vectors alone (often beneficial)\nst_read can handle other datatypes (like geodatabases)\nReturns slightly different classes"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\n\n\nlibrary(sf)\nshapefile.inR &lt;- read_sf(dsn = \"path/to/file.shp\")"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "href": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: rasters",
    "text": "Reading in Spatial Data: rasters\n\nrast will read rasters using the terra package\nAlso used to create rasters from scratch\nReturns SpatRaster object\n\n\n\n\nlibrary(terra)\nraster.inR &lt;- rast(x = \"path/to/file.tif\", \n                         lyrs=NULL)"
  },
  {
    "objectID": "slides/04-slides.html#introducing-the-data",
    "href": "slides/04-slides.html#introducing-the-data",
    "title": "Reading Spatial Data in R",
    "section": "Introducing the Data",
    "text": "Introducing the Data\n\nGood idea to get to know your data before manipulating it\nstr, summary, nrow, ncol are good places to start\nst_crs (for sf class objects) and crs (for SpatRaster objects)\nWe’ll practice a few of these now…"
  },
  {
    "objectID": "slides/04-slides.html#saving-your-data",
    "href": "slides/04-slides.html#saving-your-data",
    "title": "Reading Spatial Data in R",
    "section": "Saving your data",
    "text": "Saving your data\n\nwrite_sf for sf objects; writeRaster for SpatRasters\n\n\nlibrary(sf)\nlibrary(terra)\n\nwrite_sf(object = object.to.save, dsn = \"path/to/save/object\", append = FALSE)\nwriteRaster(x=object, filename = \"path/to/save\")"
  },
  {
    "objectID": "example/session-09-example.html",
    "href": "example/session-09-example.html",
    "title": "Intro to Mapping",
    "section": "",
    "text": "Base plot methods:\nLoad library and vector data:\n\n\nCode\nlibrary(sf)\n\ncejst &lt;- st_read(\"/opt/data/data/assignment04/cejst_nw.shp\")\n\n\nPlot vector data:\n\n\nCode\nplot(st_geometry(cejst))\n\nplot(cejst$geometry)\n\n\n\n\n\n\n\n\n\nCode\nplot(cejst[\"EALR_PFS\"])\n\n\n\n\n\n\n\n\n\nSee column name meanings:\n\n\nCode\nView(read.csv(\"/opt/data/data/assignment04/columns.csv\"))\n\n\nRead in library and raster data:\n\n\nCode\nlibrary(terra)\n\nrast.data &lt;- rast(\"/opt/data/data/assignment03/wildfire_hazard_agg.tif\")\n\n\nPlot raster:\n\n\nCode\nplot(rast.data)\n\n\n\n\n\n\n\n\n\nCode\nplot(rast.data, col=heat.colors(24, rev=TRUE))\n\n\n\n\n\n\n\n\n\nCombine raster and vector data:\n\n\nCode\nplot(rast.data, col=heat.colors(24, rev=TRUE))\nplot(st_geometry(st_transform(cejst, crs=crs(rast.data))), add=TRUE)\n\n\n\n\n\n\n\n\n\nCombining two vectors:\nIn class, we could not get the bounding box to appear. The fix is to plot the bounding box before the census tracts. Why would this be? plot will only plot a geometry if the entire shape fits in the current plot window. Because of rounding error introduced in st_as_sfc and st_transform, the bounding_box polygon is slightly larger than the plot window. Because plot couldn’t fit all its vertices, the bounding box did not appear.\n\n\nCode\nbounding_box &lt;- st_as_sfc(st_bbox(cejst))\n\nplot(st_geometry(st_transform(bounding_box, crs=st_crs(cejst))), col=\"red\")\nplot(cejst[\"EALR_PFS\"], add=TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# note how xmax of the bounding_box object is slightly higher than the true xmax\nst_bbox(cejst)\n\n\n      xmin       ymin       xmax       ymax \n-124.76255   41.98801 -111.04349   49.00249 \n\n\nCode\nst_coordinates(st_geometry(st_transform(bounding_box, crs=st_crs(cejst))))\n\n\n             X        Y L1 L2\n[1,] -124.7625 41.98801  1  1\n[2,] -111.0435 41.98801  1  1\n[3,] -111.0435 49.00249  1  1\n[4,] -124.7625 49.00249  1  1\n[5,] -124.7625 41.98801  1  1\n\n\n\n\ntmap methods:\n\n\nCode\nlibrary(tmap)\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(viridis)\n\n\nLoading required package: viridisLite\n\n\nCode\ncejst_filt &lt;- cejst %&gt;%\n  filter(!st_is_empty(.))\n\npt &lt;- tm_shape(cejst_filt) +\n  tm_polygons(col = \"EALR_PFS\", n=10, palette=viridis(10),\n              border.col = \"white\") +\n  tm_legend(outside = TRUE)\n\n\nLayering in tmap:\n\n\nCode\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS %in% c(\"ID\", \"WA\", \"OR\")) %&gt;% \n  st_transform(., crs = st_crs(cejst))\n\n\nRetrieving data for the year 2021\n\n\nCode\npt2 &lt;- tm_shape(cejst_filt) +\n  tm_polygons(col = \"EALR_PFS\", n=10, palette=viridis(10),\n              border.col=\"white\") +\n  tm_shape(st) +\n  tm_borders(col=\"red\") +\n  tm_legend(outside = TRUE)\n\n\nLayering a raster in tmap:\n\n\nCode\ncejst.proj &lt;- st_transform(cejst, crs=crs(rast.data)) %&gt;% filter(!st_is_empty(.))\nstates.proj &lt;- st_transform(st, crs=crs(rast.data))\n\npal8 &lt;- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\n\npt3 &lt;- tm_shape(rast.data) +\n  tm_raster() +\n  # tm_shape(cejst.proj) + \n  # tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n  #             border.col = \"white\") + \n  tm_shape(states.proj) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)\n\n\nYou can use tmap_mode(\"view\") to enable zoom on your maps.",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Intro to Mapping"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html",
    "href": "lesson/getting-setup.html",
    "title": "Getting Set Up",
    "section": "",
    "text": "This is an intro to git and github, most of which was originally created by Jessica Taylor and Nora Honkomp (2 former students in this class!).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#lets-git-started",
    "href": "lesson/getting-setup.html#lets-git-started",
    "title": "Getting Set Up",
    "section": "Let’s “git” started",
    "text": "Let’s “git” started\nWe are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\nAccept the invitation to the assignment repo\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installload-required-package",
    "href": "lesson/getting-setup.html#installload-required-package",
    "title": "Getting Set Up",
    "section": "Install/Load Required Package",
    "text": "Install/Load Required Package\nLoad the following packages in RStudio. If you do not have them installed, you can do so using install.packages().\n\n\nCode\nlibrary(usethis)\nlibrary(gitcreds)\nlibrary(knitr)",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#create-a-github-account",
    "href": "lesson/getting-setup.html#create-a-github-account",
    "title": "Getting Set Up",
    "section": "Create a GitHub Account",
    "text": "Create a GitHub Account\nYou will need to access GitHub with an account for this tutorial. If you don’t already have an account, you can sign up for free here: https://github.com/\nYou will be asked to sign up using your email, a password you create, and a username. Your username is what will be visible to others that you collaborate with, so it’s a good idea to make it something straight forward and professional. You can skip personalization for now by scrolling to the bottom of the page.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#github-and-rstudio-server",
    "href": "lesson/getting-setup.html#github-and-rstudio-server",
    "title": "Getting Set Up",
    "section": "Github and RStudio Server",
    "text": "Github and RStudio Server\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\nBring the project into RStudio\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\nVerify that the “Git” tab is available and that your project is shown in the upper right-hand corner\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "href": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "title": "Getting Set Up",
    "section": "Installing Git (for your local machine)",
    "text": "Installing Git (for your local machine)\nYou will need the program Git for this tutorial. First, let’s check to see if Git is already installed.\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console (bottom-left panel). Enter the following:\n\n\nCode\nwhich git\ngit --version\n\n\n/c/Program Files/Git/cmd/git\ngit version 2.45.2.windows.1\n\n\nYou will get something like the above output if Git is already installed. If it is not installed, you will get something like git: command not found.\n\nWindows\nDownload the program here: https://git-scm.com/downloads\n\n\nMac\nYou may have been prompted to install command line developer tools. You should accept this offer. If you were not prompted, use this command:\n\n\nCode\nxcode-select --install\n\n\n\n\nIntroduce Yourself to Git in R\nOnce Git is successfully installed and you have a GitHub account, you will need to let R/Git know your account information to access the remote repositories.\nEnter the following in the Console tab, substituting the user name and email with your GitHub user name and email:\n\n\nCode\nusethis::use_git_config(user.name = \"Bob Barker\", user.email = \"bobbarker@thepriceisright.org\")\n\n\n\n\nGet a Personal Access Token\nA personal access token (PAT) is used for GitHub as a type of authentication. You used to be able to use your username and password, but not any more. The token-based authentication has increased security.\nBefore generating a new PAT, check to see if you already have one. Run gitcreds_set() and one of two things will happen:\n\n\nR will prompt you to enter a token. This means you don’t already have one and need to create one. Hit Esc and run the second line of code, create_github_token(). This will bring you to GitHub where you can create a token. Save this token somewhere safe - you will not be able to access it via GitHub again.\nR will show you the saved credentials (username and password) and give you options to exit without changing (1: Abort), replace the credentials, or to see the current token. This means you already have a token and can keep it or change it if it has expired. If it expired and you need a new one, run create_github_token().\n\n\n\nCode\n## Run this to see if you already have credentials, or to change them\ngitcreds::gitcreds_set()\n## Only run this if you need to generate a token\ncreate_github_token()",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#version-control",
    "href": "lesson/getting-setup.html#version-control",
    "title": "Getting Set Up",
    "section": "Version Control",
    "text": "Version Control\nThe main function of Git is version control. This means Git will track the changes you make so you can revert to (or view) previous versions of the document. In order for this to work effectively, you need to start tracking your changes (make a repository) and frequently create versions with changes you have made (committing).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-a-local-repository",
    "href": "lesson/getting-setup.html#setting-up-a-local-repository",
    "title": "Getting Set Up",
    "section": "Setting Up a Local Repository",
    "text": "Setting Up a Local Repository\nAnother function of Git is to store all your relevant files in a repository, similar to a project. You can create a repository that is located only on your device (therefore it is considered “local”). This will allow you to track changes to a project on your computer and access previous versions of the document at any time. We will look at how to create a local repository using Rstudio, but know that it is also possible to do this using the terminal. (See page 96 of Gandrud (2015))\n\nIn Rstudio, select File in the top left corner, and then New Project\nWhen the New Project menu appears, select New Directory\n\n\nOn the following screen, select New Project\nFinally, type the name you want to use for your new project, browse to the location where you want it saved on your computer, and select the Create a git repository box.\n\n\nYou now have a folder with a .Rproj object and a .gitignore file. There is also a hidden .git folder that stores all the project information, including the version history files (commit history).\n\nAdding, Staging, and Committing\nYou can create new files or move existing files (including your data) into this project folder, and Git can track changes made to them. In order to do this, you will need to “add” files that have been created or moved into the project and “commit” these changes.\nFirst, you will need to save the new file or move an existing file into the project folder. Then, you will “add” those files to your commit by checking the boxes in the Git tab in your Rstudio window. If Git is already tracking a document, you will see an M in the status column, however, if the file hasn’t been added (and is therefore not being tracked), you will see a ? in the status column.\n\nIn order to save a version of your project as it is right now (with these added files), you will need to commit. To do this, select the Commit button, above the boxes you just checked. This will open a new window where you can see the list of files that you added.\nType a useful message in the Commit message box briefly describing the changes you made in this version. Then hit Commit.\n\nYou should see a window pop up telling you what changes were successfully made. If this window ever says “failed”, “execution halted”, or “aborted”, this means the commit did not work and you should read the message closely to determine why.\nSave and commit frequently in order to get the most use out of your version control.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#branches-and-merging",
    "href": "lesson/getting-setup.html#branches-and-merging",
    "title": "Getting Set Up",
    "section": "Branches and Merging",
    "text": "Branches and Merging\n\nBranches\nA new repository will have one branch called main. You can think of this as the master version. You can create additional branches which are an exact copy of the main branch where you can make changes without committing them to the master. This is useful when several people are working on the same thing, or if you want to try multiple approaches to the same file (i.e. test run some code).\nTo create a branch, select the button with small purple shapes on the right hand side of your Git tab in Rstudio.\n\nYou can now create a new branch, for example one called “test”. This is essentially a copy of everything that is in your repository, and making changes here will not affect the main branch.\nTo switch between branches, select the drop down menu to the right of the button you used to create a new branch.\n\nYou can easily switch to a different branch by selecting it on this menu.\nNote that any files you create on a side branch will not show up in the main branch unless you merge them.\nYou can have multiple branches coming off your main branch at once, so you can try multiple different approaches (or by multiple people) simultaneously. Anytime you make a new branch it will start as a duplicate of the main branch.\nYou can see a history of your commits on different branches by selecting the clock icon (designating “History”) near the Commit button on your Git tab. You will initially see the history for only the branch you are currently in, but if you select the drop down menu of branches at the top of this window, you can select another branch or all branches and see how the branches compare to one another.\n\n\n\nMerging\nIf you find a method that works in a side branch and you want to bring it in to the main branch, this is called a merge.\nTo merge a side branch with the main branch (changing the version of your repo main branch to match that of the successful side branch), we will use the terminal.\n\nMake sure you are on the main branch\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console.\nType git merge and then the name of the branch you want to merge into the main branch.\n\n\nThe repository of your main branch should now match the repository of whatever branch you merged with it.\nOne way to check if the branches merged the way you intended is to select the history button again (make sure you go to all branches) and check that the “HEAD” (which is the main branch) is at the same level as whichever branch you merged it with.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-remote-repositories",
    "href": "lesson/getting-setup.html#setting-up-remote-repositories",
    "title": "Getting Set Up",
    "section": "Setting Up Remote Repositories",
    "text": "Setting Up Remote Repositories\nTo set up a remote repository, make sure you are able to log into GitHub. We will go over three different ways to make repositories: starting from scratch with a new repository, creating a repository for an already started project, and accessing a repository made by a collaborator.\n\nNew Repository\nTo create a brand new repository, click on the plus sign near your profile picture in the top right corner of GitHub and select New repository.\n\nOn the next page, type in a name for your repository and choose whether you want it to be public or private. It is recommended you select the box to create a README file. This will give you a place to describe the layout of your repository and the purpose of each file. Without a README file, visitors to your repository (and maybe future you) might not be able to figure out how to properly use the files in your repository, leading to your working being non-reproducible.\n\nNow you can click Create Repository.\nOnce your repository is created, you will see only the README file is present. To create other files, let’s create a directory for this repo on our computer.\n\nIn R studio, follow the instructions for creating a new project, but when you see the following menu, select Version Control this time.\n\n\n\n\nOn the next window, select Git.\nReturn to the GitHub page for the repository you created and select the green Code button.\nCopy the HTTPS link from the menu that pops up.\n\n\n\nReturn to Rstudio and paste the link in the Repository URL line at the top of the popped up window. Make sure to check the file path that is currently set and use the Browse button if you want to save the folder for this directory in a different place.\n\n\nYour repository is now set up in a new project in Rstudio, and you can begin by creating an Rmarkdown, R Script, etc.\nNote, you can edit the contents of the README file by opening it in Rstudio.\n\n\nFrom an Existing Project\nYou may find yourself wanting to create a repo on GitHub for a project you have already started working on. Fortunately, it is easy to start version control tracking on a project and add the project to a remote repository.\nHere are the steps to follow if you have a project folder with a .Rproj file in it and your directory is not already being tracked with Git:\n\nFollow the steps above for creating a New repository, but this time do not click the box to create a README file. This will bring you to a page with a “Quick Set Up” link. Keep this page open for later.\n\n\n\n\nGo to RStudio and open the .Rproj file that will be added to the GitHub repository./\nSelect the Terminal tab next to Console and enter the following lines of code.\n\n\n\nCode\n$ git init -b main\n\n\n“git” tells bash what program we want to use   “init” tells bash to initialize a Git repository fir this directory\n“-b main” is saying we want to create a branch called “main”\n\n\n\nCode\n$ git add .\n\n\nThis will add all of the files within the current folder to the repository. You may get a lot of warnings because you have a .Rproj file and potentially a .Rhistory file in this folder which are not usually ideal to track. We will take care of this by adding these file names to the .gitignore file shortly.\n\n\nCode\n$ git commit -m \"First commit\"\n\n\nThis line creates your first commit. You can change the commit message to anything that makes sense to you.\n\nGo back to GitHub and copy the Quick start link. Type the following code in the terminal, but replace &lt;REMOTE_URL&gt; with the copied link.\n\n\n\nCode\n$ git remote add origin &lt;REMOTE_URL&gt;\n# Replace \"&lt;REMOTE_URL&gt;\" with url from GitHub\n\n\n\nLast, we will push these changes to GitHub. We will further discuss what this means later, but for now run the following line in the terminal.\n\n\n\nCode\n$ git push origin main\n\n\nNow when you return to GitHub and refresh your repository page, you should see all of the files from your existing project.\nYou can add a README and .gitignore on the GitHub website by selecting “Add file” on the repository’s page. When you make a .gitignore file, it may suggest you use a template, in which case, select the R template from the drop down list and it will automatically fill the document with file types that should typically not be tracked.\n\n\nCloning a Repository\nIf you want to join a repository that has already been created, you can either find the repository by searching for it on GitHub (if it is publicly available) or contact the creator and have them add you as a collaborator to the repository. Either way, you will follow steps 1-5 in the “New Repository” section above, but this time the link is coming from the already created repository.\nIn cases where the repository is not accessible (you do not have cloning priveleges), you will have to create a pull request.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#pushing-and-pulling",
    "href": "lesson/getting-setup.html#pushing-and-pulling",
    "title": "Getting Set Up",
    "section": "“Pushing” and “Pulling”",
    "text": "“Pushing” and “Pulling”\nOnce you have your remote repository ready, you can make changes to the files and “add” and “commit” them like we did in the “Version Control” section above. However, now we must take steps to make sure our local version of the repository is up-to-date with the online version, and the versions that all other collaborators have on their computers.\nTo do this, we will need to “push” and “pull”. “Pulling” is when we bring recent and out-of-sync changes from the online version to our local device. “Pushing” is the opposite; we are sending our commits to the online version to update it with our recent changes. This will allow anyone else working in the repository to “pull” your changes onto their computer.\nTo push and pull, use the blue and green arrows on the Git tab in R studio.\n\nYou may also notice the next time you “commit” your changes, these same push and pull buttons are located above the “Commit Message” box on the pop up window.\nIt is good practice to “pull” each time you are about to start working in a repository and to push after you commit, or at least once at the end of your working period within a repository for the day. Keeping good “push” and “pull” habits will help you avoid merge conflicts with collaborators or yourself if you work on a project on more than one computer.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "href": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "title": "Getting Set Up",
    "section": "Adding and Managing Collaborators",
    "text": "Adding and Managing Collaborators\nCollaborative coding is a huge benefit of GitHub. In order to invite your collaborators to clone your remote repository, you will need to know their GitHub username, or at least their email address.\n\nOn your repository page on GitHub, select Settings in the middle of the banner near the top of the page.\n\n\n\nIn the menu on the left, select the Collaborators page.\n\n\n\nEnter your password, and then select Add People under Manage Access\n\nAdd your collaborators one at a time. This will send them a message inviting them to join the repository.\nAs the owner of the repository, you will be able to remove people from the repository at any time.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#merge-conflicts",
    "href": "lesson/getting-setup.html#merge-conflicts",
    "title": "Getting Set Up",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts\nWhen two people are working on a branch at the same time, changes were made and not pushed before someone else started working, or a collaborator forgets to pull before starting to make changes, a merge conflict may arise. Merge conflicts happen because Git is not sure how to combine the different changes that occurred in the same sections of the document. In some instances, Git is smart enough to figure it out and will merge the versions on its own. Other times, the merge conflict will be need to be fixed manually. Note that Git will not allow the push until the merge conflict is solved.\nIf a merge conflict occurs, you will see some specific things added to your code. There will be a line at the beginning that starts with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, a line at the end starting with &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a line in the middle that just has =======. The two different version are shown above and below the middle line, labeled with which branch has which version. It is your job to decide how these versions fit together. Once you have modified the code to match the final version you want to keep, you can delete the three lines of code containing the &gt;, &lt; and = symbols and “stage” and “commit” your files as usual.\n\n\nOn GitHub\nYou can also merge branches and resolve merge conflicts on GitHub. You would push your branch to GitHub, then go to the GitHub repo webpage. You will need to create a pull request to merge your branch. If there is a merge conflict, you will need to click on “Resolve conflicts” before you can merge your branch. The syntax to edit the document and resolve the conflict is the same as above.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "href": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "title": "Getting Set Up",
    "section": "Cloning, Branching, and Forking, Oh My!",
    "text": "Cloning, Branching, and Forking, Oh My!\nCloning, branching, and forking are functions that are similar, but they are not the same. When you clone a repository, you are connected to it and are working in that repository. You can commit changes and push them to the same repository. When you create a branch, you create a copy where you can work on a specific part of a document or run test code with the intent to merge it back to the main branch. Many branches are short-lived and deleted once their purpose has been served. Anyone that has access to the repository also has access to the branches in it. A fork creates a copy of the entire repository as well, however, the collaborators are disconnected from it. The intent is generally to diverge from the original repository and never be merged back into it.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#gitignore-and-large-files",
    "href": "lesson/getting-setup.html#gitignore-and-large-files",
    "title": "Getting Set Up",
    "section": ".gitignore and Large Files",
    "text": ".gitignore and Large Files\nGitHub does not allow repositories to be larger than 5GB. While we recommend keeping all of the files necessary to run your analysis together in the repo, this may not be possible if say for example your data files are larger than the size limit. If this is the case, you can manually distribute your data file a different way. (See https://git-lfs.github.com/ first though if this is an issue you are actually having.)\nOnce all collaborators (or just you) have the large data file in your Git repo, you may forget that you cannot send this to GitHub and accidentally try to commit and push it. Fortunately, Git will recognize the problem before it takes place and will give you the following warning:\n\nTo resolve this problem, you just need to tell Git to ignore the large file when you make your commit. First, determine which file(s) is/are too big by looking at their size. Then, navigate to your .gitignore document and open it. Last, add the name(s) of the file(s) that is/are too large. Now you will be able to commit and push the tracked files within your repository.\nYou can add any files you do not want to be tracked to .gitignore. An example is the html file generated by rendering this document. It is regenerated constantly, so there is no need to track the changes.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#amend-commits",
    "href": "lesson/getting-setup.html#amend-commits",
    "title": "Getting Set Up",
    "section": "Amend Commits",
    "text": "Amend Commits\nIf at any point you find you have made a commit that should not have been made (e.x. you accidentally added and committed files that are too large and now you aren’t able to push), you can easily fix this by making the necessary changes within your files, checking the “amend previous commit” box on the commit screen, and then committing as usual. This will overwrite your previous commit with the correct version you want to commit. If you need to amend an earlier commit (i.e. not the most recent commit), you will need to use the terminal (See Oh S#!*, Git!?!).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installing-a-git-client",
    "href": "lesson/getting-setup.html#installing-a-git-client",
    "title": "Getting Set Up",
    "section": "Installing a Git Client",
    "text": "Installing a Git Client\nWith all of the commits, branching, merging, and collaboration, it can be tricky to keep track of everything going on in your repository. Viewing the commit history in GitHub or under the Git tab in RStudio is useful, though it can still be a little hard to follow. Using a Git client software helps visualize the workflow and allows you to use commands in the graphic user interface (GUI) that you would normally need to type into the terminal.\n\nThere are a few Git Clients out there and you may want to try a few to see what works for you. I use GitKraken which you can download here.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "assignment/06-vectoropssolutions.html",
    "href": "assignment/06-vectoropssolutions.html",
    "title": "Assignment 6 Solutions: Vector Operations",
    "section": "",
    "text": "We want to begin to assess the role of distance from schools in determining the education outcomes for Idahoans. We’ll use the landmarks_pnw.csv and cejst_pnw.shp datasets as the basis for this assignment. You’ll need to load the csv and convert it to an sf object. We want to compare the percentage of individuals age 25 or over with less than a high school degree (HSEF in the cejst dataset) for of counties within 50km of a school (MTFCC == K2543) to those that are more than 50km. \nYou’ll need to follow many of the same operations in the video example from class. Your assignment is:\n1. Write out the pseudocode for your analysis\n\nWe’ll need to do a few things here including load the data, find the tracts within 50km of a school, and then compare the cejst results. Breaking that into pseudocode would look like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Find tracts within 50km\n6. Make Maps\n\n\nNote that my fifth step (find tracts within 50km) is a little vague. There are lots of ways I could do this. It might be more helpful to add some specificity here like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Buffer schools by 50km\n6. Select tracts within the buffer and attribute\n7. Make Maps\n\n\nThere are other ways to do this too (like calculating the distance), but those are likely to be more computationally intensive so I’ll leave it at this.\n\n2. Translate the pseudocode into code chunks and create the necessary code (You’ll need to use things like st_distance, st_buffer, st_sym_difference)\n\nLoading the data should be pretty straightforward for you by now. We use read_sf for the shapefile and read_csv for the landmarks.csv. We then filter the data here so that we aren’t working with the entire landmarks dataset.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\n\ncejst.pnw &lt;- read_sf(\"/opt/data/data/assignment06/cejst_pnw.shp\")\nlandmarks.pnw &lt;- read_csv(\"/opt/data/data/assignment06/landmarks_pnw.csv\") %&gt;% \n  filter(., MTFCC == \"K2543\")\n\n\nWe know that one of the datasets are still in long/lat form so we’ll need to make it a sf object before checking the geometry makes any sense. We’ll also assign the crs here by adding it to the st_as_sf call. We also need to make sure that there aren’t any empty geometries as that will cause problems for mapping later.\n\n\nlandmarks.sf &lt;- landmarks.pnw %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"), crs=4269)\nall(st_is_valid(cejst.pnw))\n\n[1] TRUE\n\nall(st_is_valid(landmarks.sf))\n\n[1] TRUE\n\nany(st_is_empty(cejst.pnw))\n\n[1] TRUE\n\nany(st_is_empty(landmarks.sf))\n\n[1] FALSE\n\n\n\nLooks like all the geometries are valid, but there are some empty geometries in the cejst dataset. We will drop those by using filter combined with the negation operator (!) and st_is_empty to return the rows where st_is_empty is not equal to TRUE. Then we can move forward with making sure the two datasets are aligned by using st_transform to change the CRS. We can verify the alignment using a simple call to the plot function.\n\n\ncejst.pnw &lt;- cejst.pnw %&gt;% \n  filter(., !st_is_empty(.))\nlandmarks.proj &lt;- landmarks.sf %&gt;% \n  st_transform(., crs=st_crs(cejst.pnw))\nplot(st_geometry(cejst.pnw))\nplot(st_geometry(landmarks.proj), add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n\nNow it’s time to find the tracts that are within 50km of a school. We can do this a few ways. First, we’ll use the st_buffer approach. We can also calculate the distance matrix between the schools and the tracts using st_distance. This adds a little more complexity as we have to then find the values that are greater than 50km. You’ll notice that st_distance returns a matrix with a row for each school and a column for each tract. This is a little clumsier to deal with, but more precise than a simple buffer.\n\n\nschool.buf &lt;- landmarks.proj %&gt;% \n  st_buffer(., dist=50000) \n\nschool.dist &lt;- st_distance(landmarks.proj, cejst.pnw)\ndim(school.dist)\n\n[1]  220 2571\n\n\n\nOnce we have the buffered “footprint” of the school we can use st_filter (which filters using topological relations) combined with the st_covered_by predicate to find all of the cejst.pnw tracts that are covered by the buffer. Notice that if we use the typical [] subset we get over 200 more records. This is because the latter takes all tracts with an intersection (rather than using our covered by criteria.). We can alter this to achieve the same result as the st_filter by adding the op= argument. Using the distance to the points themselves can be a more conservative way of calculating this, but takes a little more work. First we have to get a list of all of the tracts that fall within 50km (using st_is_within_distance), then identify which of those list elements are empty (i.e., no schools are within 50km), then set that as an index to subset our cejst data.\n\n\nschool.tracts.stf &lt;- cejst.pnw %&gt;% \n  st_filter(x =., y = school.buf, .predicate = st_covered_by)\nschool.tracts.sbst &lt;- cejst.pnw[school.buf,]\nschool.tracts.sbst2 &lt;- cejst.pnw[school.buf,, op=st_covered_by]\n\nnrow(school.tracts.stf)\n\n[1] 2285\n\nnrow(school.tracts.sbst)\n\n[1] 2512\n\nnrow(school.tracts.sbst2)\n\n[1] 2285\n\nidentical(school.tracts.stf, school.tracts.sbst2)\n\n[1] TRUE\n\nwithin50 &lt;- st_is_within_distance(cejst.pnw, landmarks.proj, dist=50000, sparse = TRUE)\nwithin50.idx &lt;- lengths(within50) &gt; 0\nschool.tracts.sbst3 &lt;- cejst.pnw[within50.idx,]\nnrow(school.tracts.sbst3)\n\n[1] 2511\n\n\n3. Make a map for both the percentage of individuals with less than a high school degree in counties within 50km and beyond 50km (i.e. make 2 maps)\n\nWe now have the full cejst dataset and a dataset that is subsetted to the tracts within 50km of a school. Plotting the HSEF values for the tracts within 50km of a school is easy enough. Just map a layer that contains all of the tracts and set it’s color to gray. Then layer the subsetted features on top. Plotting the values of HSEF for the tracts beyond 50km is a little trickier (because we haven’t created that dataset yet). We can use the index we created in the previous step to do that here. We can also use the mutate function to create an indicator variable using our index and then create a “small multiples” style map that plots the two side by side. We’ll learn more about “prettying” up these maps in the later parts of the course.\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.tracts.sbst3) +\n  tm_fill(col=\"HSEF\")\n\n\n\n\n\n\n\nnoschool.tracts &lt;- cejst.pnw[!within50.idx,]\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(noschool.tracts) +\n  tm_fill(col=\"HSEF\")\n\n\n\n\n\n\n\nschool.combined &lt;- cejst.pnw %&gt;% \n  mutate(., indist = if_else(lengths(within50) &gt; 0, \"within50km\", \"notWithin50km\"))\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.combined) +\n  tm_fill(col=\"HSEF\") +\n  tm_facets(by = c(\"indist\"), nrow = 1)"
  },
  {
    "objectID": "example/session-4-example.html",
    "href": "example/session-4-example.html",
    "title": "Session 4 Live Code",
    "section": "",
    "text": "Read spreadsheet into R:\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\nfile.to.read &lt;- read_csv(file = \"/opt/data/data/assignment01/landmarks_ID.csv\", col_names = TRUE, col_type = NULL, na = c(\"\", NA))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, coords = c(\"longitude\", \"lattitude\"), crs=4326)\n\n\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 12169 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): ANSICODE, FULLNAME, MTFCC\ndbl (4): STATEFP, POINTID, longitude, lattitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRead in a shapefile:\n\n\nCode\nshapefile.inR &lt;- read_sf(dsn = \"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\nRead in a raster:\n\n\nCode\nlibrary(terra)\nraster.inR &lt;- rast(x = \"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nterra 1.7.78\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#reading-in-the-data",
    "href": "example/session-4-example.html#reading-in-the-data",
    "title": "Session 4 Live Code",
    "section": "",
    "text": "Read spreadsheet into R:\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\nfile.to.read &lt;- read_csv(file = \"/opt/data/data/assignment01/landmarks_ID.csv\", col_names = TRUE, col_type = NULL, na = c(\"\", NA))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, coords = c(\"longitude\", \"lattitude\"), crs=4326)\n\n\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 12169 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): ANSICODE, FULLNAME, MTFCC\ndbl (4): STATEFP, POINTID, longitude, lattitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRead in a shapefile:\n\n\nCode\nshapefile.inR &lt;- read_sf(dsn = \"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\nRead in a raster:\n\n\nCode\nlibrary(terra)\nraster.inR &lt;- rast(x = \"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nterra 1.7.78\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#introducing-yourself-to-vector-data",
    "href": "example/session-4-example.html#introducing-yourself-to-vector-data",
    "title": "Session 4 Live Code",
    "section": "Introducing yourself to vector data",
    "text": "Introducing yourself to vector data\nStructure of the data:\n\n\nCode\nstr(shapefile.inR)\n\n\nsf [2,590 × 124] (S3: sf/tbl_df/tbl/data.frame)\n $ GEOID10   : chr [1:2590] \"16019970700\" \"16025970100\" \"16027021700\" \"16027020700\" ...\n $ SF        : chr [1:2590] \"Idaho\" \"Idaho\" \"Idaho\" \"Idaho\" ...\n $ CF        : chr [1:2590] \"Bonneville County\" \"Camas County\" \"Canyon County\" \"Canyon County\" ...\n $ DF_PFS    : num [1:2590] 0.44 0.67 0.74 0.48 0.75 0.8 0.76 0.72 0.76 0.52 ...\n $ AF_PFS    : num [1:2590] 0.73 0.65 0.86 0.63 0.79 0.88 0.86 0.79 0.8 0.65 ...\n $ HDF_PFS   : num [1:2590] 0.57 0.79 0.69 0.47 0.72 0.55 0.53 0.71 0.74 0.45 ...\n $ DSF_PFS   : num [1:2590] 0.53 0 0.58 0.46 0.12 0.79 0.68 0.27 0.06 0.11 ...\n $ EBF_PFS   : num [1:2590] 0.63 0.95 0.54 0.3 0.63 0.81 0.81 0.44 0.73 0.52 ...\n $ EALR_PFS  : num [1:2590] 0.55 0.83 0.63 0.63 0.65 NA 0.41 0.64 0.65 0.64 ...\n $ EBLR_PFS  : num [1:2590] 0.19 0.93 0.05 0.55 0.09 0.09 0.08 0.09 0.47 0.17 ...\n $ EPLR_PFS  : num [1:2590] 0.8 0.99 0.09 0.1 0.11 0.31 0.08 0.09 0.1 0.09 ...\n $ HBF_PFS   : num [1:2590] 0.69 0.6 0.53 0.07 0.46 0.86 0.93 0.22 0.53 0.28 ...\n $ LLEF_PFS  : num [1:2590] 0.73 0.48 0.67 0.29 0.51 0.87 0.62 0.38 0.32 0.26 ...\n $ LIF_PFS   : num [1:2590] 0.54 0.54 0.53 0.12 0.48 0.61 0.88 0.46 0.41 0.32 ...\n $ LMI_PFS   : num [1:2590] 0.81 0.75 0.61 0.25 0.55 0.92 0.95 0.44 0.42 0.35 ...\n $ PM25F_PFS : num [1:2590] 0.11 0 0.68 0.63 0.42 0.66 0.69 0.57 0.26 0.4 ...\n $ HSEF      : num [1:2590] 0.14 0.06 0.2 0.08 0.21 0.24 0.31 0.18 0.18 0.09 ...\n $ P100_PFS  : num [1:2590] 0.83 0.63 0.47 0.24 0.6 0.72 0.93 0.27 0.39 0.33 ...\n $ P200_I_PFS: num [1:2590] 0.86 0.71 0.81 0.39 0.75 0.98 0.91 0.49 0.67 0.54 ...\n $ AJDLI_ET  : int [1:2590] 1 1 1 0 1 1 1 0 1 1 ...\n $ LPF_PFS   : num [1:2590] 0.75 0.52 0.26 0.3 0.65 0.62 0.63 0.4 0.51 0.36 ...\n $ KP_PFS    : num [1:2590] 0.86 0.21 0.84 0.21 0.61 0.57 0.21 0.43 0.9 0.21 ...\n $ NPL_PFS   : num [1:2590] 0.09 0.05 0.06 0.08 0.03 0.08 0.05 0.05 0.08 0.1 ...\n $ RMP_PFS   : num [1:2590] 0.49 0.01 0.56 0.84 0.26 0.96 0.58 0.36 0.1 0.2 ...\n $ TSDF_PFS  : num [1:2590] 0.22 0.01 0.14 0.42 0.07 0.55 0.13 0.11 0.1 0.19 ...\n $ TPF       : num [1:2590] 5589 1048 11701 3901 5059 ...\n $ TF_PFS    : num [1:2590] 0.55 0.06 0.54 0.53 0.28 0.71 0.83 0.24 0.05 0.09 ...\n $ UF_PFS    : num [1:2590] 0.61 0.16 0.64 0.33 0.66 0.86 0.8 0.45 0.47 0.61 ...\n $ WF_PFS    : num [1:2590] 0.03 0.11 0.98 0.24 0.79 0.98 0.98 0.85 0.42 0.04 ...\n $ UST_PFS   : num [1:2590] 0.59 0.02 0.44 0.31 0.17 0.73 0.83 0.13 0.06 0.18 ...\n $ N_WTR     : int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ N_WKFC    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ N_CLT     : int [1:2590] 0 1 0 0 0 1 1 0 0 0 ...\n $ N_ENY     : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ N_TRN     : int [1:2590] 0 0 0 0 0 0 0 0 1 0 ...\n $ N_HSG     : int [1:2590] 0 0 0 0 0 0 1 0 1 0 ...\n $ N_PLN     : int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ N_HLTH    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ SN_C      : int [1:2590] 0 1 1 0 0 1 1 0 1 0 ...\n $ SN_T      : chr [1:2590] NA NA NA NA ...\n $ DLI       : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ ALI       : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ PLHSE     : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ LMILHSE   : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ ULHSE     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ EPL_ET    : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ EAL_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ EBL_ET    : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ EB_ET     : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ PM25_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ DS_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TP_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LPP_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ HRS_ET    : chr [1:2590] NA NA NA NA ...\n $ KP_ET     : int [1:2590] 0 0 0 0 0 0 0 0 1 0 ...\n $ HB_ET     : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ RMP_ET    : int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ NPL_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TSDF_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ WD_ET     : int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ UST_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ DB_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ A_ET      : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ HD_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LLE_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ UN_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LISO_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ POV_ET    : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ LMI_ET    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ IA_LMI_ET : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IA_UN_ET  : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IA_POV_ET : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TC        : num [1:2590] 0 3 1 0 0 4 5 0 2 0 ...\n $ CC        : num [1:2590] 0 2 1 0 0 4 4 0 2 0 ...\n $ IAULHSE   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IAPLHSE   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IALMILHSE : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IALMIL_76 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ IAPLHS_77 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ IAULHS_78 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ LHE       : int [1:2590] 1 0 1 0 1 1 1 1 1 0 ...\n $ IALHE     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IAHSEF    : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ N_CLT_EOMI: int [1:2590] 0 1 0 0 0 1 1 0 0 0 ...\n $ N_ENY_EOMI: int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ N_TRN_EOMI: int [1:2590] 0 0 0 0 0 0 0 1 1 0 ...\n $ N_HSG_EOMI: int [1:2590] 0 0 0 0 0 0 1 0 1 0 ...\n $ N_PLN_EOMI: int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ N_WTR_EOMI: int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ N_HLTH_88 : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ N_WKFC_89 : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ FPL200S   : int [1:2590] 1 1 1 0 1 1 1 0 1 0 ...\n $ N_WKFC_91 : int [1:2590] 1 0 1 0 1 1 1 1 1 0 ...\n $ TD_ET     : int [1:2590] 0 0 0 0 0 0 0 1 1 0 ...\n $ TD_PFS    : num [1:2590] 0.67 0.78 0.6 0.63 0.85 0.35 0.29 0.94 0.91 0.78 ...\n $ FLD_PFS   : num [1:2590] 0.83 0.88 0.43 0.24 0.82 0.93 0.97 0.44 0.49 0.71 ...\n $ WFR_PFS   : num [1:2590] 0.7 0.82 0.33 0.87 0.8 0.33 0.83 0.77 0.81 0.78 ...\n $ FLD_ET    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ WFR_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n  [list output truncated]\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:123] \"GEOID10\" \"SF\" \"CF\" \"DF_PFS\" ...\n\n\nNumber of rows and columns:\n\n\nCode\nnrow(shapefile.inR)\n\n\n[1] 2590\n\n\nCode\nncol(shapefile.inR)\n\n\n[1] 124\n\n\nColumn names:\n\n\nCode\ncolnames(shapefile.inR)\n\n\n  [1] \"GEOID10\"    \"SF\"         \"CF\"         \"DF_PFS\"     \"AF_PFS\"    \n  [6] \"HDF_PFS\"    \"DSF_PFS\"    \"EBF_PFS\"    \"EALR_PFS\"   \"EBLR_PFS\"  \n [11] \"EPLR_PFS\"   \"HBF_PFS\"    \"LLEF_PFS\"   \"LIF_PFS\"    \"LMI_PFS\"   \n [16] \"PM25F_PFS\"  \"HSEF\"       \"P100_PFS\"   \"P200_I_PFS\" \"AJDLI_ET\"  \n [21] \"LPF_PFS\"    \"KP_PFS\"     \"NPL_PFS\"    \"RMP_PFS\"    \"TSDF_PFS\"  \n [26] \"TPF\"        \"TF_PFS\"     \"UF_PFS\"     \"WF_PFS\"     \"UST_PFS\"   \n [31] \"N_WTR\"      \"N_WKFC\"     \"N_CLT\"      \"N_ENY\"      \"N_TRN\"     \n [36] \"N_HSG\"      \"N_PLN\"      \"N_HLTH\"     \"SN_C\"       \"SN_T\"      \n [41] \"DLI\"        \"ALI\"        \"PLHSE\"      \"LMILHSE\"    \"ULHSE\"     \n [46] \"EPL_ET\"     \"EAL_ET\"     \"EBL_ET\"     \"EB_ET\"      \"PM25_ET\"   \n [51] \"DS_ET\"      \"TP_ET\"      \"LPP_ET\"     \"HRS_ET\"     \"KP_ET\"     \n [56] \"HB_ET\"      \"RMP_ET\"     \"NPL_ET\"     \"TSDF_ET\"    \"WD_ET\"     \n [61] \"UST_ET\"     \"DB_ET\"      \"A_ET\"       \"HD_ET\"      \"LLE_ET\"    \n [66] \"UN_ET\"      \"LISO_ET\"    \"POV_ET\"     \"LMI_ET\"     \"IA_LMI_ET\" \n [71] \"IA_UN_ET\"   \"IA_POV_ET\"  \"TC\"         \"CC\"         \"IAULHSE\"   \n [76] \"IAPLHSE\"    \"IALMILHSE\"  \"IALMIL_76\"  \"IAPLHS_77\"  \"IAULHS_78\" \n [81] \"LHE\"        \"IALHE\"      \"IAHSEF\"     \"N_CLT_EOMI\" \"N_ENY_EOMI\"\n [86] \"N_TRN_EOMI\" \"N_HSG_EOMI\" \"N_PLN_EOMI\" \"N_WTR_EOMI\" \"N_HLTH_88\" \n [91] \"N_WKFC_89\"  \"FPL200S\"    \"N_WKFC_91\"  \"TD_ET\"      \"TD_PFS\"    \n [96] \"FLD_PFS\"    \"WFR_PFS\"    \"FLD_ET\"     \"WFR_ET\"     \"ADJ_ET\"    \n[101] \"IS_PFS\"     \"IS_ET\"      \"AML_ET\"     \"FUDS_RAW\"   \"FUDS_ET\"   \n[106] \"IMP_FLG\"    \"DM_B\"       \"DM_AI\"      \"DM_A\"       \"DM_HI\"     \n[111] \"DM_T\"       \"DM_W\"       \"DM_H\"       \"DM_O\"       \"AGE_10\"    \n[116] \"AGE_MIDDLE\" \"AGE_OLD\"    \"TA_COU_116\" \"TA_COUNT_C\" \"TA_PERC\"   \n[121] \"TA_PERC_FE\" \"UI_EXP\"     \"THRHLD\"     \"geometry\"  \n\n\nFirst few rows of data:\n\n\nCode\nhead(file.as.sf, n = 6)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -116.7492 ymin: 43.18526 xmax: -111.3299 ymax: 43.80765\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n  STATEFP ANSICODE       POINTID FULLNAME        MTFCC             geometry\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;POINT [°]&gt;\n1      16 NA        110680906449 State Hospital… K1231 (-112.3351 43.18526)\n2      16 00390731 1102653699212 Snake River Ra… C3081 (-111.4147 43.45992)\n3      16 00395965 1102653699303 South Falls Ctr C3081  (-112.0222 43.4813)\n4      16 00399195 1102653700141 Swan Valley Ra… C3081 (-111.3299 43.44797)\n5      16 00382976 1102653683820 Hawley Gulch R… C3081 (-111.5744 43.65464)\n6      16 00398090 1102653708691 Sand Holw       C3081 (-116.7492 43.80765)\n\n\nSimple plotting:\n\n\nCode\nplot(st_geometry(shapefile.inR))\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(shapefile.inR[\"AGE_10\"])\n\n\n\n\n\n\n\n\n\nCRS of vector:\n\n\nCode\nst_crs(shapefile.inR)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#introducing-yourself-to-raster-data",
    "href": "example/session-4-example.html#introducing-yourself-to-raster-data",
    "title": "Session 4 Live Code",
    "section": "Introducing yourself to raster data",
    "text": "Introducing yourself to raster data\nDescribe raster before reading it in:\n\n\nCode\ndescribe(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\n [1] \"Driver: GTiff/GeoTIFF\"                                                                                 \n [2] \"Files: C:/Users/carolynkoehn/Documents/HES505_Fall_2024/data/2023/assignment01/wildfire_hazard_agg.tif\"\n [3] \"Size is 4607, 4016\"                                                                                    \n [4] \"Coordinate System is:\"                                                                                 \n [5] \"PROJCRS[\\\"unnamed\\\",\"                                                                                  \n [6] \"    BASEGEOGCRS[\\\"NAD83\\\",\"                                                                            \n [7] \"        DATUM[\\\"North American Datum 1983\\\",\"                                                          \n [8] \"            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\"                                          \n [9] \"                LENGTHUNIT[\\\"metre\\\",1]]],\"                                                            \n[10] \"        PRIMEM[\\\"Greenwich\\\",0,\"                                                                       \n[11] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\"                                                \n[12] \"        ID[\\\"EPSG\\\",4269]],\"                                                                           \n[13] \"    CONVERSION[\\\"Albers Equal Area\\\",\"                                                                 \n[14] \"        METHOD[\\\"Albers Equal Area\\\",\"                                                                 \n[15] \"            ID[\\\"EPSG\\\",9822]],\"                                                                       \n[16] \"        PARAMETER[\\\"Latitude of false origin\\\",23,\"                                                    \n[17] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[18] \"            ID[\\\"EPSG\\\",8821]],\"                                                                       \n[19] \"        PARAMETER[\\\"Longitude of false origin\\\",-96,\"                                                  \n[20] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[21] \"            ID[\\\"EPSG\\\",8822]],\"                                                                       \n[22] \"        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\"                                         \n[23] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[24] \"            ID[\\\"EPSG\\\",8823]],\"                                                                       \n[25] \"        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\"                                         \n[26] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[27] \"            ID[\\\"EPSG\\\",8824]],\"                                                                       \n[28] \"        PARAMETER[\\\"Easting at false origin\\\",0,\"                                                      \n[29] \"            LENGTHUNIT[\\\"metre\\\",1],\"                                                                  \n[30] \"            ID[\\\"EPSG\\\",8826]],\"                                                                       \n[31] \"        PARAMETER[\\\"Northing at false origin\\\",0,\"                                                     \n[32] \"            LENGTHUNIT[\\\"metre\\\",1],\"                                                                  \n[33] \"            ID[\\\"EPSG\\\",8827]]],\"                                                                      \n[34] \"    CS[Cartesian,2],\"                                                                                  \n[35] \"        AXIS[\\\"easting\\\",east,\"                                                                        \n[36] \"            ORDER[1],\"                                                                                 \n[37] \"            LENGTHUNIT[\\\"metre\\\",1,\"                                                                   \n[38] \"                ID[\\\"EPSG\\\",9001]]],\"                                                                  \n[39] \"        AXIS[\\\"northing\\\",north,\"                                                                      \n[40] \"            ORDER[2],\"                                                                                 \n[41] \"            LENGTHUNIT[\\\"metre\\\",1,\"                                                                   \n[42] \"                ID[\\\"EPSG\\\",9001]]]]\"                                                                  \n[43] \"Data axis to CRS axis mapping: 1,2\"                                                                    \n[44] \"Origin = (-2294745.000000000000000,3172575.000000000000000)\"                                           \n[45] \"Pixel Size = (240.000000000000000,-240.000000000000000)\"                                               \n[46] \"Metadata:\"                                                                                             \n[47] \"  AREA_OR_POINT=Area\"                                                                                  \n[48] \"Image Structure Metadata:\"                                                                             \n[49] \"  COMPRESSION=LZW\"                                                                                     \n[50] \"  INTERLEAVE=BAND\"                                                                                     \n[51] \"Corner Coordinates:\"                                                                                   \n[52] \"Upper Left  (-2294745.000, 3172575.000) (127d 6'55.97\\\"W, 48d 8'21.26\\\"N)\"                             \n[53] \"Lower Left  (-2294745.000, 2208735.000) (123d27'25.80\\\"W, 39d53'32.24\\\"N)\"                             \n[54] \"Upper Right (-1189065.000, 3172575.000) (112d33'19.93\\\"W, 50d38'58.88\\\"N)\"                             \n[55] \"Lower Right (-1189065.000, 2208735.000) (110d31'22.39\\\"W, 42d 3'38.51\\\"N)\"                             \n[56] \"Center      (-1741905.000, 2690655.000) (118d26'35.33\\\"W, 45d20'39.18\\\"N)\"                             \n[57] \"Band 1 Block=4607x1 Type=Float32, ColorInterp=Gray\"                                                    \n[58] \"  Description = WHP_ID\"                                                                                \n[59] \"  Min=0.000 Max=64185.656 \"                                                                            \n[60] \"  Minimum=0.000, Maximum=64185.656, Mean=-9999.000, StdDev=-9999.000\"                                  \n[61] \"  NoData Value=nan\"                                                                                    \n[62] \"  Metadata:\"                                                                                           \n[63] \"    STATISTICS_MAXIMUM=64185.65625\"                                                                    \n[64] \"    STATISTICS_MEAN=-9999\"                                                                             \n[65] \"    STATISTICS_MINIMUM=0\"                                                                              \n[66] \"    STATISTICS_STDDEV=-9999\"                                                                           \n\n\nBasic object info:\n\n\nCode\nraster.inR\n\n\nclass       : SpatRaster \ndimensions  : 4016, 4607, 1  (nrow, ncol, nlyr)\nresolution  : 240, 240  (x, y)\nextent      : -2294745, -1189065, 2208735, 3172575  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : wildfire_hazard_agg.tif \nname        :   WHP_ID \nmin value   :     0.00 \nmax value   : 64185.66 \n\n\nSummary of values:\n\n\nCode\n# Summary of all values\nsummary(values(raster.inR))\n\n\n     WHP_ID       \n Min.   :    0    \n 1st Qu.:   85    \n Median :  242    \n Mean   :  952    \n 3rd Qu.:  668    \n Max.   :64186    \n NA's   :7349366  \n\n\nCode\n# Summary of some values\nsummary(raster.inR)\n\n\nWarning: [summary] used a sample\n\n\n     WHP_ID        \n Min.   :    0.00  \n 1st Qu.:   83.91  \n Median :  242.20  \n Mean   :  949.59  \n 3rd Qu.:  666.39  \n Max.   :57327.66  \n NA's   :39866     \n\n\nBasic plot:\n\n\nCode\nplot(raster.inR)\n\n\n\n\n\n\n\n\n\nCode\n#Change color of NA\nplot(raster.inR, colNA = \"black\")\n\n\n\n\n\n\n\n\n\nCRS of raster:\n\n\nCode\ncrs(raster.inR)\n\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\nCode\n# You can also use st_crs!\nst_crs(raster.inR)\n\n\nCoordinate Reference System:\n  User input: unnamed \n  wkt:\nPROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "slides/08-slides.html#revisiting-the-raster-data-model",
    "href": "slides/08-slides.html#revisiting-the-raster-data-model",
    "title": "Areal Data: Rasters",
    "section": "Revisiting the Raster Data Model",
    "text": "Revisiting the Raster Data Model\n\n\n\n\nVector data describe the “exact” locations of features on a landscape (including a Cartesian landscape)\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\n\nOperations mimic those for matrix objects in R\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/08-slides.html#rasters-with-terra",
    "href": "slides/08-slides.html#rasters-with-terra",
    "title": "Areal Data: Rasters",
    "section": "Rasters with terra",
    "text": "Rasters with terra\n\nsyntax is different for terra compared to sf\nRepresentation in Environment is also different\nCan break pipes, Be Explicit"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-1",
    "href": "slides/08-slides.html#rasters-by-construction-1",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction",
    "text": "Rasters by Construction\n\n\n\nmtx &lt;- matrix(1:16, nrow=4)\nmtx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nrstr &lt;- terra::rast(mtx)\nrstr\n\nclass       : SpatRaster \ndimensions  : 4, 4, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 4, 0, 4  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     1 \nmax value   :    16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: you must have terra loaded for plot() to work on Rast* objects; otherwise you get Error in as.double(y) : cannot coerce type 'S4' to vector of type 'double'"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-origin",
    "href": "slides/08-slides.html#rasters-by-construction-origin",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Origin",
    "text": "Rasters by Construction: Origin\n\nOrigin defines the location of the intersection of the x and y axes\n\n\n\n\nIdeally, the origin is the cell “corner” closest to c(0, 0)\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, ncols=10)\nr[] &lt;- runif(ncell(r))\norigin(r)\n\n[1] 0.05 0.00\n\nr2 &lt;- r\norigin(r2) &lt;- c(2,2)"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-resolution",
    "href": "slides/08-slides.html#rasters-by-construction-resolution",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Resolution",
    "text": "Rasters by Construction: Resolution\n\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size\nResolution (res) defines the length and width of an individual pixel\n\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          ncols=10)\nres(r)\n\n[1] 1.35 1.00\n\nr2 &lt;- rast(xmin=-4, xmax = 5, \n           ncols=10)\nres(r2)\n\n[1] 0.9 1.0\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          res=c(0.5,0.5))\nncol(r)\n\n[1] 27\n\nr2 &lt;- rast(xmin=-4, xmax = 9.5, \n           res=c(5,5))\nncol(r2)\n\n[1] 3"
  },
  {
    "objectID": "slides/08-slides.html#extending-predicates",
    "href": "slides/08-slides.html#extending-predicates",
    "title": "Areal Data: Rasters",
    "section": "Extending predicates",
    "text": "Extending predicates\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nterra does not follow the same hierarchy as sf so a little trickier"
  },
  {
    "objectID": "slides/08-slides.html#unary-predicates-in-terra",
    "href": "slides/08-slides.html#unary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\nCan tell us qualities of a raster dataset\nMany similar operations for SpatVector class (note use of .)\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis.lonlat\nDoes the object have a longitude/latitude CRS?\n\n\ninMemory\nis the object stored in memory?\n\n\nis.factor\nAre there categorical layers?\n\n\nhasValues\nDo the cells have values?"
  },
  {
    "objectID": "slides/08-slides.html#unary-predicates-in-terra-1",
    "href": "slides/08-slides.html#unary-predicates-in-terra-1",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\n\n\n\nglobal: tests if the raster covers all longitudes (from -180 to 180 degrees) such that the extreme columns are in fact adjacent\n\n\nr &lt;- rast()\nis.lonlat(r)\n\n[1] TRUE\n\nis.lonlat(r, global=TRUE)\n\n[1] TRUE\n\n\n\n\n\n\nperhaps: If TRUE and the crs is unknown, the method returns TRUE if the coordinates are plausible for longitude/latitude\n\n\ncrs(r) &lt;- \"\"\nis.lonlat(r)\n\n[1] NA\n\nis.lonlat(r, perhaps=TRUE, warn=FALSE)\n\n[1] TRUE\n\n\n\ncrs(r) &lt;- \"+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84\"\nis.lonlat(r)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/08-slides.html#binary-predicates-in-terra",
    "href": "slides/08-slides.html#binary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary predicates in terra",
    "text": "Binary predicates in terra\n\nTake exactly 2 inputs, return 1 matrix of cell locs where value is TRUE\nadjacent: identifies cells adajcent to a set of raster cells"
  },
  {
    "objectID": "slides/08-slides.html#unary-measures-in-terra",
    "href": "slides/08-slides.html#unary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary measures in terra",
    "text": "Unary measures in terra\n\nSlightly more flexible than sf\nOne result for each layer in a stack\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ncellSize\narea of individual cells\n\n\nexpanse\nsummed area of all cells\n\n\nvalues\nreturns all cell values\n\n\nncol\nnumber of columns\n\n\nnrow\nnumber of rows\n\n\nncell\nnumber of cells\n\n\nres\nresolution\n\n\next\nminimum and maximum of x and y coords\n\n\norigin\nthe orgin of a SpatRaster\n\n\ncrs\nthe coordinate reference system\n\n\ncats\ncategories of a categorical raster"
  },
  {
    "objectID": "slides/08-slides.html#binary-measures-in-terra",
    "href": "slides/08-slides.html#binary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary measures in terra",
    "text": "Binary measures in terra\n\nReturns a matrix or SpatRaster describing the measure\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndistance\nshortest distance to non-NA or vector object\n\n\ngridDistance\nshortest distance through adjacent grid cells\n\n\ncostDist\nShortest distance considering cell-varying friction\n\n\ndirection\nazimuth to cells that are not NA"
  },
  {
    "objectID": "slides/08-slides.html#extra-practice",
    "href": "slides/08-slides.html#extra-practice",
    "title": "Areal Data: Rasters",
    "section": "Extra Practice",
    "text": "Extra Practice\n\nRun the examples for costDist and gridDistance to see how those functions can be used.\nLoad the wildfire_hazard_agg.tif data from the assignment03 folder. Use the data as the input for the distance function and plot the result. How might this be useful in your research?"
  },
  {
    "objectID": "example/session-13-example.html",
    "href": "example/session-13-example.html",
    "title": "Session 13 Code",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(terra)\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)\n\n\n\n\nCode\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")\n\n\n\n\nCode\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats\n\n\nWarning: [set.cats] setting categories like this is deprecated; use a\ntwo-column data.frame instead\n\n\nReassigning the levels like this shows a warning that this method is deprecated. This is the two column data.frame method it prefers:\n\n\nCode\ntempcats2 &lt;- data.frame(value = c(0, 1, 2),\n                        category = c(\"cold\", \"mild\", \"warm\"))\n\nlevels(temp_reclass) &lt;- tempcats2",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#code-from-slides",
    "href": "example/session-13-example.html#code-from-slides",
    "title": "Session 13 Code",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(terra)\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)\n\n\n\n\nCode\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")\n\n\n\n\nCode\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats\n\n\nWarning: [set.cats] setting categories like this is deprecated; use a\ntwo-column data.frame instead\n\n\nReassigning the levels like this shows a warning that this method is deprecated. This is the two column data.frame method it prefers:\n\n\nCode\ntempcats2 &lt;- data.frame(value = c(0, 1, 2),\n                        category = c(\"cold\", \"mild\", \"warm\"))\n\nlevels(temp_reclass) &lt;- tempcats2",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#hillshade",
    "href": "example/session-13-example.html#hillshade",
    "title": "Session 13 Code",
    "section": "Hillshade",
    "text": "Hillshade\n\n\nCode\nsrtm.slope &lt;- terrain(srtm, \"slope\", unit=\"radians\")\nsrtm.aspect &lt;- terrain(srtm, \"aspect\", unit=\"radians\")\n\nsrtm.shade &lt;- shade(srtm.slope, srtm.aspect)\n\n\n\n\nCode\nplot(srtm.shade, col=grey(0:100/100), legend=FALSE)\nplot(srtm, col=rainbow(25, alpha=0.35), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#practice",
    "href": "example/session-13-example.html#practice",
    "title": "Session 13 Code",
    "section": "Practice:",
    "text": "Practice:\n\nLoad wildfire risk data\n\n\nCode\nwildfire.risk &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\n\nplot(wildfire.risk)\n\n\n\n\nGet boundary for county\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nid_counties &lt;- tigris::counties(\"ID\", progress_bar = FALSE)\n\nada.cty &lt;- filter(id_counties, NAME == \"Ada\")\n\nplot(st_geometry(ada.cty))\n\n\n\n\n\n\n\n\n\n\n\nReclassify wildfire data\n\n\nCode\n# Method 1\nrcl &lt;- data.frame(from = c(0,10,30,50,80),\n                  to = c(10,30,50,80,100),\n                  becomes = c(0:4))\n\n# Method 2\nrcl.m &lt;- matrix(c(\n  0, 10, 0,\n  10, 30, 1,\n  30, 50, 2,\n  50, 80, 3,\n  80, 100, 4\n), ncol=3, byrow=TRUE)\n\n# Both methods work for the second argument\nwr_reclass &lt;- classify(wildfire.risk, rcl.m)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\n# Add names\nwr_cats &lt;- data.frame(value = 0:4,\n                      category = c(\"No worries\", \n                                   \"A little bit risky\",\n                                   \"Moderate risk\",\n                                   \"Very risky\",\n                                   \"Don't move here\"))\nlevels(wr_reclass) &lt;- wr_cats\n\nplot(wr_reclass)\n\n\n\n\n\n\n\n\n\n\n\nSmooth categorical raster\n\n\nCode\nwr_sm &lt;- focal(wr_reclass, w=7, fun=\"modal\")\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nplot(wr_sm)\n\n\n\n\n\n\n\n\n\n\n\nMake county-level map\n\n\nCode\n# match CRS\nada_proj &lt;- st_transform(ada.cty, crs(wr_sm))\n\nwr_sm_ada &lt;- crop(wr_sm, ada_proj, mask=TRUE)\n\nplot(wr_sm_ada)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "slides/09-slides.html#objectives",
    "href": "slides/09-slides.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "slides/09-slides.html#packages-with-plot-methods",
    "href": "slides/09-slides.html#packages-with-plot-methods",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Packages with plot methods",
    "text": "Packages with plot methods\n\n\n\nOften the fastest way to view data\nUse ?plot to see which packages export a method for the plot function\nOr you can use ?plot.*** to see which classes of objects have plot functions defined"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-sf-objects",
    "href": "slides/09-slides.html#plot-for-sf-objects",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\nCan plot outlines using plot(st_geometry(your.shapefile)) or plot(your.shapefile$geometry)\nPlotting attributes requires “extracting” the attributes (using plot(your.shapefile[\"ATTRIBUTE\"]))\nControlling aesthetics can be challenging\nlayering requires add=TRUE"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-sf-objects-1",
    "href": "slides/09-slides.html#plot-for-sf-objects-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\n\n\nplot(st_geometry(cejst))\n\n\n\n\n\n\n\n\n\n\nplot(cejst[\"EALR_PFS\"])"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-spatrasters",
    "href": "slides/09-slides.html#plot-for-spatrasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data)"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-spatrasters-1",
    "href": "slides/09-slides.html#plot-for-spatrasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))"
  },
  {
    "objectID": "slides/09-slides.html#combining-the-two-with-addtrue",
    "href": "slides/09-slides.html#combining-the-two-with-addtrue",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Combining the two with add=TRUE",
    "text": "Combining the two with add=TRUE\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))\nplot(st_geometry(st_transform(cejst, crs=crs(rast.data))), add=TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#grammar-of-graphics-wilkinson-2005",
    "href": "slides/09-slides.html#grammar-of-graphics-wilkinson-2005",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Grammar of Graphics (Wilkinson 2005)",
    "text": "Grammar of Graphics (Wilkinson 2005)\n\nGrammar: A set of structural rules that help establish the components of a language\nSystem and structure of language consist of syntax and semantics\nGrammar of Graphics: a framework that allows us to concisely describe the components of any graphic\nFollows a layered approach by using defined components to build a visualization\nggplot2 is a formal implementation in R"
  },
  {
    "objectID": "slides/09-slides.html#aesthetics-mapping-data-to-visual-elements",
    "href": "slides/09-slides.html#aesthetics-mapping-data-to-visual-elements",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Aesthetics: Mapping Data to Visual Elements",
    "text": "Aesthetics: Mapping Data to Visual Elements\n\n\n\n\nDefine the systematic conversion of data into elements of the visualization\nAre either categorical or continuous (exclusively)\nExamples include x, y, fill, color, and alpha\n\n\n\n\n\n\nFrom Wilke 2019"
  },
  {
    "objectID": "slides/09-slides.html#scales",
    "href": "slides/09-slides.html#scales",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Scales",
    "text": "Scales\n\nScales map data values to their aesthetics\nMust be a one-to-one relationship; each specific data value should map to only one aesthetic"
  },
  {
    "objectID": "slides/09-slides.html#using-tmap",
    "href": "slides/09-slides.html#using-tmap",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\",\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#using-tmap-1",
    "href": "slides/09-slides.html#using-tmap-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap"
  },
  {
    "objectID": "slides/09-slides.html#changing-aesthetics",
    "href": "slides/09-slides.html#changing-aesthetics",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics\n\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#changing-aesthetics-1",
    "href": "slides/09-slides.html#changing-aesthetics-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics"
  },
  {
    "objectID": "slides/09-slides.html#adding-layers",
    "href": "slides/09-slides.html#adding-layers",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers\nORDER MATTERS\n\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% filter(STUSPS %in% c(\"ID\", \"WA\", \"OR\")) %&gt;% st_transform(., crs = st_crs(cejst))\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(st) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#adding-layers-1",
    "href": "slides/09-slides.html#adding-layers-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers"
  },
  {
    "objectID": "slides/09-slides.html#integrating-rasters",
    "href": "slides/09-slides.html#integrating-rasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters\n\ncejst.proj &lt;- st_transform(cejst, crs=crs(rast.data)) %&gt;% filter(!st_is_empty(.))\nstates.proj &lt;- st_transform(st, crs=crs(rast.data))\npal8 &lt;- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\npt &lt;- tm_shape(rast.data[\"category\"]) +\n  tm_raster(palette = pal8) +\n  tm_shape(cejst.proj) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(states.proj) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#integrating-rasters-1",
    "href": "slides/09-slides.html#integrating-rasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters"
  },
  {
    "objectID": "slides/09-slides.html#todays-objectives",
    "href": "slides/09-slides.html#todays-objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Today’s Objectives",
    "text": "Today’s Objectives\nYou should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "example/getting-setup.html",
    "href": "example/getting-setup.html",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#lets-git-started",
    "href": "example/getting-setup.html#lets-git-started",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#quarto",
    "href": "example/getting-setup.html#quarto",
    "title": "Getting Setup",
    "section": "Quarto",
    "text": "Quarto\nThis is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "example/getting-setup.html#the-example",
    "href": "example/getting-setup.html#the-example",
    "title": "Getting Setup",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "slides/14-slides.html#objectives",
    "href": "slides/14-slides.html#objectives",
    "title": "Building Spatial Databases with Attributes",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nBegin building a database for spatial analysis"
  },
  {
    "objectID": "slides/14-slides.html#what-is-spatial-analysis-1",
    "href": "slides/14-slides.html#what-is-spatial-analysis-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n“The process of examining the locations, attributes, and relationships of features in spatial data through overlay and other analytical techniques in order to address a question or gain useful knowledge. Spatial analysis extracts or creates new information from spatial data”.\n\n— ESRI Dictionary"
  },
  {
    "objectID": "slides/14-slides.html#what-is-spatial-analysis-2",
    "href": "slides/14-slides.html#what-is-spatial-analysis-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n\n\nThe process of turning maps into information\nAny- or everything we do with GIS\nThe use of computational and statistical algorithms to understand the relations between things that co-occur in space.\n\n\n\n\n\nJohn Snow’s cholera outbreak map"
  },
  {
    "objectID": "slides/14-slides.html#common-goals-for-spatial-analysis",
    "href": "slides/14-slides.html#common-goals-for-spatial-analysis",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common goals for spatial analysis",
    "text": "Common goals for spatial analysis\n\n\n\n\n\ncourtesy of NatureServe\n\n\n\n\nDescribe and visualize locations or events\nQuantify patterns\nCharacterize ‘suitability’\nDetermine (statistical) relations"
  },
  {
    "objectID": "slides/14-slides.html#common-pitfalls-of-spatial-analysis",
    "href": "slides/14-slides.html#common-pitfalls-of-spatial-analysis",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common pitfalls of spatial analysis",
    "text": "Common pitfalls of spatial analysis\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals\n\n\n\nSpatial analysis is an inherently complex endeavor and one that is advancing rapidly. So-called “best practices” for addressing many of these issues are still being developed and debated. This doesn’t mean you shouldn’t do spatial analysis, but you should keep these things in mind as you design, implement, and interpret your analyses"
  },
  {
    "objectID": "slides/14-slides.html#workflows-for-spatial-analysis-1",
    "href": "slides/14-slides.html#workflows-for-spatial-analysis-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Workflows for spatial analysis",
    "text": "Workflows for spatial analysis\n\n\n\nAcquisition (not really a focus, but see Resources)\nGeoprocessing\nAnalysis\nVisualization\n\n\n\n\n\ncourtesy of University of Illinois"
  },
  {
    "objectID": "slides/14-slides.html#geoprocessing",
    "href": "slides/14-slides.html#geoprocessing",
    "title": "Building Spatial Databases with Attributes",
    "section": "Geoprocessing",
    "text": "Geoprocessing\nManipulation of data for subsequent use\n\nAlignment\nData cleaning and transformation\nCombination of multiple datasets\nSelection and subsetting"
  },
  {
    "objectID": "slides/14-slides.html#databases-and-attributes-1",
    "href": "slides/14-slides.html#databases-and-attributes-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Databases and attributes",
    "text": "Databases and attributes\n\n\n\n\n\ncourtesy of Giscommons\n\n\n\n\n\nPrevious focus has been largely on location\nGeographic data often also includes non-spatial data\nAttributes: Non-spatial information that further describes a spatial feature\nTypically stored in tables where each row represents a spatial feature\n\nWide vs. long format"
  },
  {
    "objectID": "slides/14-slides.html#common-attribute-operations",
    "href": "slides/14-slides.html#common-attribute-operations",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common attribute operations",
    "text": "Common attribute operations\n\nsf designed to work with tidyverse\nAllows use of dplyr data manipulation verbs (e.g. filter, select, slice)\nCan use scales package for units\nAlso allows %&gt;% to chain together multiple steps\ngeometries are “sticky”"
  },
  {
    "objectID": "slides/14-slides.html#subsetting-by-field",
    "href": "slides/14-slides.html#subsetting-by-field",
    "title": "Building Spatial Databases with Attributes",
    "section": "Subsetting by Field",
    "text": "Subsetting by Field\n\nFields contain individual attributes\nSelecting fields\n\n\n\n\ncolnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nhead(world)[,1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 6 × 3\n  iso_a2 name_long      continent    \n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;        \n1 FJ     Fiji           Oceania      \n2 TZ     Tanzania       Africa       \n3 EH     Western Sahara Africa       \n4 CA     Canada         North America\n5 US     United States  North America\n6 KZ     Kazakhstan     Asia         \n\n\n\n\nworld %&gt;%\n  dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.) \n\n# A tibble: 6 × 2\n  name_long      continent    \n  &lt;chr&gt;          &lt;chr&gt;        \n1 Fiji           Oceania      \n2 Tanzania       Africa       \n3 Western Sahara Africa       \n4 Canada         North America\n5 United States  North America\n6 Kazakhstan     Asia"
  },
  {
    "objectID": "slides/14-slides.html#subsetting-by-features",
    "href": "slides/14-slides.html#subsetting-by-features",
    "title": "Building Spatial Databases with Attributes",
    "section": "Subsetting by Features",
    "text": "Subsetting by Features\n\nFeatures refer to the individual observations in the dataset\nSelecting features\n\n\n\n\nhead(world)[1:3, 1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 3 × 3\n  iso_a2 name_long      continent\n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;    \n1 FJ     Fiji           Oceania  \n2 TZ     Tanzania       Africa   \n3 EH     Western Sahara Africa   \n\n\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 2\n  name_long   continent\n  &lt;chr&gt;       &lt;chr&gt;    \n1 Kazakhstan  Asia     \n2 Uzbekistan  Asia     \n3 Indonesia   Asia     \n4 Timor-Leste Asia     \n5 Israel      Asia     \n6 Lebanon     Asia"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse",
    "href": "slides/14-slides.html#revisiting-the-tidyverse",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent, pop, gdpPercap ,area_km2) %&gt;%\n  mutate(., dens = pop/area_km2,\n         totGDP = gdpPercap * pop) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 7\n  name_long   continent       pop gdpPercap area_km2   dens  totGDP\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Kazakhstan  Asia       17288285    23587. 2729811.   6.33 4.08e11\n2 Uzbekistan  Asia       30757700     5371.  461410.  66.7  1.65e11\n3 Indonesia   Asia      255131116    10003. 1819251. 140.   2.55e12\n4 Timor-Leste Asia        1212814     6263.   14715.  82.4  7.60e 9\n5 Israel      Asia        8215700    31702.   22991. 357.   2.60e11\n6 Lebanon     Asia        5603279    13831.   10099. 555.   7.75e10"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse-1",
    "href": "slides/14-slides.html#revisiting-the-tidyverse-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse-2",
    "href": "slides/14-slides.html#revisiting-the-tidyverse-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\n\n\nAggregating data\n\n\nworld %&gt;%\n  st_drop_geometry(.) %&gt;% \n  group_by(continent) %&gt;%\n  summarize(pop = sum(pop, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  continent                      pop\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811"
  },
  {
    "objectID": "slides/14-slides.html#joining-aspatial-data-1",
    "href": "slides/14-slides.html#joining-aspatial-data-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Joining (a)spatial data",
    "text": "Joining (a)spatial data\n\n\n\nRequires a “key” field\nMultiple outcomes possible\nThink about your final data form"
  },
  {
    "objectID": "slides/14-slides.html#left-join",
    "href": "slides/14-slides.html#left-join",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join\n\nUseful for adding other attributes not in your spatial data\nReturns all of the records in x attributed with y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/14-slides.html#left-join-1",
    "href": "slides/14-slides.html#left-join-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/14-slides.html#left-join-2",
    "href": "slides/14-slides.html#left-join-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join\n\n\n\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\n\n\nworld_coffee = left_join(world, coffee_data)\nnrow(world_coffee)\n\n[1] 177"
  },
  {
    "objectID": "slides/14-slides.html#left-join-3",
    "href": "slides/14-slides.html#left-join-3",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/14-slides.html#inner-join",
    "href": "slides/14-slides.html#inner-join",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join\n\nUseful for subsetting to “complete” records\nReturns all of the records in x with matching y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/14-slides.html#inner-join-1",
    "href": "slides/14-slides.html#inner-join-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/14-slides.html#inner-join-2",
    "href": "slides/14-slides.html#inner-join-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join\n\n\n\nworld_coffee_inner = inner_join(world, coffee_data)\nnrow(world_coffee_inner)\n\n[1] 45\n\n\n\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\""
  },
  {
    "objectID": "slides/14-slides.html#inner-join-3",
    "href": "slides/14-slides.html#inner-join-3",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/11-slides.html#objectives",
    "href": "slides/11-slides.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data."
  },
  {
    "objectID": "slides/11-slides.html#example-questions",
    "href": "slides/11-slides.html#example-questions",
    "title": "Operations With Vector Data II",
    "section": "Example questions",
    "text": "Example questions\n\nWhat is the chronic heart disease risk of the 10 ID tracts that are furthest from hospitals?\nHow may \\(km^2\\) of ID are served by more than 1 hospital?\nWhat is the difference between the average risk of chronic heart disease in the tracts served by at least two hospitals compared to those that aren’t served by any?"
  },
  {
    "objectID": "slides/11-slides.html#key-assummptions",
    "href": "slides/11-slides.html#key-assummptions",
    "title": "Operations With Vector Data II",
    "section": "Key assummptions",
    "text": "Key assummptions\n\nAll hospital locations are contained in the landmarks dataset\nA hospital service area is defined as a 50km radius\nHospital service areas can cross state lines."
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know",
    "href": "slides/11-slides.html#what-do-we-need-to-know",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nHow far are the hospitals from ID tracts?\nWhich tracts are the furthest?\nWhat is the CHD risk?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode",
    "href": "slides/11-slides.html#pseudocode",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital and cdc datasets\n2. Align the data\n3. Filter cdc so it only has Idaho tracts\n4. Calculate distance from hospitals\n5. Find top 10 tracts based on distance\n6. Map chronic heart disease risk"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions",
    "href": "slides/11-slides.html#adding-functions",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital and cdc datasets\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nhospital.sf &lt;- read_csv(\"../../data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\nst_crs(hospital.sf)\n\nCoordinate Reference System: NA\n\ncdc.sf &lt;- read_sf(\"../../data/2023/vectorexample/cdc_nw.shp\")\nst_crs(cdc.sf)$epsg\n\n[1] 4269"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-1",
    "href": "slides/11-slides.html#adding-functions-1",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nAlign the data\n\n\nst_crs(hospital.sf) &lt;- 4326\n\nhospital.sf.proj &lt;- hospital.sf %&gt;% \n  st_transform(., crs=st_crs(cdc.sf))\n\nst_crs(hospital.sf.proj) == st_crs(cdc.sf)\n\n[1] TRUE\n\nidentical(st_crs(hospital.sf.proj), st_crs(cdc.sf))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-2",
    "href": "slides/11-slides.html#adding-functions-2",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFilter cdc so it only has Idaho tracts\n\n\n\n\ncdc.idaho &lt;- cdc.sf %&gt;% \n  filter(STATEFP == \"16\")\n\n\n\nplot(st_geometry(cdc.idaho))"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-3",
    "href": "slides/11-slides.html#adding-functions-3",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate distance from hospitals\n\n\nnearest.hosp &lt;- st_nearest_feature(cdc.idaho, hospital.sf.proj)\nstr(nearest.hosp)\n\n int [1:191] 6 45 45 45 3 3 3 3 6 3 ...\n\nnearest.hosp.sf &lt;- hospital.sf.proj[nearest.hosp,]\nhospital.dist &lt;- st_distance(cdc.idaho, nearest.hosp.sf, by_element = TRUE)\nstr(hospital.dist)\n\n Units: [m] num [1:191] 29414 46610 39432 32817 23548 ..."
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-4",
    "href": "slides/11-slides.html#adding-functions-4",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind top 10 counties based on distance\n\n\ncdc.idaho.hosp &lt;- cdc.idaho %&gt;% \n  mutate(., disthosp = hospital.dist)\n\ncdc.furthest &lt;- cdc.idaho.hosp %&gt;% \n  slice_max(., n=10, order_by= disthosp)\n\nhead(cdc.furthest$disthosp)\n\nUnits: [m]\n[1] 94622.55 83296.77 80916.73 70646.03 70292.69 69877.25"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-5",
    "href": "slides/11-slides.html#adding-functions-5",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nMap chronic heart disease risk\n\n\nlibrary(tmap)\n\ntm_shape(tigris::counties(\"ID\", progress_bar=FALSE)) +\n  tm_polygons() +\n  tm_shape(cdc.furthest) +\n  tm_polygons(\"disthosp\", title=\"Dist to Hospital (m2)\") +\n  tm_shape(hospital.sf.proj[cdc.idaho,]) +\n  tm_symbols(size=0.25)"
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know-1",
    "href": "slides/11-slides.html#what-do-we-need-to-know-1",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nWhat is the service area for each hospital?\nWhere do those service areas overlap?\nHow big is the overlap area?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode-1",
    "href": "slides/11-slides.html#pseudocode-1",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital dataset and add projection\n2. Buffer hospitals by service area\n3. Find intersection of service areas\n4. Calculate area of overlap"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-6",
    "href": "slides/11-slides.html#adding-functions-6",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital dataset and add projection\n\n\nhospital.sf &lt;- read_csv(\"../../data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\n\nst_crs(hospital.sf) &lt;- 4326"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-7",
    "href": "slides/11-slides.html#adding-functions-7",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\n\n\nBuffer hospitals by service area\n\n\nhospital.buf &lt;- hospital.sf %&gt;%\n  filter(STATEFP == \"16\") %&gt;% \n  st_buffer(., dist = units::set_units(50, \"kilometers\"))\n\n\n\nplot(st_geometry(hospital.buf))"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-8",
    "href": "slides/11-slides.html#adding-functions-8",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind intersection of service areas\n\n\nhospital.int &lt;- hospital.buf %&gt;% \n  st_intersection()\nall(st_is_valid(hospital.int))"
  },
  {
    "objectID": "slides/11-slides.html#troubleshooting-process",
    "href": "slides/11-slides.html#troubleshooting-process",
    "title": "Operations With Vector Data II",
    "section": "Troubleshooting Process",
    "text": "Troubleshooting Process\nGoogling error code with package name and R lead to this issue page: https://github.com/r-spatial/sf/issues/2143\n\nhospital.buf &lt;- hospital.buf %&gt;%\n  # project to planar CRS to get rid of warning\n  st_transform(., crs = 5070) %&gt;%\n  # remove +/- duplicate buffer\n  filter(!row_number() %in% c(7,8))\n  \n\nhospital.int &lt;- hospital.buf %&gt;% \n  st_intersection(.)\nall(st_is_valid(hospital.int))\n\n[1] TRUE\n\nhospital.int.overlaps &lt;- hospital.int %&gt;%\n  filter(n.overlaps &gt; 1)"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-9",
    "href": "slides/11-slides.html#adding-functions-9",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate area of overlap\n\n\noverlap.areas &lt;- st_area(hospital.int.overlaps)\n\narea_m2 &lt;- sum(overlap.areas) + units::set_units(pi*50000^2, m^2)\n\nunits::set_units(area_m2, km^2)\n\n32664.78 [km^2]"
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know-2",
    "href": "slides/11-slides.html#what-do-we-need-to-know-2",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode-2",
    "href": "slides/11-slides.html#pseudocode-2",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-10",
    "href": "slides/11-slides.html#adding-functions-10",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions"
  },
  {
    "objectID": "slides/11-slides.html#plotting-the-results",
    "href": "slides/11-slides.html#plotting-the-results",
    "title": "Operations With Vector Data II",
    "section": "Plotting the Results",
    "text": "Plotting the Results"
  },
  {
    "objectID": "slides/12-slides.html#objectives",
    "href": "slides/12-slides.html#objectives",
    "title": "Operations on Raster Data I",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis"
  },
  {
    "objectID": "slides/12-slides.html#projecting-raster-data",
    "href": "slides/12-slides.html#projecting-raster-data",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\n\nTransformation from lat/long to planar CRS involves some loss of precision\nNew cell values estimated using overlap with original cells\nInterpolation for continuous data, nearest neighbor for categorical data\nEqual-area projections are preferred; especially for large areas\n\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nr &lt;- rast(xmin=-110, xmax=-90, ymin=40, ymax=60, ncols=40, nrows=40)\nvalues(r) &lt;- 1:ncell(r)\nplot(r)"
  },
  {
    "objectID": "slides/12-slides.html#projecting-raster-data-1",
    "href": "slides/12-slides.html#projecting-raster-data-1",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\nsimple method; alignment not guaranteed\n\n\nnewcrs &lt;- \"+proj=robin +datum=WGS84\"\npr1 &lt;- terra::project(r, newcrs)\nplot(pr1)\n\n\n\n\n\n\n\n\n\n\nproviding a template to ensure alignment\n\n\nx &lt;- rast(pr1)\n# Set the cell size\nres(x) &lt;- 200000\npr3 &lt;- terra::project(r, x)\nplot(pr3)"
  },
  {
    "objectID": "slides/12-slides.html#aligning-data-resample",
    "href": "slides/12-slides.html#aligning-data-resample",
    "title": "Operations on Raster Data I",
    "section": "Aligning Data: resample",
    "text": "Aligning Data: resample\n\nr &lt;- rast(nrow=3, ncol=3, xmin=0, xmax=10, ymin=0, ymax=10)\nvalues(r) &lt;- 1:ncell(r)\ns &lt;- rast(nrow=25, ncol=30, xmin=1, xmax=11, ymin=-1, ymax=11)\nx &lt;- resample(r, s, method=\"bilinear\")"
  },
  {
    "objectID": "slides/12-slides.html#downscaling-and-upscaling",
    "href": "slides/12-slides.html#downscaling-and-upscaling",
    "title": "Operations on Raster Data I",
    "section": "Downscaling and Upscaling",
    "text": "Downscaling and Upscaling\n\nAligning data for later analysis\nRemembering scale\nThinking about support"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions",
    "href": "slides/12-slides.html#changing-resolutions",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions",
    "text": "Changing resolutions\n\naggregate, disaggregate, resample allow changes in cell size\naggregate requires a function (e.g., mean() or min()) to determine what to do with the grouped values\nresample allows changes in cell size and shifting of cell centers (slower)"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions-aggregate",
    "href": "slides/12-slides.html#changing-resolutions-aggregate",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: aggregate",
    "text": "Changing resolutions: aggregate\n\n\n\nr &lt;- rast()\nr\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\nra &lt;- aggregate(r, 20)\nra\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\nplot(ra)"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions-disagg",
    "href": "slides/12-slides.html#changing-resolutions-disagg",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: disagg",
    "text": "Changing resolutions: disagg\n\n\n\nra &lt;- aggregate(r, 20)\nplot(ra)\n\n\n\n\n\n\n\n\n\n\nrd &lt;- disagg(r, 20)\n\n\nrd\n\nclass       : SpatRaster \ndimensions  : 3600, 7200, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : spat_193c19277655_6460.tif \nname        : lyr.1 \nmin value   :     1 \nmax value   : 64800 \n\nplot(rd)"
  },
  {
    "objectID": "slides/12-slides.html#dealing-with-different-extents",
    "href": "slides/12-slides.html#dealing-with-different-extents",
    "title": "Operations on Raster Data I",
    "section": "Dealing with Different Extents",
    "text": "Dealing with Different Extents\n\n\n\nRaster extents often larger than our analysis\nReducing memory and computational resources\nMaking attractive maps"
  },
  {
    "objectID": "slides/12-slides.html#using-terracrop",
    "href": "slides/12-slides.html#using-terracrop",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinate Reference System must be the same for both objects\nCrop is based on the (converted) SpatExtent of the 2nd object\nsnap describes how y will be aligned to the raster\nReturns all data within the extent"
  },
  {
    "objectID": "slides/12-slides.html#using-terracrop-1",
    "href": "slides/12-slides.html#using-terracrop-1",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n[1] TRUE\n\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")"
  },
  {
    "objectID": "slides/12-slides.html#using-mask",
    "href": "slides/12-slides.html#using-mask",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nOften want to get rid of all values outside of vector\nCan set mask=TRUE in crop() (y must be SpatVector)\nOr use mask()\n\n\n\n\n\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion))\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/12-slides.html#using-mask-1",
    "href": "slides/12-slides.html#using-mask-1",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nAllows more control over what the mask does\nCan set maskvalues and updatevalues to change the resulting raster\nCan also use inverse to mask out the vector\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=0)\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/12-slides.html#extending-boundaries",
    "href": "slides/12-slides.html#extending-boundaries",
    "title": "Operations on Raster Data I",
    "section": "Extending boundaries",
    "text": "Extending boundaries\n\n\nVector slightly larger than raster\nEspecially when using buffered datasets\nCan use extend\nNot exact; depends on snap()\n\n\n\n\n\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\next(vect(zion.buff))\n\nSpatExtent : -113.343652923976, -112.745006809213, 37.0477357596604, 37.5977812137969 (xmin, xmax, ymin, ymax)"
  },
  {
    "objectID": "slides/12-slides.html#practice",
    "href": "slides/12-slides.html#practice",
    "title": "Operations on Raster Data I",
    "section": "Practice",
    "text": "Practice\n\nmosaic is a funciton that combines adjacent rasters, but they need to have the same origin and resolution. Let’s practice preparing some rasters for a mosaic.\n\nLoad wildfire hazard data from the rasterexample folder for OR and ID: Copy of CRPS_OR.tif and Copy of CRPS_ID.tif.\nThese rasters have a fine resolution that will make our calculations slow. Transform them to have a resolution of 900 m.\nDo the rasters have the same CRS, origin, resolution, and extent? Check this with ==.\nUse new functions from this lecture to align the properties mentioned in #3 (plot often to check your work). Why not use project?\nmosaic the two rasters together."
  }
]