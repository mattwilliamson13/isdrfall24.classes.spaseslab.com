[
  {
    "objectID": "slides/24-slides.html#assignment-9-scaling-the-hazard-data",
    "href": "slides/24-slides.html#assignment-9-scaling-the-hazard-data",
    "title": "Data Visualization and Maps I",
    "section": "Assignment 9: Scaling the hazard data",
    "text": "Assignment 9: Scaling the hazard data\n\nhazard.smooth.scl &lt;- (hazard.smooth - mean(incident.cejst.prep$hazard))/sd(incident.cejst.prep$hazard)\n#versus\nhazard.smooth.scl.nogood &lt;- scale(hazard.smooth)"
  },
  {
    "objectID": "slides/24-slides.html#assignment-9-scaling-the-hazard-data-1",
    "href": "slides/24-slides.html#assignment-9-scaling-the-hazard-data-1",
    "title": "Data Visualization and Maps I",
    "section": "Assignment 9: Scaling the hazard data",
    "text": "Assignment 9: Scaling the hazard data"
  },
  {
    "objectID": "slides/24-slides.html#assignment-9-different-predictions-for-different-scaling",
    "href": "slides/24-slides.html#assignment-9-different-predictions-for-different-scaling",
    "title": "Data Visualization and Maps I",
    "section": "Assignment 9: Different predictions for different scaling",
    "text": "Assignment 9: Different predictions for different scaling"
  },
  {
    "objectID": "slides/24-slides.html#principles-vs.-rules",
    "href": "slides/24-slides.html#principles-vs.-rules",
    "title": "Data Visualization and Maps I",
    "section": "Principles vs. Rules",
    "text": "Principles vs. Rules\n\n\n\nLots of examples of good and bad data visualization\nWhat makes a graphic good (or bad)?\nWho decides?\n\n\n\n\nRule: externally compels you, through force, threat or punishment, to do the things someone else has deemed good or right.\nPrinciple: internally motivating because it is a good practice; a general statement describing a philosophy that good rules should satisfy\nRules contribute to the design process, but do not guarantee a satisfactory outcome"
  },
  {
    "objectID": "slides/24-slides.html#ugly-wrong-and-bad",
    "href": "slides/24-slides.html#ugly-wrong-and-bad",
    "title": "Data Visualization and Maps I",
    "section": "Ugly, Wrong, and Bad",
    "text": "Ugly, Wrong, and Bad\n\n\n\nUgly: graphic is clear and informative, but has aesthetic issues\nBad: graphic is unclear, confusing, or decieving\nWrong: the figure is objectively incorrect\n\n\n\n\n\nMonstrous Costs’ by Nigel Holmes from Healy 2018"
  },
  {
    "objectID": "slides/24-slides.html#bad-and-wrong",
    "href": "slides/24-slides.html#bad-and-wrong",
    "title": "Data Visualization and Maps I",
    "section": "Bad and Wrong",
    "text": "Bad and Wrong\n\nPresentation of the data is (intentionally?) decieving\nPresentation is just incorrect\n\n\n\n\n\n\nTricky (from Healy 2018)\n\n\n\n\n\n\nWrong\n\n\n\n\nTricky because: countries were asked slightly different questions, data is scaled and raw data shows different trend"
  },
  {
    "objectID": "slides/24-slides.html#grammar-of-graphics-wilkinson-2005",
    "href": "slides/24-slides.html#grammar-of-graphics-wilkinson-2005",
    "title": "Data Visualization and Maps I",
    "section": "Grammar of Graphics (Wilkinson 2005)",
    "text": "Grammar of Graphics (Wilkinson 2005)\n\nGrammar: A set of structural rules that help establish the components of a language\nSystem and structure of language consist of syntax and semantics\nGrammar of Graphics: a framework that allows us to concisely describe the components of any graphic\nFollows a layered approach by using defined components to build a visualization\nggplot2 is a formal implementation in R"
  },
  {
    "objectID": "slides/24-slides.html#aesthetics-mapping-data-to-visual-elements",
    "href": "slides/24-slides.html#aesthetics-mapping-data-to-visual-elements",
    "title": "Data Visualization and Maps I",
    "section": "Aesthetics: Mapping Data to Visual Elements",
    "text": "Aesthetics: Mapping Data to Visual Elements\n\n\n\n\nDefine the systematic conversion of data into elements of the visualization\nAre either categorical or continuous (exclusively)\nExamples include x, y, fill, color, and alpha\n\n\n\n\n\n\nFrom Wilke 2019"
  },
  {
    "objectID": "slides/24-slides.html#scales",
    "href": "slides/24-slides.html#scales",
    "title": "Data Visualization and Maps I",
    "section": "Scales",
    "text": "Scales\n\nScales map data values to their aesthetics\nMust be a one-to-one relationship; each specific data value should map to only one aesthetic"
  },
  {
    "objectID": "slides/24-slides.html#principles-of-data-visualization",
    "href": "slides/24-slides.html#principles-of-data-visualization",
    "title": "Data Visualization and Maps I",
    "section": "Principles of Data Visualization",
    "text": "Principles of Data Visualization\n\nBe Honest\nPrinciple of proportional ink\nAvoid unnecessary ‘chart junk’\nUse color judiciously\nBalance data and context"
  },
  {
    "objectID": "slides/24-slides.html#telling-stories-with-maps",
    "href": "slides/24-slides.html#telling-stories-with-maps",
    "title": "Data Visualization and Maps I",
    "section": "Telling stories with maps",
    "text": "Telling stories with maps\n\n\n\n\n\nMaps organize a lot of information in a coherent way\nThey invite critique and inspection\nThey are also aesthetic objects that can engage broader audiences"
  },
  {
    "objectID": "slides/24-slides.html#key-issues",
    "href": "slides/24-slides.html#key-issues",
    "title": "Data Visualization and Maps I",
    "section": "Key Issues",
    "text": "Key Issues\n\nThinking about projections\nScale of the map\nErrors of Omission"
  },
  {
    "objectID": "slides/24-slides.html#cartographic-principles",
    "href": "slides/24-slides.html#cartographic-principles",
    "title": "Data Visualization and Maps I",
    "section": "Cartographic Principles",
    "text": "Cartographic Principles\n\nConcept before compilation\nHierarchy with harmony (Important things should look important)\nSimplicity from sacrifice\nMaximum information at minimum cost\nEngage emotion to enhance understanding"
  },
  {
    "objectID": "slides/24-slides.html#scale",
    "href": "slides/24-slides.html#scale",
    "title": "Data Visualization and Maps I",
    "section": "Scale",
    "text": "Scale\n\n\n\nRelates map distance to distance on the ground\nRatio scales (1:24,000 or 1/24,000)\nGraphic scales\nLarge vs. small-scale?"
  },
  {
    "objectID": "slides/24-slides.html#projection",
    "href": "slides/24-slides.html#projection",
    "title": "Data Visualization and Maps I",
    "section": "Projection",
    "text": "Projection\n\n\n\n\n\nDevelopable Surfaces\n\n\n\n\nDistortion makes scale invalid across large areas\nDistortion increases with distance from standard line\nFive distortions: areas, angles, shapes, distances, and direction"
  },
  {
    "objectID": "slides/24-slides.html#map-symbols",
    "href": "slides/24-slides.html#map-symbols",
    "title": "Data Visualization and Maps I",
    "section": "Map Symbols",
    "text": "Map Symbols\n\n\n\n\n\nGraphic code for retrieving information\n(De-)emphasize (un)important information\nContrast and the role of colors"
  },
  {
    "objectID": "slides/24-slides.html#section-2",
    "href": "slides/24-slides.html#section-2",
    "title": "Data Visualization and Maps I",
    "section": "",
    "text": "A good map tells a multitude of little white lies: it supresses truth to help the user see what needs to be seen…\n\n— Mark Monmonier"
  },
  {
    "objectID": "slides/24-slides.html#geometry",
    "href": "slides/24-slides.html#geometry",
    "title": "Data Visualization and Maps I",
    "section": "Geometry",
    "text": "Geometry\n\nZhilin et al. 2008"
  },
  {
    "objectID": "slides/24-slides.html#context",
    "href": "slides/24-slides.html#context",
    "title": "Data Visualization and Maps I",
    "section": "Context",
    "text": "Context\n\n\n\nFilter out irrelevant details\nTwo elements: selection and classification\nReflect interpretations of the relative importance of different features\n\n\n\n\n\nMackaness and Chaudry"
  },
  {
    "objectID": "slides/24-slides.html#point-maps",
    "href": "slides/24-slides.html#point-maps",
    "title": "Data Visualization and Maps I",
    "section": "Point Maps",
    "text": "Point Maps\n\n\n\n\n\nDot Maps: quantity represented by amount and concentration of dots\nProportional Symbol Map: Geometric symbols scaled in proportion to a quantity"
  },
  {
    "objectID": "slides/24-slides.html#ebbinghaus-illusion",
    "href": "slides/24-slides.html#ebbinghaus-illusion",
    "title": "Data Visualization and Maps I",
    "section": "Ebbinghaus’ illusion",
    "text": "Ebbinghaus’ illusion"
  },
  {
    "objectID": "slides/24-slides.html#line-maps",
    "href": "slides/24-slides.html#line-maps",
    "title": "Data Visualization and Maps I",
    "section": "Line Maps",
    "text": "Line Maps\n\nFrom High Country News"
  },
  {
    "objectID": "slides/24-slides.html#choropleth",
    "href": "slides/24-slides.html#choropleth",
    "title": "Data Visualization and Maps I",
    "section": "Choropleth",
    "text": "Choropleth\n\nMapping color to geographies\nCommon problems\n\n\nFrom Healy 2019"
  },
  {
    "objectID": "slides/24-slides.html#cartogram",
    "href": "slides/24-slides.html#cartogram",
    "title": "Data Visualization and Maps I",
    "section": "Cartogram",
    "text": "Cartogram\n\nAdjusts for differences in area, population, etc\nCommon Problems\n\n\nFrom Healy 2019"
  },
  {
    "objectID": "slides/24-slides.html#section-4",
    "href": "slides/24-slides.html#section-4",
    "title": "Data Visualization and Maps I",
    "section": "",
    "text": "{ggplot2} package description\n\n\n\n{ggplot2} is a system for declaratively creating graphics,based on “The Grammar of Graphics” (Wilkinson, 2005).\n\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details."
  },
  {
    "objectID": "slides/24-slides.html#advantages-of-ggplot2",
    "href": "slides/24-slides.html#advantages-of-ggplot2",
    "title": "Data Visualization and Maps I",
    "section": "Advantages of {ggplot2}",
    "text": "Advantages of {ggplot2}\n\n\nconsistent underlying “grammar of graphics” (Wilkinson 2005)\n\nvery flexible, layered plot specification\n\ntheme system for polishing plot appearance\n\nlots of additional functionality thanks to extensions\n\nactive and helpful community"
  },
  {
    "objectID": "slides/24-slides.html#the-grammar-of-ggplot2",
    "href": "slides/24-slides.html#the-grammar-of-ggplot2",
    "title": "Data Visualization and Maps I",
    "section": "The Grammar of {ggplot2}",
    "text": "The Grammar of {ggplot2}\n\n\n\n\nComponent\n\n\nFunction\n\n\nExplanation\n\n\n\n\nData\n\n\nggplot(data)         \n\n\nThe raw data that you want to visualise.\n\n\n\n\nAesthetics          \n\n\naes()\n\n\nAesthetic mappings between variables and visual properties.\n\n\n\nGeometries\n\n\ngeom_*()\n\n\nThe geometric shapes representing the data."
  },
  {
    "objectID": "slides/24-slides.html#the-grammar-of-ggplot2-1",
    "href": "slides/24-slides.html#the-grammar-of-ggplot2-1",
    "title": "Data Visualization and Maps I",
    "section": "The Grammar of {ggplot2}",
    "text": "The Grammar of {ggplot2}\n\n\n\n\nComponent\n\n\nFunction\n\n\nExplanation\n\n\n\n\nData\n\n\nggplot(data)         \n\n\nThe raw data that you want to visualise.\n\n\n\n\nAesthetics          \n\n\naes()\n\n\nAesthetic mappings between variables and visual properties.\n\n\n\nGeometries\n\n\ngeom_*()\n\n\nThe geometric shapes representing the data.\n\n\n\n\nStatistics\n\n\nstat_*()\n\n\nThe statistical transformations applied to the data.\n\n\n\n\nScales\n\n\nscale_*()\n\n\nMaps between the data and the aesthetic dimensions.\n\n\n\n\nCoordinate System\n\n\ncoord_*()\n\n\nMaps data into the plane of the data rectangle.\n\n\n\n\nFacets\n\n\nfacet_*()\n\n\nThe arrangement of the data into a grid of plots.\n\n\n\n\nVisual Themes\n\n\ntheme() and theme_*()\n\n\nThe overall visual defaults of a plot."
  },
  {
    "objectID": "slides/24-slides.html#the-data",
    "href": "slides/24-slides.html#the-data",
    "title": "Data Visualization and Maps I",
    "section": "The Data",
    "text": "The Data\nBike sharing counts in London, UK, powered by TfL Open Data\n\n\ncovers the years 2015 and 2016\nincl. weather data acquired from freemeteo.com\nprepared by Hristo Mavrodiev for Kaggle\nfurther modification by myself"
  },
  {
    "objectID": "slides/24-slides.html#ggplot2ggplot",
    "href": "slides/24-slides.html#ggplot2ggplot",
    "title": "Data Visualization and Maps I",
    "section": "ggplot2::ggplot()",
    "text": "ggplot2::ggplot()"
  },
  {
    "objectID": "slides/24-slides.html#data",
    "href": "slides/24-slides.html#data",
    "title": "Data Visualization and Maps I",
    "section": "Data",
    "text": "Data\n\n\nggplot(data = bikes)"
  },
  {
    "objectID": "slides/24-slides.html#aesthetic-mapping",
    "href": "slides/24-slides.html#aesthetic-mapping",
    "title": "Data Visualization and Maps I",
    "section": "Aesthetic Mapping",
    "text": "Aesthetic Mapping\n= link variables to graphical properties\n\n\npositions (x, y)\ncolors (color, fill)\nshapes (shape, linetype)\nsize (size)\ntransparency (alpha)\ngroupings (group)"
  },
  {
    "objectID": "slides/24-slides.html#aesthetic-mapping-1",
    "href": "slides/24-slides.html#aesthetic-mapping-1",
    "title": "Data Visualization and Maps I",
    "section": "Aesthetic Mapping",
    "text": "Aesthetic Mapping\n\n\nggplot(data = bikes) +\n  aes(x = temp_feel, y = count)"
  },
  {
    "objectID": "slides/24-slides.html#aesthetics",
    "href": "slides/24-slides.html#aesthetics",
    "title": "Data Visualization and Maps I",
    "section": "aesthetics",
    "text": "aesthetics\naes() outside as component\n\nggplot(data = bikes) +\n  aes(x = temp_feel, y = count)\n\n\naes() inside, explicit matching\n\nggplot(data = bikes, mapping = aes(x = temp_feel, y = count))\n\n\n\naes() inside, implicit matching\n\nggplot(bikes, aes(temp_feel, count))\n\n\n\naes() inside, mixed matching\n\nggplot(bikes, aes(x = temp_feel, y = count))"
  },
  {
    "objectID": "slides/24-slides.html#geometries",
    "href": "slides/24-slides.html#geometries",
    "title": "Data Visualization and Maps I",
    "section": "Geometries",
    "text": "Geometries\n\n= interpret aesthetics as graphical representations\n\n\npoints\nlines\npolygons\ntext labels\n…"
  },
  {
    "objectID": "slides/24-slides.html#geometries-1",
    "href": "slides/24-slides.html#geometries-1",
    "title": "Data Visualization and Maps I",
    "section": "Geometries",
    "text": "Geometries\n\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point()"
  },
  {
    "objectID": "slides/24-slides.html#visual-properties-of-layers",
    "href": "slides/24-slides.html#visual-properties-of-layers",
    "title": "Data Visualization and Maps I",
    "section": "Visual Properties of Layers",
    "text": "Visual Properties of Layers\n\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point(\n    color = \"#28a87d\",\n    alpha = .5,\n    shape = \"X\",\n    stroke = 1,\n    size = 4\n  )"
  },
  {
    "objectID": "slides/24-slides.html#setting-vs-mapping-of-visual-properties",
    "href": "slides/24-slides.html#setting-vs-mapping-of-visual-properties",
    "title": "Data Visualization and Maps I",
    "section": "Setting vs Mapping of Visual Properties",
    "text": "Setting vs Mapping of Visual Properties\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point(\n    color = \"#28a87d\",\n    alpha = .5\n  )\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point(\n    aes(color = season),\n    alpha = .5\n  )"
  },
  {
    "objectID": "slides/24-slides.html#mapping-expressions",
    "href": "slides/24-slides.html#mapping-expressions",
    "title": "Data Visualization and Maps I",
    "section": "Mapping Expressions",
    "text": "Mapping Expressions\n\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point(\n    aes(color = temp_feel &gt; 20),\n    alpha = .5\n  )"
  },
  {
    "objectID": "slides/24-slides.html#mapping-expressions-1",
    "href": "slides/24-slides.html#mapping-expressions-1",
    "title": "Data Visualization and Maps I",
    "section": "Mapping Expressions",
    "text": "Mapping Expressions\n\n\nggplot(\n    bikes,\n    aes(x = temp, y = temp_feel)\n  ) +\n  geom_point(\n    aes(color = weather_type == \"clear\"),\n    alpha = .5,\n    size = 2\n  )"
  },
  {
    "objectID": "slides/24-slides.html#mapping-to-size",
    "href": "slides/24-slides.html#mapping-to-size",
    "title": "Data Visualization and Maps I",
    "section": "Mapping to Size",
    "text": "Mapping to Size\n\n\nggplot(\n    bikes,\n    aes(x = temp, y = temp_feel)\n  ) +\n  geom_point(\n    aes(color = weather_type == \"clear\",\n        size = count),\n    alpha = .5\n  )"
  },
  {
    "objectID": "slides/24-slides.html#setting-a-constant-property",
    "href": "slides/24-slides.html#setting-a-constant-property",
    "title": "Data Visualization and Maps I",
    "section": "Setting a Constant Property",
    "text": "Setting a Constant Property\n\n\nggplot(\n    bikes,\n    aes(x = temp, y = temp_feel)\n  ) +\n  geom_point(\n    aes(color = weather_type == \"clear\",\n        size = count),\n    shape = 18,\n    alpha = .5\n  )"
  },
  {
    "objectID": "slides/24-slides.html#adding-more-layers",
    "href": "slides/24-slides.html#adding-more-layers",
    "title": "Data Visualization and Maps I",
    "section": "Adding More Layers",
    "text": "Adding More Layers\n\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season)\n  ) +\n  geom_point(\n    alpha = .5\n  ) +\n  geom_smooth(\n    method = \"lm\"\n  )"
  },
  {
    "objectID": "slides/24-slides.html#stat_-and-geom_",
    "href": "slides/24-slides.html#stat_-and-geom_",
    "title": "Data Visualization and Maps I",
    "section": "`stat_*()` and `geom_*()`",
    "text": "`stat_*()` and `geom_*()`\n\nggplot(bikes, aes(x = temp_feel, y = count)) +\n  stat_smooth(geom = \"smooth\")"
  },
  {
    "objectID": "slides/24-slides.html#stat_-and-geom_-1",
    "href": "slides/24-slides.html#stat_-and-geom_-1",
    "title": "Data Visualization and Maps I",
    "section": "`stat_*()` and `geom_*()`",
    "text": "`stat_*()` and `geom_*()`\n\nggplot(bikes, aes(x = temp_feel, y = count)) +\n  geom_smooth(stat = \"smooth\")"
  },
  {
    "objectID": "slides/24-slides.html#stat_-and-geom_-2",
    "href": "slides/24-slides.html#stat_-and-geom_-2",
    "title": "Data Visualization and Maps I",
    "section": "`stat_*()` and `geom_*()`",
    "text": "`stat_*()` and `geom_*()`\nggplot(bikes, aes(x = season)) +\n  stat_count(geom = \"bar\")\nggplot(bikes, aes(x = season)) +\n  geom_bar(stat = \"count\")"
  },
  {
    "objectID": "slides/24-slides.html#stat_-and-geom_-3",
    "href": "slides/24-slides.html#stat_-and-geom_-3",
    "title": "Data Visualization and Maps I",
    "section": "`stat_*()` and `geom_*()`",
    "text": "`stat_*()` and `geom_*()`\nggplot(bikes, aes(x = date, y = temp_feel)) +\n  stat_identity(geom = \"point\")\nggplot(bikes, aes(x = date, y = temp_feel)) +\n  geom_point(stat = \"identity\")"
  },
  {
    "objectID": "slides/24-slides.html#facets-1",
    "href": "slides/24-slides.html#facets-1",
    "title": "Data Visualization and Maps I",
    "section": "Facets",
    "text": "Facets\n\n= split variables to multiple panels\n\nFacets are also known as:\n\nsmall multiples\ntrellis graphs\nlattice plots\nconditioning"
  },
  {
    "objectID": "slides/24-slides.html#wrapped-facets",
    "href": "slides/24-slides.html#wrapped-facets",
    "title": "Data Visualization and Maps I",
    "section": "Wrapped Facets",
    "text": "Wrapped Facets\n\n\ng &lt;-\n  ggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season)\n  ) +\n  geom_point(\n    alpha = .3,\n    guide = \"none\"\n  )\ng +\n  facet_wrap(\n    vars(day_night)\n  )"
  },
  {
    "objectID": "slides/24-slides.html#wrapped-facets-1",
    "href": "slides/24-slides.html#wrapped-facets-1",
    "title": "Data Visualization and Maps I",
    "section": "Wrapped Facets",
    "text": "Wrapped Facets\n\n\ng +\n  facet_wrap(\n    ~ day_night\n  )"
  },
  {
    "objectID": "slides/24-slides.html#scales-2",
    "href": "slides/24-slides.html#scales-2",
    "title": "Data Visualization and Maps I",
    "section": "Scales",
    "text": "Scales\n\n= translate between variable ranges and property ranges\n\n\nfeels-like temperature  ⇄  x\nreported bike shares  ⇄  y\nseason  ⇄  color\nyear  ⇄  shape\n…"
  },
  {
    "objectID": "slides/24-slides.html#scales-3",
    "href": "slides/24-slides.html#scales-3",
    "title": "Data Visualization and Maps I",
    "section": "Scales",
    "text": "Scales\nThe scale_*() components control the properties of all the aesthetic dimensions mapped to the data.\nConsequently, there are scale_*() functions for all aesthetics such as:\n\npositions via scale_x_*() and scale_y_*()\ncolors via scale_color_*() and scale_fill_*()\nsizes via scale_size_*() and scale_radius_*()\nshapes via scale_shape_*() and scale_linetype_*()\ntransparency via scale_alpha_*()"
  },
  {
    "objectID": "slides/24-slides.html#scales-4",
    "href": "slides/24-slides.html#scales-4",
    "title": "Data Visualization and Maps I",
    "section": "Scales",
    "text": "Scales\nThe scale_*() components control the properties of all the aesthetic dimensions mapped to the data.\nThe extensions (*) can be filled by e.g.:\n\ncontinuous(), discrete(), reverse(), log10(), sqrt(), date() for positions\ncontinuous(), discrete(), manual(), gradient(), gradient2(), brewer() for colors\ncontinuous(), discrete(), manual(), ordinal(), area(), date() for sizes\ncontinuous(), discrete(), manual(), ordinal() for shapes\ncontinuous(), discrete(), manual(), ordinal(), date() for transparency"
  },
  {
    "objectID": "slides/24-slides.html#continuous-vs.-discrete-in-ggplot2",
    "href": "slides/24-slides.html#continuous-vs.-discrete-in-ggplot2",
    "title": "Data Visualization and Maps I",
    "section": "Continuous vs. Discrete in {ggplot2}",
    "text": "Continuous vs. Discrete in {ggplot2}\n\n\n\n\n\n\nContinuous:quantitative or numerical data\n\nheight\nweight\nage\ncounts\n\n\n\nDiscrete:qualitative or categorical data\n\nspecies\nsex\nstudy sites\nage group"
  },
  {
    "objectID": "slides/24-slides.html#continuous-vs.-discrete-in-ggplot2-1",
    "href": "slides/24-slides.html#continuous-vs.-discrete-in-ggplot2-1",
    "title": "Data Visualization and Maps I",
    "section": "Continuous vs. Discrete in {ggplot2}",
    "text": "Continuous vs. Discrete in {ggplot2}\n\n\n\n\n\n\nContinuous:quantitative or numerical data\n\nheight (continuous)\nweight (continuous)\nage (continuous or discrete)\ncounts (discrete)\n\n\n\nDiscrete:qualitative or categorical data\n\nspecies (nominal)\nsex (nominal)\nstudy site (nominal or ordinal)\nage group (ordinal)"
  },
  {
    "objectID": "slides/24-slides.html#aesthetics-scales",
    "href": "slides/24-slides.html#aesthetics-scales",
    "title": "Data Visualization and Maps I",
    "section": "Aesthetics + Scales",
    "text": "Aesthetics + Scales\n\n\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point()"
  },
  {
    "objectID": "slides/24-slides.html#aesthetics-scales-1",
    "href": "slides/24-slides.html#aesthetics-scales-1",
    "title": "Data Visualization and Maps I",
    "section": "Aesthetics + Scales",
    "text": "Aesthetics + Scales\n\n\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_x_date() +\n  scale_y_continuous() +\n  scale_color_discrete()"
  },
  {
    "objectID": "slides/24-slides.html#scales-5",
    "href": "slides/24-slides.html#scales-5",
    "title": "Data Visualization and Maps I",
    "section": "Scales",
    "text": "Scales\n\n\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_color_discrete()"
  },
  {
    "objectID": "slides/24-slides.html#coordinate-systems",
    "href": "slides/24-slides.html#coordinate-systems",
    "title": "Data Visualization and Maps I",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n= interpret the position aesthetics\n\n\nlinear coordinate systems: preserve the geometrical shapes\n\ncoord_cartesian()\ncoord_fixed()\ncoord_flip()\n\nnon-linear coordinate systems: likely change the geometrical shapes\n\ncoord_polar()\ncoord_map() and coord_sf()\ncoord_trans()"
  },
  {
    "objectID": "slides/19-slides.html#statistical-interpolation-1",
    "href": "slides/19-slides.html#statistical-interpolation-1",
    "title": "Areal Data and Proximity",
    "section": "Statistical Interpolation",
    "text": "Statistical Interpolation\n\n\n\nPrevious methods predict \\(z\\) as a (weighted) function of distance\nTreat the observations as perfect (no error)\nIf we imagine that \\(z\\) is the outcome of some spatial process such that:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder of inverse distance weighting - z (outcome) is just a function of distance"
  },
  {
    "objectID": "slides/19-slides.html#trend-surface-modeling",
    "href": "slides/19-slides.html#trend-surface-modeling",
    "title": "Areal Data and Proximity",
    "section": "Trend Surface Modeling",
    "text": "Trend Surface Modeling\n\nBasically a regression on the coordinates of your data points\nCoefficients apply to the coordinates and their interaction\nRelies on different functional forms\n\n\n\nStill no covariates, just prediction based on coordinates\nCoefficients describe how changes in x, y, or their interaction change z\nDraw out regression line and equation (z = b1x + b2y + b0)"
  },
  {
    "objectID": "slides/19-slides.html#th-order-trend-surface",
    "href": "slides/19-slides.html#th-order-trend-surface",
    "title": "Areal Data and Proximity",
    "section": "0th Order Trend Surface",
    "text": "0th Order Trend Surface\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplest form of trend surface\n\\(Z=a\\) where \\(a\\) is the mean value of air quality\nResult is a simple horizontal surface where all values are the same.\n\n\n\nIntercept only model (mean when all coefficients are 0)"
  },
  {
    "objectID": "slides/19-slides.html#th-order-trend-surface-1",
    "href": "slides/19-slides.html#th-order-trend-surface-1",
    "title": "Areal Data and Proximity",
    "section": "0th order trend surface",
    "text": "0th order trend surface\n\n#set up interpolation grid\n# Create an empty grid where n is the total number of cells\nbbox &lt;- st_bbox(id.cty)\ngrd &lt;- st_make_grid(id.cty, n=150, \n                    what = \"centers\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2])\n\n# Define the polynomial equation\nf.0  &lt;- as.formula(meanpm25 ~ 1)\n\n# Run the regression model\nlm.0 &lt;- lm( f.0 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var0.pred &lt;- predict(lm.0, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.0th &lt;- grd %&gt;%\n  select(X, Y, var0.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.0th, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + \n  tm_raster( title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/19-slides.html#st-order-trend-surface",
    "href": "slides/19-slides.html#st-order-trend-surface",
    "title": "Areal Data and Proximity",
    "section": "1st Order Trend Surface",
    "text": "1st Order Trend Surface\n\n\n\nCreates a slanted surface\n\\(Z = a + bX + cY\\)\nX and Y are the coordinate pairs"
  },
  {
    "objectID": "slides/19-slides.html#st-order-trend-surface-1",
    "href": "slides/19-slides.html#st-order-trend-surface-1",
    "title": "Areal Data and Proximity",
    "section": "1st Order Trend Surface",
    "text": "1st Order Trend Surface\n\n# Define the polynomial equation\nf.1  &lt;- as.formula(meanpm25 ~ X + Y)\n\naq.sum$X &lt;- st_coordinates(aq.sum)[,1]\naq.sum$Y &lt;- st_coordinates(aq.sum)[,2]\n\n# Run the regression model\nlm.1 &lt;- lm( f.1 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var1.pred &lt;- predict(lm.1, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.1st &lt;- grd %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.1st, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + \n  tm_raster( title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/19-slides.html#nd-order-trend-surfaces",
    "href": "slides/19-slides.html#nd-order-trend-surfaces",
    "title": "Areal Data and Proximity",
    "section": "2nd Order Trend Surfaces",
    "text": "2nd Order Trend Surfaces\n\nProduces a parabolic surface\n\\(Z = a + bX + cY + dX^2 + eY^2 + fXY\\)\nHighlights the interaction of both directions\n\n\n\nWe have an east-west trend and a north-south trend"
  },
  {
    "objectID": "slides/19-slides.html#nd-order-trend-surfaces-1",
    "href": "slides/19-slides.html#nd-order-trend-surfaces-1",
    "title": "Areal Data and Proximity",
    "section": "2nd Order Trend Surfaces",
    "text": "2nd Order Trend Surfaces\n\n# Define the 1st order polynomial equation\nf.2 &lt;- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n \n# Run the regression model\nlm.2 &lt;- lm( f.2, data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var2.pred &lt;- predict(lm.2, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.2nd &lt;- grd %&gt;%\n  select(X, Y, var2.pred) %&gt;%\n  st_drop_geometry()\n\nr   &lt;- rast(dat.2nd, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/19-slides.html#kriging",
    "href": "slides/19-slides.html#kriging",
    "title": "Areal Data and Proximity",
    "section": "Kriging",
    "text": "Kriging\n\n\nPrevious methods predict \\(z\\) as a (weighted) function of distance\nTreat the observations as perfect (no error)\nIf we imagine that \\(z\\) is the outcome of some spatial process such that:\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu(\\mathbf{x}) + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\]\nthen any observed value of \\(z\\) is some function of the process (\\(\\mu(\\mathbf{x})\\)) and some error (\\(\\epsilon(\\mathbf{x})\\))\n\nKriging exploits autocorrelation in \\(\\epsilon(\\mathbf{x})\\) to identify the trend and interpolate accordingly"
  },
  {
    "objectID": "slides/19-slides.html#autocorrelation",
    "href": "slides/19-slides.html#autocorrelation",
    "title": "Areal Data and Proximity",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nCorrelation the tendency for two variables to be related\nAutocorrelation the tendency for observations that are closer (in space or time) to be correlated\nPositive autocorrelation neighboring observations have \\(\\epsilon\\) with the same sign\nNegative autocorrelation neighboring observations have \\(\\epsilon\\) with a different sign (rare in geography)"
  },
  {
    "objectID": "slides/19-slides.html#ordinary-kriging",
    "href": "slides/19-slides.html#ordinary-kriging",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\nAssumes that the deterministic part of the process (\\(\\mu(\\mathbf{x})\\)) is an unknown constant (\\(\\mu\\))\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/19-slides.html#steps-for-ordinary-kriging",
    "href": "slides/19-slides.html#steps-for-ordinary-kriging",
    "title": "Areal Data and Proximity",
    "section": "Steps for Ordinary Kriging",
    "text": "Steps for Ordinary Kriging\n\nRemoving any spatial trend in the data (if present).\nComputing the experimental variogram, \\(\\gamma\\), which is a measure of spatial autocorrelation.\nDefining an experimental variogram model that best characterizes the spatial autocorrelation in the data.\nInterpolating the surface using the experimental variogram.\nAdding the kriged interpolated surface to the trend interpolated surface to produce the final output.\n\n\nFigure out the part of the spatial trend that’s not related to X/Y then merge them back together"
  },
  {
    "objectID": "slides/19-slides.html#removing-spatial-trend",
    "href": "slides/19-slides.html#removing-spatial-trend",
    "title": "Areal Data and Proximity",
    "section": "Removing Spatial Trend",
    "text": "Removing Spatial Trend\n\nMean and variance need to be constant across study area\nTrend surfaces indicate that is not the case\nNeed to remove that trend\n\n\nf.2 &lt;- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n \n# Run the regression model\nlm.2 &lt;- lm( f.2, data=aq.sum)\n\n# Copy the residuals to the point object\naq.sum$res &lt;- lm.2$residuals\n\n\nClarify what residuals are – distance of points from line in lm"
  },
  {
    "objectID": "slides/19-slides.html#removing-the-trend",
    "href": "slides/19-slides.html#removing-the-trend",
    "title": "Areal Data and Proximity",
    "section": "Removing the trend",
    "text": "Removing the trend\n\n\nBand of strong red/green colors indicate part of the state where the X/Y model isn’t doing well"
  },
  {
    "objectID": "slides/19-slides.html#calculate-the-experimental-variogram",
    "href": "slides/19-slides.html#calculate-the-experimental-variogram",
    "title": "Areal Data and Proximity",
    "section": "Calculate the experimental variogram",
    "text": "Calculate the experimental variogram\n\nnugget - the proportion of semivariance that occurs at small distances\nsill - the maximum semivariance between pairs of observations\nrange - the distance at which the sill occurs\nexperimental vs. fitted variograms\n\n\nVariogram - how variation changes over distance semivariogram - all positive (absolute value of residuals)"
  },
  {
    "objectID": "slides/19-slides.html#a-note-about-semivariograms",
    "href": "slides/19-slides.html#a-note-about-semivariograms",
    "title": "Areal Data and Proximity",
    "section": "A Note about Semivariograms",
    "text": "A Note about Semivariograms\n\n\nSill - things can only get so weird\nConnect to range in Ripley’s K"
  },
  {
    "objectID": "slides/19-slides.html#fitted-semivariograms",
    "href": "slides/19-slides.html#fitted-semivariograms",
    "title": "Areal Data and Proximity",
    "section": "Fitted Semivariograms",
    "text": "Fitted Semivariograms\n\nRely on functional forms to model semivariance"
  },
  {
    "objectID": "slides/19-slides.html#calculate-the-experimental-variogram-1",
    "href": "slides/19-slides.html#calculate-the-experimental-variogram-1",
    "title": "Areal Data and Proximity",
    "section": "Calculate the experimental variogram",
    "text": "Calculate the experimental variogram\n\nvar.cld  &lt;- gstat::variogram(res ~ 1, aq.sum, cloud = TRUE)\nvar.df  &lt;- as.data.frame(var.cld)\nindex1  &lt;- which(with(var.df, left==21 & right==2))"
  },
  {
    "objectID": "slides/19-slides.html#calculate-the-experimental-variogram-2",
    "href": "slides/19-slides.html#calculate-the-experimental-variogram-2",
    "title": "Areal Data and Proximity",
    "section": "Calculate the experimental variogram",
    "text": "Calculate the experimental variogram\n\nOP &lt;- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\n\npar(OP)"
  },
  {
    "objectID": "slides/19-slides.html#simplifying-the-cloud-plot",
    "href": "slides/19-slides.html#simplifying-the-cloud-plot",
    "title": "Areal Data and Proximity",
    "section": "Simplifying the cloud plot",
    "text": "Simplifying the cloud plot\n\n# Compute the sample experimental variogram\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE)\n\nbins.ct &lt;- c(0, var.smpl$dist , max(var.cld$dist) )\nbins &lt;- vector()\nfor (i in 1: (length(bins.ct) - 1) ){\n  bins[i] &lt;- mean(bins.ct[ seq(i,i+1, length.out=2)] ) \n}\nbins[length(bins)] &lt;- max(var.cld$dist)\nvar.bins &lt;- findInterval(var.cld$dist, bins)"
  },
  {
    "objectID": "slides/19-slides.html#simplifying-the-cloud-plot-1",
    "href": "slides/19-slides.html#simplifying-the-cloud-plot-1",
    "title": "Areal Data and Proximity",
    "section": "Simplifying the cloud plot",
    "text": "Simplifying the cloud plot\n\n# Point data cloud with bin boundaries\nOP &lt;- par( mar = c(5,6,1,1))\nplot(var.cld$gamma ~ eval(var.cld$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,\n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( gamma ) )\npoints( var.smpl$dist/1000, var.smpl$gamma, pch=21, col=\"black\", bg=\"red\", cex=1.3)\nabline(v=bins/1000, col=\"red\", lty=2)\n\npar(OP)"
  },
  {
    "objectID": "slides/19-slides.html#looking-at-the-sample-variogram",
    "href": "slides/19-slides.html#looking-at-the-sample-variogram",
    "title": "Areal Data and Proximity",
    "section": "Looking at the sample Variogram",
    "text": "Looking at the sample Variogram"
  },
  {
    "objectID": "slides/19-slides.html#estimating-the-sample-variogram",
    "href": "slides/19-slides.html#estimating-the-sample-variogram",
    "title": "Areal Data and Proximity",
    "section": "Estimating the sample variogram",
    "text": "Estimating the sample variogram\n\n# Compute the sample variogram, note the f.2 trend model is one of the parameters\n# passed to variogram(). This tells the function to create the variogram on\n# the de-trended data\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range= 60000, model=\"Gau\", cutoff=1000000))\n\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit)"
  },
  {
    "objectID": "slides/19-slides.html#ordinary-kriging-1",
    "href": "slides/19-slides.html#ordinary-kriging-1",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\n\n[using ordinary kriging]"
  },
  {
    "objectID": "slides/19-slides.html#ordinary-kriging-2",
    "href": "slides/19-slides.html#ordinary-kriging-2",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\ndat.krg &lt;- gstat::krige( formula = res~1, \n                         locations = aq.sum, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit)\n\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + \n  tm_raster(n=10, palette=\"RdBu\", title=\"Predicted residual \\nair quality\") +\n  tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/19-slides.html#combining-with-the-trend-data",
    "href": "slides/19-slides.html#combining-with-the-trend-data",
    "title": "Areal Data and Proximity",
    "section": "Combining with the trend data",
    "text": "Combining with the trend data\n\n\n[using universal kriging]"
  },
  {
    "objectID": "slides/19-slides.html#combining-with-the-trend-data-1",
    "href": "slides/19-slides.html#combining-with-the-trend-data-1",
    "title": "Areal Data and Proximity",
    "section": "Combining with the trend data",
    "text": "Combining with the trend data\n\ndat.krg &lt;- gstat::krige( formula = f.2, \n                         locations = aq.sum, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit)\n\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/19-slides.html#visualizing-uncertainty",
    "href": "slides/19-slides.html#visualizing-uncertainty",
    "title": "Areal Data and Proximity",
    "section": "Visualizing Uncertainty",
    "text": "Visualizing Uncertainty\n\n\nPoint out edge effects"
  },
  {
    "objectID": "slides/15-slides.html#objectives",
    "href": "slides/15-slides.html#objectives",
    "title": "Building Spatial Databases based on Location",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nCreate new features based on topological relationships\nUse topological subsetting to reduce features\nUse spatial joins to add attributes based on location"
  },
  {
    "objectID": "slides/15-slides.html#what-is-spatial-analysis",
    "href": "slides/15-slides.html#what-is-spatial-analysis",
    "title": "Building Spatial Databases based on Location",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n“The process of examining the locations, attributes, and relationships of features in spatial data through overlay and other analytical techniques in order to address a question or gain useful knowledge. Spatial analysis extracts or creates new information from spatial data”.\n\n— ESRI Dictionary"
  },
  {
    "objectID": "slides/15-slides.html#workflows-for-spatial-analysis",
    "href": "slides/15-slides.html#workflows-for-spatial-analysis",
    "title": "Building Spatial Databases based on Location",
    "section": "Workflows for spatial analysis",
    "text": "Workflows for spatial analysis\n\n\n\n\n\ncourtesy of Humboldt State University\n\n\n\n\nAlign processing with objectives\nImagining the visualizations and analysis clarifies file formats and variables\nHelps build reproducibility"
  },
  {
    "objectID": "slides/15-slides.html#databases-and-attributes",
    "href": "slides/15-slides.html#databases-and-attributes",
    "title": "Building Spatial Databases based on Location",
    "section": "Databases and Attributes",
    "text": "Databases and Attributes\n\n\n\n\n\ncourtesy of Giscommons\n\n\n\n\n\nAttributes: Information that further describes a spatial feature\nAttributes → predictors for analysis\nMonday’s focus on thematic relations between datasets\n\nShared ‘keys’ help define linkages between objects\n\nSometimes we are interested in attributes that describe location (overlaps, contains, distance)\nSometimes we want to join based on location rather than thematic connections\n\nMust have the same CRS"
  },
  {
    "objectID": "slides/15-slides.html#attributes-based-on-geometry-and-location-measures",
    "href": "slides/15-slides.html#attributes-based-on-geometry-and-location-measures",
    "title": "Building Spatial Databases based on Location",
    "section": "Attributes based on geometry and location (measures)",
    "text": "Attributes based on geometry and location (measures)\n\nAttributes like area and length can be useful for a number of analyses\n\nEstimates of ‘effort’ in sampling designs\nOffsets for modeling rates (e.g., Poisson regression)\n\nNeed to assign the result of the function to a column in data frame (e.g., $, mutate, and summarize)\nOften useful to test before assigning"
  },
  {
    "objectID": "slides/15-slides.html#estimating-area",
    "href": "slides/15-slides.html#estimating-area",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating area",
    "text": "Estimating area\n\n\n\n\nsf bases area (and length) calculations on the map units of the CRS\nthe units library allows conversion into a variety of units\n\n\n\n\nnz.sf &lt;- nz %&gt;% \n  mutate(area = st_area(nz))\nhead(nz.sf$area, 3)\n\nUnits: [m^2]\n[1] 12890576439  4911565037 24588819863\n\n\n\nnz.sf$areakm &lt;- units::set_units(st_area(nz), km^2)\nhead(nz.sf$areakm, 3)\n\nUnits: [km^2]\n[1] 12890.576  4911.565 24588.820"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons",
    "href": "slides/15-slides.html#estimating-density-in-polygons",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating new features based on the frequency of occurrence\nClarifying graphics\nUnderlies quadrat sampling for point patterns\nTwo steps: count and area"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons-1",
    "href": "slides/15-slides.html#estimating-density-in-polygons-1",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons\n\n\n\n\n\n\n\n\n\n\n\n\n\nnz.df &lt;- nz %&gt;% \nmutate(counts = lengths(st_intersects(., random_nz)),\n       area = st_area(nz),\n       density = counts/area)\nhead(st_drop_geometry(nz.df[,7:10]))\n\n  counts              area              density\n1     14 12890576439 [m^2] 1.086065e-09 [1/m^2]\n2      9  4911565037 [m^2] 1.832410e-09 [1/m^2]\n3     38 24588819863 [m^2] 1.545418e-09 [1/m^2]\n4     18 12271015945 [m^2] 1.466871e-09 [1/m^2]\n5      9  8364554416 [m^2] 1.075969e-09 [1/m^2]\n6     22 14242517871 [m^2] 1.544671e-09 [1/m^2]"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons-2",
    "href": "slides/15-slides.html#estimating-density-in-polygons-2",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons"
  },
  {
    "objectID": "slides/15-slides.html#estimating-distance",
    "href": "slides/15-slides.html#estimating-distance",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Distance",
    "text": "Estimating Distance\n\nAs a covariate\nFor use in covariance matrices\nAs a means of assigning connections in networks"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance",
    "href": "slides/15-slides.html#estimating-single-point-distance",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\n\n\nst_distance returns distances between all features in x and all features in y\nOne-to-One relationship requires choosing a single point for y"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance-1",
    "href": "slides/15-slides.html#estimating-single-point-distance-1",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\nSubsetting y into a single feature\n\n\n\n\ncanterbury = nz %&gt;% filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\nco = filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n\nUnits: [m]\n          [,1]     [,2]\n[1,] 123537.16 15497.72\n[2,]  94282.77     0.00\n[3,]  93018.56     0.00"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance-2",
    "href": "slides/15-slides.html#estimating-single-point-distance-2",
    "title": "Building Spatial Databases based on Location",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\nUsing nearest neighbor distances\n\n\n\n\nua &lt;- urban_areas(cb = FALSE, progress_bar = FALSE) %&gt;% \n  filter(., UATYP10 == \"U\") %&gt;% \n  filter(., str_detect(NAME10, \"ID\")) %&gt;% \n  st_transform(., crs=2163)\n\n#get index of nearest ID city\nnearest &lt;-  st_nearest_feature(ua)\n#estimate distance\n(dist = st_distance(ua, ua[nearest,], by_element=TRUE))\n\nUnits: [m]\n[1]  61373.575  61373.575   1647.128   1647.128 136917.546 136917.546"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-1",
    "href": "slides/15-slides.html#topological-subsetting-1",
    "title": "Building Spatial Databases based on Location",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\nTopological relations describe the spatial relationships between objects\nWe can use the overlap (or not) of vector data to subset the data based on topology\nNeed valid geometries\nEasiest way is to use [ notation, but also most restrictive\n\n\n\n\n\nctby_height &lt;-  nz_height[canterbury, ]"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-2",
    "href": "slides/15-slides.html#topological-subsetting-2",
    "title": "Building Spatial Databases based on Location",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\n\n\nLots of verbs in sf for doing this (e.g., st_intersects, st_contains, st_touches)\nsee ?geos_binary_pred for a full list\nCreates an implicit attribute (the records in x that are “in” y)\n\n\n\nUsing sparse=TRUE\n\nst_intersects(nz_height, co, \n              sparse = TRUE)[1:3] \n\n[[1]]\ninteger(0)\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 2\n\nlengths(st_intersects(nz_height, \n                      co, sparse = TRUE))[1:3] &gt; 0\n\n[1] FALSE  TRUE  TRUE"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-3",
    "href": "slides/15-slides.html#topological-subsetting-3",
    "title": "Building Spatial Databases based on Location",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\nThe sparse option controls how the results are returned\nWe can then find out if one or more elements satisfies the criteria\n\nUsing sparse=FALSE\n\nst_intersects(nz_height, co, sparse = FALSE)[1:3,] \n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,] FALSE  TRUE\n[3,] FALSE  TRUE\n\napply(st_intersects(nz_height, co, sparse = FALSE), 1,any)[1:3]\n\n[1] FALSE  TRUE  TRUE"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-4",
    "href": "slides/15-slides.html#topological-subsetting-4",
    "title": "Building Spatial Databases based on Location",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\n\ncanterbury_height3 = nz_height %&gt;%\n  filter(st_intersects(x = ., y = canterbury, sparse = FALSE))"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-1",
    "href": "slides/15-slides.html#spatial-joins-1",
    "title": "Building Spatial Databases based on Location",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\nsf package provides st_join for vectors\nAllows joins based on the predicates (st_intersects, st_touches, st_within_distance, etc.)\nDefault is a left join"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-2",
    "href": "slides/15-slides.html#spatial-joins-2",
    "title": "Building Spatial Databases based on Location",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\n\n\nset.seed(2018)\n(bb = st_bbox(world)) # the world's bounds\n\n      xmin       ymin       xmax       ymax \n-180.00000  -89.90000  179.99999   83.64513 \n\n#&gt;   xmin   ymin   xmax   ymax \n#&gt; -180.0  -89.9  180.0   83.6\nrandom_df = data.frame(\n  x = runif(n = 10, min = bb[1], max = bb[3]),\n  y = runif(n = 10, min = bb[2], max = bb[4])\n)\nrandom_points &lt;- random_df %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\")) %&gt;% # set coordinates\n  st_set_crs(\"EPSG:4326\") # set geographic CRS\n\nrandom_joined = st_join(random_points, world[\"name_long\"])"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-3",
    "href": "slides/15-slides.html#spatial-joins-3",
    "title": "Building Spatial Databases based on Location",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\nSometimes we may want to be less restrictive\nJust because objects don’t touch doesn’t mean they don’t relate to each other\nCan use predicates in st_join\nRemember that default is left_join (so the number of records can grow if multiple matches)"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-4",
    "href": "slides/15-slides.html#spatial-joins-4",
    "title": "Building Spatial Databases based on Location",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n\n[1] FALSE\n\nz = st_join(cycle_hire, cycle_hire_osm, st_is_within_distance, dist = 20)\nnrow(cycle_hire)\n\n[1] 742\n\nnrow(z)\n\n[1] 762"
  },
  {
    "objectID": "slides/15-slides.html#extending-joins-1",
    "href": "slides/15-slides.html#extending-joins-1",
    "title": "Building Spatial Databases based on Location",
    "section": "Extending Joins",
    "text": "Extending Joins\n\n\nSometimes we are interested in analyzing locations that contain the overlap between two vectors\n\nHow much of home range a occurs on soil type b\nHow much of each Census tract is contained with a service provision area?\n\nst_intersection, st_union, and st_difference return new geometries that we can use as records in our spatial database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintersect_pct &lt;- st_intersection(nc, tr_buff) %&gt;% \n   mutate(intersect_area = st_area(.)) %&gt;%   # create new column with shape area\n   dplyr::select(NAME, intersect_area) %&gt;%   # only select columns needed to merge\n   st_drop_geometry()\n\nnc &lt;- mutate(nc, county_area = st_area(nc))\n\n# Merge by county name\nnc &lt;- merge(nc, intersect_pct, by = \"NAME\", all.x = TRUE)\n\n# Calculate coverage\nnc &lt;- nc %&gt;% \n   mutate(coverage = as.numeric(intersect_area/county_area))"
  },
  {
    "objectID": "slides/15-slides.html#extending-joins-2",
    "href": "slides/15-slides.html#extending-joins-2",
    "title": "Building Spatial Databases based on Location",
    "section": "Extending Joins",
    "text": "Extending Joins"
  },
  {
    "objectID": "slides/21-slides.html#overlays",
    "href": "slides/21-slides.html#overlays",
    "title": "Statistical Modelling I",
    "section": "Overlays",
    "text": "Overlays\n\nMethods for identifying optimal site selection or suitability\nApply a common scale to diverse or dissimilar outputs"
  },
  {
    "objectID": "slides/21-slides.html#getting-started",
    "href": "slides/21-slides.html#getting-started",
    "title": "Statistical Modelling I",
    "section": "Getting Started",
    "text": "Getting Started\n\n\nDefine the problem.\nBreak the problem into submodels.\nDetermine significant layers.\nReclassify or transform the data within a layer.\nAdd or combine the layers.\nVerify\n\n\n\nExample of new park:\n\nWhat makes a good park? How should it integrate with other parks?\n\n2-3. How big should a park be? – so search for parcels that are big enough. Should it have Greenbelt access? – so only search parcels with Greenbelt access, etc."
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays",
    "href": "slides/21-slides.html#boolean-overlays",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays\n\n\n\nSuccessive disqualification of areas\nSeries of “yes/no” questions\n“Sieve” mapping"
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays-1",
    "href": "slides/21-slides.html#boolean-overlays-1",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays\n\nReclassifying\nWhich types of land are appropriate\n\n\nnlcd &lt;-  rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\nplot(nlcd)"
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays-2",
    "href": "slides/21-slides.html#boolean-overlays-2",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays\n\nWhich types of land are appropriate?\n\n\nnlcd.segments &lt;- segregate(nlcd)\nnames(nlcd.segments) &lt;- levels(nlcd)[[1]][-1,2]\nplot(nlcd.segments)"
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays-3",
    "href": "slides/21-slides.html#boolean-overlays-3",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays\n\nWhich types of land are appropriate?\n\n\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nslope &lt;- terrain(srtm, v = \"slope\")"
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays-4",
    "href": "slides/21-slides.html#boolean-overlays-4",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays\n\nMake sure data is aligned!\n\n\nsuit.slope &lt;- slope &lt; 10\nsuit.landcov &lt;- nlcd.segments[\"Shrubland\"]\nsuit.slope.match &lt;- project(suit.slope, suit.landcov)\nsuit &lt;- suit.slope.match + suit.landcov"
  },
  {
    "objectID": "slides/21-slides.html#boolean-overlays-5",
    "href": "slides/21-slides.html#boolean-overlays-5",
    "title": "Statistical Modelling I",
    "section": "Boolean Overlays",
    "text": "Boolean Overlays"
  },
  {
    "objectID": "slides/21-slides.html#challenges-with-boolean-overlays",
    "href": "slides/21-slides.html#challenges-with-boolean-overlays",
    "title": "Statistical Modelling I",
    "section": "Challenges with Boolean Overlays",
    "text": "Challenges with Boolean Overlays\n\nAssume relationships are really Boolean\nNo measurement error\nCategorical measurements are known exactly\nBoundaries are well-represented"
  },
  {
    "objectID": "slides/21-slides.html#a-more-general-approach",
    "href": "slides/21-slides.html#a-more-general-approach",
    "title": "Statistical Modelling I",
    "section": "A more general approach",
    "text": "A more general approach\n\nDefine a favorability metric\n\n\\[\n\\begin{equation}\nF(\\mathbf{s}) = \\prod_{M=1}^{m}X_m(\\mathbf{s})\n\\end{equation}\n\\]\n\nTreat \\(F(\\mathbf{s})\\) as binary\nThen \\(F(\\mathbf{s}) = 1\\) if all inputs (\\(X_m(\\mathbf{s})\\)) are suitable\nThen \\(F(\\mathbf{s}) = 0\\) if not"
  },
  {
    "objectID": "slides/21-slides.html#estimating-favorability",
    "href": "slides/21-slides.html#estimating-favorability",
    "title": "Statistical Modelling I",
    "section": "Estimating favorability",
    "text": "Estimating favorability\n\\[\n\\begin{equation}\nF(\\mathbf{s}) = f(w_1X_1(\\mathbf{s}), w_2X_2(\\mathbf{s}), w_3X_3(\\mathbf{s}), ..., w_mX_m(\\mathbf{s}))\n\\end{equation}\n\\]\n\n\\(F(\\mathbf{s})\\) does not have to be binary (could be ordinal or continuous)\n\\(X_m(\\mathbf{s})\\) could also be extended beyond simply ‘suitable/not suitable’\nAdding weights allows incorporation of relative importance\nOther functions for combining inputs (\\(X_m(\\mathbf{s})\\))"
  },
  {
    "objectID": "slides/21-slides.html#weighted-linear-combinations",
    "href": "slides/21-slides.html#weighted-linear-combinations",
    "title": "Statistical Modelling I",
    "section": "Weighted Linear Combinations",
    "text": "Weighted Linear Combinations\n\\[\n\\begin{equation}\nF(\\mathbf{s}) = \\frac{\\sum_{i=1}^{m}w_iX_i(\\mathbf{s})}{\\sum_{i=1}^{m}w_i}\n\\end{equation}\n\\]\n\n\\(F(s)\\) is now an index based on the values of \\(X_m(\\mathbf{s})\\)\n\\(w_i\\) can incorporate weights of evidence, uncertainty, or different participant preferences\nDividing by \\(\\sum_{i=1}^{m}w_i\\) normalizes by the sum of weights"
  },
  {
    "objectID": "slides/21-slides.html#model-driven-overlay",
    "href": "slides/21-slides.html#model-driven-overlay",
    "title": "Statistical Modelling I",
    "section": "Model-driven overlay",
    "text": "Model-driven overlay\n\\[\n\\begin{equation}\nF(\\mathbf{s}) = w_0 + \\sum_{i=1}^{m}w_iX_i(\\mathbf{s}) + \\epsilon\n\\end{equation}\n\\]\n\nIf we estimate \\(w_i\\) using data, we specify \\(F(s)\\) as the outcome of regression\nWhen \\(F(s)\\) is binary → logistic regression\nWhen \\(F(s)\\) is continuous → linear (gamma) regression\nWhen \\(F(s)\\) is discrete → Poisson regression\nAssumptions about \\(\\epsilon\\) matter!!\n\n\nWith spatial autocorrelation, assumptions about \\(\\epsilon\\) are probably wrong, but we’ll start simple today and assume it’s ok. Later we’ll talk about modelling error."
  },
  {
    "objectID": "slides/21-slides.html#why-do-we-create-distribution-models",
    "href": "slides/21-slides.html#why-do-we-create-distribution-models",
    "title": "Statistical Modelling I",
    "section": "Why do we create distribution models?",
    "text": "Why do we create distribution models?\n\n\n\n\nTo identify important correlations between predictors and the occurrence of an event\nGenerate maps of the ‘range’ or ‘niche’ of events\nUnderstand spatial patterns of event co-occurrence\nForecast changes in event distributions\n\n\n\n\n\n\nFrom Wiens et al. 2009"
  },
  {
    "objectID": "slides/21-slides.html#general-analysis-situation",
    "href": "slides/21-slides.html#general-analysis-situation",
    "title": "Statistical Modelling I",
    "section": "General analysis situation",
    "text": "General analysis situation\n\nFrom Long\n\nSpatially referenced locations of events \\((\\mathbf{y})\\) sampled from the study extent\nA matrix of predictors \\((\\mathbf{X})\\) that can be assigned to each event based on spatial location\n\n\nGoal: Estimate the probability of occurrence of events across unsampled regions of the study area based on correlations with predictors"
  },
  {
    "objectID": "slides/21-slides.html#modeling-presence-absence-data",
    "href": "slides/21-slides.html#modeling-presence-absence-data",
    "title": "Statistical Modelling I",
    "section": "Modeling Presence-Absence Data",
    "text": "Modeling Presence-Absence Data\n\n\n\nRandom or systematic sample of the study region\nThe presence (or absence) of the event is recorded for each point\nHypothesized predictors of occurrence are measured (or extracted) at each point\n\n\n\n\n\nFrom By Ragnvald - Own work, CC BY-SA 3.0"
  },
  {
    "objectID": "slides/21-slides.html#logistic-regression",
    "href": "slides/21-slides.html#logistic-regression",
    "title": "Statistical Modelling I",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\nWe can model favorability as the probability of occurrence using a logistic regression\nA link function maps the linear predictor \\((\\mathbf{x_i}'\\beta + \\alpha)\\) onto the support (0-1) for probabilities\nEstimates of \\(\\beta\\) can then be used to generate ‘wall-to-wall’ spatial predictions\n\n\n\n\\[\n\\begin{equation}\ny_{i} \\sim \\text{Bern}(p_i)\\\\\n\\text{link}(p_i) = \\mathbf{x_i}'\\beta + \\alpha\n\\end{equation}\n\\]\n\n\n\nFrom Mendoza"
  },
  {
    "objectID": "slides/21-slides.html#an-example",
    "href": "slides/21-slides.html#an-example",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nInputs from the dismo package"
  },
  {
    "objectID": "slides/21-slides.html#an-example-1",
    "href": "slides/21-slides.html#an-example-1",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nThe sample data\n\n\n\nhead(pres.abs)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -106.75 ymin: 31.25 xmax: -98.75 ymax: 37.75\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs\n  y              geometry\n1 0  POINT (-99.25 35.25)\n2 1  POINT (-98.75 36.25)\n3 1 POINT (-106.75 35.25)\n4 0 POINT (-100.75 31.25)\n5 1  POINT (-99.75 37.75)\n6 1 POINT (-104.25 36.75)"
  },
  {
    "objectID": "slides/21-slides.html#an-example-2",
    "href": "slides/21-slides.html#an-example-2",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nBuilding our dataframe\n\npts.df &lt;- terra::extract(pred.stack, vect(pres.abs), df=TRUE)\nhead(pts.df)\n\n  ID MeanAnnTemp TotalPrecip PrecipWetQuarter PrecipDryQuarter MinTempCold\n1  1         155         667              253               71         350\n2  2         147         678              266               66         351\n3  3         123         261              117               40         329\n4  4         181         533              198               69         348\n5  5         127         589              257               48         338\n6  6          83         438              213               38         278\n  TempRange\n1       -45\n2       -58\n3       -64\n4        -5\n5       -81\n6      -107"
  },
  {
    "objectID": "slides/21-slides.html#an-example-3",
    "href": "slides/21-slides.html#an-example-3",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nBuilding our dataframe\n\npts.df[,2:7] &lt;- scale(pts.df[,2:7])\nsummary(pts.df)\n\n       ID          MeanAnnTemp       TotalPrecip      PrecipWetQuarter \n Min.   :  1.00   Min.   :-3.3729   Min.   :-1.3377   Min.   :-1.6926  \n 1st Qu.: 25.75   1st Qu.:-0.4594   1st Qu.:-0.7980   1st Qu.:-0.6895  \n Median : 50.50   Median : 0.2282   Median :-0.2373   Median :-0.2224  \n Mean   : 50.50   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 75.25   3rd Qu.: 0.7118   3rd Qu.: 0.7140   3rd Qu.: 0.6508  \n Max.   :100.00   Max.   : 1.4285   Max.   : 2.4843   Max.   : 2.2713  \n PrecipDryQuarter   MinTempCold        TempRange      \n Min.   :-1.0828   Min.   :-3.9919   Min.   :-2.7924  \n 1st Qu.:-0.7013   1st Qu.:-0.0598   1st Qu.:-0.5216  \n Median :-0.3770   Median : 0.3582   Median : 0.2075  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.4290   3rd Qu.: 0.5495   3rd Qu.: 0.6450  \n Max.   : 3.1713   Max.   : 1.1092   Max.   : 2.0407"
  },
  {
    "objectID": "slides/21-slides.html#an-example-4",
    "href": "slides/21-slides.html#an-example-4",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nLooking at correlations\n\npairs(pts.df[,2:7])"
  },
  {
    "objectID": "slides/21-slides.html#an-example-5",
    "href": "slides/21-slides.html#an-example-5",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nLooking at correlations\n\ncorrplot(cor(pts.df[,2:7]), method = \"number\")"
  },
  {
    "objectID": "slides/21-slides.html#an-example-6",
    "href": "slides/21-slides.html#an-example-6",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nFitting some models\n\npts.df &lt;- cbind(pts.df, pres.abs$y)\ncolnames(pts.df)[8] &lt;- \"y\"\nlogistic.global &lt;- glm(y~., family=binomial(link=\"logit\"), data=pts.df[,2:8])\nlogistic.simple &lt;- glm(y ~ MeanAnnTemp + TotalPrecip, family=binomial(link=\"logit\"), data=pts.df[,2:8])\nlogistic.rich &lt;- glm(y ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter, family=binomial(link=\"logit\"), data=pts.df[,2:8])"
  },
  {
    "objectID": "slides/21-slides.html#an-example-7",
    "href": "slides/21-slides.html#an-example-7",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nChecking out the results\n\nsummary(logistic.global)\n\n\nCall:\nglm(formula = y ~ ., family = binomial(link = \"logit\"), data = pts.df[, \n    2:8])\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -1.4461     0.5096  -2.837  0.00455 **\nMeanAnnTemp       -6.3578     6.1645  -1.031  0.30237   \nTotalPrecip        7.1453     4.5577   1.568  0.11694   \nPrecipWetQuarter  -5.4207     3.0432  -1.781  0.07487 . \nPrecipDryQuarter  -1.3110     2.2482  -0.583  0.55981   \nMinTempCold        3.0890     2.6334   1.173  0.24080   \nTempRange         -0.6213     4.5470  -0.137  0.89131   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 125.374  on 99  degrees of freedom\nResidual deviance:  51.764  on 93  degrees of freedom\nAIC: 65.764\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "slides/21-slides.html#an-example-8",
    "href": "slides/21-slides.html#an-example-8",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nChecking out the results\n\nsummary(logistic.simple)\n\n\nCall:\nglm(formula = y ~ MeanAnnTemp + TotalPrecip, family = binomial(link = \"logit\"), \n    data = pts.df[, 2:8])\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.9880     0.3145  -3.141  0.00168 ** \nMeanAnnTemp  -2.9990     0.6647  -4.512 6.42e-06 ***\nTotalPrecip   0.3924     0.3827   1.025  0.30517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 125.374  on 99  degrees of freedom\nResidual deviance:  68.108  on 97  degrees of freedom\nAIC: 74.108\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/21-slides.html#an-example-9",
    "href": "slides/21-slides.html#an-example-9",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nChecking out the results\n\nsummary(logistic.rich)\n\n\nCall:\nglm(formula = y ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter, \n    family = binomial(link = \"logit\"), data = pts.df[, 2:8])\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.96504    0.35650  -2.707  0.00679 ** \nMeanAnnTemp      -2.85446    0.66142  -4.316 1.59e-05 ***\nPrecipWetQuarter  0.03212    0.43102   0.075  0.94060    \nPrecipDryQuarter  0.16759    0.64935   0.258  0.79634    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 125.374  on 99  degrees of freedom\nResidual deviance:  69.006  on 96  degrees of freedom\nAIC: 77.006\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/21-slides.html#an-example-10",
    "href": "slides/21-slides.html#an-example-10",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nComparing models\n\nAIC(logistic.global, logistic.simple, logistic.rich)\n\n                df      AIC\nlogistic.global  7 65.76394\nlogistic.simple  3 74.10760\nlogistic.rich    4 77.00622"
  },
  {
    "objectID": "slides/21-slides.html#an-example-11",
    "href": "slides/21-slides.html#an-example-11",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nGenerating predictions\n\npreds &lt;- predict(object=pred.stack, model=logistic.simple)\nplot(preds)\nplot(pres.pts$geometry, add=TRUE, pch=3, col=\"blue\")\nplot(abs.pts$geometry, add=TRUE, pch =\"-\", col=\"red\")"
  },
  {
    "objectID": "slides/21-slides.html#an-example-12",
    "href": "slides/21-slides.html#an-example-12",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nGenerating predictions\n\npreds &lt;- predict(object=pred.stack, model=logistic.simple, type=\"response\")\nplot(preds)\nplot(pres.pts$geometry, add=TRUE, pch=3, col=\"blue\")\nplot(abs.pts$geometry, add=TRUE, pch =\"-\", col=\"red\")"
  },
  {
    "objectID": "slides/21-slides.html#an-example-13",
    "href": "slides/21-slides.html#an-example-13",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nGenerating predictions\n\npreds &lt;- predict(object=pred.stack, model=logistic.global, type=\"response\")\nplot(preds)\nplot(pres.pts$geometry, add=TRUE, pch=3, col=\"blue\")\nplot(abs.pts$geometry, add=TRUE, pch =\"-\", col=\"red\")"
  },
  {
    "objectID": "slides/21-slides.html#an-example-14",
    "href": "slides/21-slides.html#an-example-14",
    "title": "Statistical Modelling I",
    "section": "An Example",
    "text": "An Example\nGenerating predictions\n\npreds &lt;- predict(object=pred.stack, model=logistic.rich, type=\"response\")\nplot(preds)\nplot(pres.pts$geometry, add=TRUE, pch=3, col=\"blue\")\nplot(abs.pts$geometry, add=TRUE, pch =\"-\", col=\"red\")"
  },
  {
    "objectID": "slides/21-slides.html#key-assumptions-of-logistic-regression",
    "href": "slides/21-slides.html#key-assumptions-of-logistic-regression",
    "title": "Statistical Modelling I",
    "section": "Key assumptions of logistic regression",
    "text": "Key assumptions of logistic regression\n\nDependent variable must be binary\nObservations must be independent (important for spatial analyses)\nPredictors should not be collinear\nPredictors should be linearly related to the log-odds\nSample Size"
  },
  {
    "objectID": "slides/12-slides.html#objectives",
    "href": "slides/12-slides.html#objectives",
    "title": "Operations on Raster Data I",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis"
  },
  {
    "objectID": "slides/12-slides.html#projecting-raster-data",
    "href": "slides/12-slides.html#projecting-raster-data",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\n\nTransformation from lat/long to planar CRS involves some loss of precision\nNew cell values estimated using overlap with original cells\nInterpolation for continuous data, nearest neighbor for categorical data\nEqual-area projections are preferred; especially for large areas\n\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nr &lt;- rast(xmin=-110, xmax=-90, ymin=40, ymax=60, ncols=40, nrows=40)\nvalues(r) &lt;- 1:ncell(r)\nplot(r)"
  },
  {
    "objectID": "slides/12-slides.html#projecting-raster-data-1",
    "href": "slides/12-slides.html#projecting-raster-data-1",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\nsimple method; alignment not guaranteed\n\n\nnewcrs &lt;- \"+proj=robin +datum=WGS84\"\npr1 &lt;- terra::project(r, newcrs)\nplot(pr1)\n\n\n\n\n\n\n\n\n\n\nproviding a template to ensure alignment\n\n\nx &lt;- rast(pr1)\n# Set the cell size\nres(x) &lt;- 200000\npr3 &lt;- terra::project(r, x)\nplot(pr3)"
  },
  {
    "objectID": "slides/12-slides.html#aligning-data-resample",
    "href": "slides/12-slides.html#aligning-data-resample",
    "title": "Operations on Raster Data I",
    "section": "Aligning Data: resample",
    "text": "Aligning Data: resample\n\nr &lt;- rast(nrow=3, ncol=3, xmin=0, xmax=10, ymin=0, ymax=10)\nvalues(r) &lt;- 1:ncell(r)\ns &lt;- rast(nrow=25, ncol=30, xmin=1, xmax=11, ymin=-1, ymax=11)\nx &lt;- resample(r, s, method=\"bilinear\")"
  },
  {
    "objectID": "slides/12-slides.html#downscaling-and-upscaling",
    "href": "slides/12-slides.html#downscaling-and-upscaling",
    "title": "Operations on Raster Data I",
    "section": "Downscaling and Upscaling",
    "text": "Downscaling and Upscaling\n\nAligning data for later analysis\nRemembering scale\nThinking about support"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions",
    "href": "slides/12-slides.html#changing-resolutions",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions",
    "text": "Changing resolutions\n\naggregate, disaggregate, resample allow changes in cell size\naggregate requires a function (e.g., mean() or min()) to determine what to do with the grouped values\nresample allows changes in cell size and shifting of cell centers (slower)"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions-aggregate",
    "href": "slides/12-slides.html#changing-resolutions-aggregate",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: aggregate",
    "text": "Changing resolutions: aggregate\n\n\n\nr &lt;- rast()\nr\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\nra &lt;- aggregate(r, 20)\nra\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\nplot(ra)"
  },
  {
    "objectID": "slides/12-slides.html#changing-resolutions-disagg",
    "href": "slides/12-slides.html#changing-resolutions-disagg",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: disagg",
    "text": "Changing resolutions: disagg\n\n\n\nra &lt;- aggregate(r, 20)\nplot(ra)\n\n\n\n\n\n\n\n\n\n\nrd &lt;- disagg(r, 20)\n\n\nrd\n\nclass       : SpatRaster \ndimensions  : 3600, 7200, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : spat_14dc31c8319c_5340.tif \nname        : lyr.1 \nmin value   :     1 \nmax value   : 64800 \n\nplot(rd)"
  },
  {
    "objectID": "slides/12-slides.html#dealing-with-different-extents",
    "href": "slides/12-slides.html#dealing-with-different-extents",
    "title": "Operations on Raster Data I",
    "section": "Dealing with Different Extents",
    "text": "Dealing with Different Extents\n\n\n\nRaster extents often larger than our analysis\nReducing memory and computational resources\nMaking attractive maps"
  },
  {
    "objectID": "slides/12-slides.html#using-terracrop",
    "href": "slides/12-slides.html#using-terracrop",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinate Reference System must be the same for both objects\nCrop is based on the (converted) SpatExtent of the 2nd object\nsnap describes how y will be aligned to the raster\nReturns all data within the extent"
  },
  {
    "objectID": "slides/12-slides.html#using-terracrop-1",
    "href": "slides/12-slides.html#using-terracrop-1",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n[1] TRUE\n\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")"
  },
  {
    "objectID": "slides/12-slides.html#using-mask",
    "href": "slides/12-slides.html#using-mask",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nOften want to get rid of all values outside of vector\nCan set mask=TRUE in crop() (y must be SpatVector)\nOr use mask()\n\n\n\n\n\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion))\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/12-slides.html#using-mask-1",
    "href": "slides/12-slides.html#using-mask-1",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nAllows more control over what the mask does\nCan set maskvalues and updatevalues to change the resulting raster\nCan also use inverse to mask out the vector\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=0)\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/12-slides.html#extending-boundaries",
    "href": "slides/12-slides.html#extending-boundaries",
    "title": "Operations on Raster Data I",
    "section": "Extending boundaries",
    "text": "Extending boundaries\n\n\nVector slightly larger than raster\nEspecially when using buffered datasets\nCan use extend\nNot exact; depends on snap()\n\n\n\n\n\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\next(vect(zion.buff))\n\nSpatExtent : -113.343652923976, -112.745006809213, 37.0477357596604, 37.5977812137969 (xmin, xmax, ymin, ymax)"
  },
  {
    "objectID": "slides/12-slides.html#practice",
    "href": "slides/12-slides.html#practice",
    "title": "Operations on Raster Data I",
    "section": "Practice",
    "text": "Practice\n\nmosaic is a funciton that combines adjacent rasters, but they need to have the same origin and resolution. Let’s practice preparing some rasters for a mosaic.\n\nLoad wildfire hazard data from the rasterexample folder for OR and ID: Copy of CRPS_OR.tif and Copy of CRPS_ID.tif.\nThese rasters have a fine resolution that will make our calculations slow. Transform them to have a resolution of 900 m.\nDo the rasters have the same CRS, origin, resolution, and extent? Check this with ==.\nUse new functions from this lecture to align the properties mentioned in #3 (plot often to check your work). Why not use project?\nmosaic the two rasters together."
  },
  {
    "objectID": "example/session-16-example.html",
    "href": "example/session-16-example.html",
    "title": "Session 16 code",
    "section": "",
    "text": "Load libraries:\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nLoad data:\nCode\nhospitals_pnw &lt;- read_csv(\"/opt/data/data/assignment06/landmarks_pnw.csv\") %&gt;%\n  filter(., MTFCC == \"K2543\") %&gt;%\n  st_as_sf(., coords = c(\"longitude\", \"latitude\"), crs=4269) %&gt;%\n  st_transform(crs = 5070)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Integrating vector and raster data"
    ]
  },
  {
    "objectID": "example/session-16-example.html#rasterize",
    "href": "example/session-16-example.html#rasterize",
    "title": "Session 16 code",
    "section": "Rasterize",
    "text": "Rasterize\n\n\nCode\nraster_template = rast(ext(hospitals_pnw), resolution = 10000,\n                       crs = st_crs(hospitals_pnw)$wkt)\n\nhosp_raster1 = rasterize(hospitals_pnw, raster_template,\n                         field = 1)\n\nplot(hosp_raster1, colNA = \"navy\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# add dummy numeric data\nhospitals_pnw$rand_capacity &lt;- rnorm(n = nrow(hospitals_pnw),\n                                     mean = 5000,\n                                     sd = 2000)\n\nhosp_raster3 = rasterize(hospitals_pnw, raster_template, \n                         field = \"rand_capacity\", fun = sum)\n\nplot(hosp_raster3)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Integrating vector and raster data"
    ]
  },
  {
    "objectID": "example/session-16-example.html#raster-to-vector",
    "href": "example/session-16-example.html#raster-to-vector",
    "title": "Session 16 code",
    "section": "Raster to Vector",
    "text": "Raster to Vector\n\n\nCode\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Integrating vector and raster data"
    ]
  },
  {
    "objectID": "example/session-16-example.html#creating-new-data",
    "href": "example/session-16-example.html#creating-new-data",
    "title": "Session 16 code",
    "section": "Creating New Data",
    "text": "Creating New Data\n\nDistance\n\n\nCode\nraster_template = rast(ext(hospitals_pnw), resolution = 1000,\n                       crs = st_crs(hospitals_pnw)$wkt)\nhosp_raster1 = rasterize(hospitals_pnw, raster_template,\n                       field = 1)\n\nhosp_dist_rast &lt;- distance(hosp_raster1)\nplot(hosp_dist_rast)\n\n\n\n\n\n\n\n\n\n\n\nAttributes\n\n\nCode\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment07/wildfire_hazard_agg.tif\")\n\n\n\n\nCode\nhospitals_pnw_proj &lt;- st_transform(hospitals_pnw, crs(wildfire_haz))\n\nhosp_fire_haz &lt;- terra::extract(wildfire_haz, hospitals_pnw_proj)\nhead(hosp_fire_haz)\n\n\n  ID    WHP_ID\n1  1 1952.8750\n2  2    0.0000\n3  3  741.4531\n4  4  200.2812\n5  5    0.0000\n6  6  150.5938\n\n\nCode\nhospitals_pnw_proj$wildfire &lt;- hosp_fire_haz$WHP_ID\n\n\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment06/cejst_pnw.shp\") %&gt;%\n  st_transform(crs = crs(wildfire_haz)) %&gt;%\n  filter(!st_is_empty(.))\n\n\n\n\nCode\nwildfire.zones &lt;- terra::zonal(wildfire_haz, vect(cejst), fun=\"mean\", na.rm=TRUE)\n\nhead(wildfire.zones)\n\n\n       WHP_ID\n1    3.053172\n2 2997.795051\n3    6.647930\n4   85.971309\n5   34.706535\n6   17.306250",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Integrating vector and raster data"
    ]
  },
  {
    "objectID": "example/session-16-example.html#practice",
    "href": "example/session-16-example.html#practice",
    "title": "Session 16 code",
    "section": "Practice",
    "text": "Practice\n\nStep 1: Load libraries\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\n\n\n\n\nStep 2: Load in data\nWe loaded the wildfire hazard data and the cejst data earlier in the example.\n\n\nCode\n# custom function to download and load Forest Service data\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)\n\n\n\n\nStep 3: Check validity\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] FALSE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] FALSE\n\n\n\n\nCode\nfs.bdry &lt;- st_make_valid(fs.bdry)\n\ncflrp.bdry &lt;- st_make_valid(cflrp.bdry)\n\n\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] TRUE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] TRUE\n\n\n\n\nStep 3: Check alignment\n\n\nCode\nst_crs(fs.bdry)\n\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nCode\nst_crs(wildfire_haz)\n\n\nCoordinate Reference System:\n  User input: unnamed \n  wkt:\nPROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nCode\nst_crs(cflrp.bdry)\n\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nCode\nst_crs(cejst)\n\n\nCoordinate Reference System:\n  User input: PROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]] \n  wkt:\nPROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cejst)\n\n\n[1] TRUE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(fs.bdry)\n\n\n[1] FALSE\n\n\nCode\ntarget_crs &lt;- st_crs(wildfire_haz)\n\n\n\n\nCode\nfs.bdry_proj &lt;- st_transform(fs.bdry, crs = target_crs)\nst_crs(fs.bdry_proj) == target_crs\n\n\n[1] TRUE\n\n\nCode\nst_crs(fs.bdry_proj) == st_crs(wildfire_haz)\n\n\n[1] TRUE\n\n\nCode\ncflrp.bdry_proj &lt;- st_transform(cflrp.bdry, crs = st_crs(wildfire_haz))\n\n\n\n\nStep 4: Subset to geographies\ncejst is our study area (the Pacific Northwest) to subset to.\n\n\nCode\nfs.subset &lt;- fs.bdry_proj[cejst, ]\ncflrp.subset &lt;- cflrp.bdry_proj[cejst, ]\n\n# keep only tracts that intersect Forest Service land\ncejst.subset &lt;- cejst[fs.subset, ]\n\n\n\n\nStep 5: Select attributes of interest\n\n\nCode\ncejst.df &lt;- cejst.subset %&gt;% \n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\nhead(cejst.df)\n\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -1598729 ymin: 2388182 xmax: -1475201 ymax: 3000813\nProjected CRS: unnamed\n       GEOID10 LMI_PFS LHE HBF_PFS                       geometry\n2  16025970100    0.75   0    0.60 MULTIPOLYGON (((-1485848 24...\n17 16057005500    0.43   0    0.44 MULTIPOLYGON (((-1567845 28...\n21 16057005600    0.30   0    0.05 MULTIPOLYGON (((-1573408 28...\n22 16009950100    0.55   1    0.40 MULTIPOLYGON (((-1566013 28...\n23 16017950500    0.75   1    0.53 MULTIPOLYGON (((-1545064 29...\n24 16017950400    0.60   1    0.68 MULTIPOLYGON (((-1544958 29...\n\n\n\n\nStep 6: Extract wildfire hazard\n\n\nCode\ncejst.fire &lt;- terra::extract(wildfire_haz, vect(cejst.df), fun=mean, na.rm=TRUE)\nhead(cejst.fire)\n\n\n  ID    WHP_ID\n1  1 2997.7951\n2  2  182.8864\n3  3  386.9580\n4  4  173.1703\n5  5  193.4199\n6  6  210.4406\n\n\n\n\nCode\ncejst.df$WHP_ID &lt;- cejst.fire$WHP_ID\n\n\n\n\nStep 7: Does each tract intersect a CFLRP boundary?\n\n\nCode\ncejst.cflrp &lt;- apply(st_intersects(cejst.df, cflrp.subset, sparse=FALSE), 1, any)\n\n\n\n\nCode\ncejst.df &lt;- cejst.df %&gt;%\n  mutate(CFLRP = cejst.cflrp)\n\n\n\n\nStep 8: Compare areas\nMany comparisons are possible! Here are some examples.\n\n\nCode\ncflrp.summ &lt;- cejst.df %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(CFLRP) %&gt;%\n  summarise(across(LMI_PFS:WHP_ID, ~ mean(.x, na.rm=TRUE)))\n\nggplot(data = cejst.df, aes(x=CFLRP, y=WHP_ID)) +\n  geom_boxplot()",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Integrating vector and raster data"
    ]
  },
  {
    "objectID": "example/session-23-example.html",
    "href": "example/session-23-example.html",
    "title": "Session 23 code",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(tree)\nlibrary(randomForest)\n\n\n\n\n\n\n\nCode\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)\n\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\", quiet=TRUE) %&gt;%\n  filter(!st_is_empty(.))\n\n\n\n\n\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] FALSE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] FALSE\n\n\nCode\nfs.bdry &lt;- st_make_valid(fs.bdry)\ncflrp.bdry &lt;- st_make_valid(cflrp.bdry)\n\n\n\n\n\n\n\nCode\nst_crs(wildfire_haz) == st_crs(fs.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cflrp.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cejst)\n\n\n[1] FALSE\n\n\nCode\nfs.bdry_proj &lt;- st_transform(fs.bdry, crs = st_crs(wildfire_haz))\ncflrp.bdry_proj &lt;- st_transform(cflrp.bdry, crs = st_crs(wildfire_haz))\ncejst_proj &lt;- st_transform(cejst, crs = st_crs(wildfire_haz))\n\n\n\n\n\n\n\nCode\nfs.bdry_sub &lt;- fs.bdry_proj[cejst_proj, ]\ncflrp.bdry_sub &lt;- cflrp.bdry_proj[cejst_proj, ]\n\ncejst_sub &lt;- cejst_proj[fs.bdry_sub, ]\n\n\n\n\n\n\n\nCode\ncejst_sub &lt;- cejst_sub %&gt;%\n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\n\n\n\n\n\n\n\nCode\nwf_risk &lt;- terra::extract(wildfire_haz, cejst_sub, fun=mean)\n\ncejst_sub$WHP_ID &lt;- wf_risk$WHP_ID\n\n\n\n\n\n\n\nCode\ncflrp &lt;- apply(st_intersects(cejst_sub, cflrp.bdry_sub, sparse = FALSE), 1, any)\n\ncejst_sub$CFLRP &lt;- cflrp\n\n\n\n\n\n\n\nCode\ncejst_mod &lt;- cejst_sub %&gt;%\n  st_drop_geometry(.) %&gt;%\n  na.omit(.)\n\ncejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")] &lt;- scale(cejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")])\n\n\n\n\n\n\nCode\nlogistic.global &lt;- glm(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID,\n                       family = binomial(link = \"logit\"),\n                       data = cejst_mod)\nsummary(logistic.global)\n\n\n\nCall:\nglm(formula = CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, family = binomial(link = \"logit\"), \n    data = cejst_mod)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.66462    0.14347  -4.632 3.62e-06 ***\nLMI_PFS      0.17284    0.16881   1.024    0.306    \nLHE         -0.25551    0.15791  -1.618    0.106    \nHBF_PFS     -0.01511    0.16081  -0.094    0.925    \nWHP_ID       0.88902    0.16785   5.297 1.18e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 330.74  on 255  degrees of freedom\nResidual deviance: 290.03  on 251  degrees of freedom\nAIC: 300.03\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\nCode\nlibrary(tree)\ncejst_mod$CFLRP &lt;- as.factor(ifelse(cejst_mod$CFLRP == 1, \"Yes\", \"No\"))\ntree.model &lt;- tree(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, cejst_mod)\n\n\n\n\n\n\n\nCode\nlibrary(randomForest)\nclass.model &lt;- CFLRP ~ .\nrf2 &lt;- randomForest(formula = class.model, cejst_mod[,-1])",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#pre-processing",
    "href": "example/session-23-example.html#pre-processing",
    "title": "Session 23 code",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(tree)\nlibrary(randomForest)\n\n\n\n\n\n\n\nCode\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)\n\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\", quiet=TRUE) %&gt;%\n  filter(!st_is_empty(.))\n\n\n\n\n\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] FALSE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] FALSE\n\n\nCode\nfs.bdry &lt;- st_make_valid(fs.bdry)\ncflrp.bdry &lt;- st_make_valid(cflrp.bdry)\n\n\n\n\n\n\n\nCode\nst_crs(wildfire_haz) == st_crs(fs.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cflrp.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cejst)\n\n\n[1] FALSE\n\n\nCode\nfs.bdry_proj &lt;- st_transform(fs.bdry, crs = st_crs(wildfire_haz))\ncflrp.bdry_proj &lt;- st_transform(cflrp.bdry, crs = st_crs(wildfire_haz))\ncejst_proj &lt;- st_transform(cejst, crs = st_crs(wildfire_haz))\n\n\n\n\n\n\n\nCode\nfs.bdry_sub &lt;- fs.bdry_proj[cejst_proj, ]\ncflrp.bdry_sub &lt;- cflrp.bdry_proj[cejst_proj, ]\n\ncejst_sub &lt;- cejst_proj[fs.bdry_sub, ]\n\n\n\n\n\n\n\nCode\ncejst_sub &lt;- cejst_sub %&gt;%\n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\n\n\n\n\n\n\n\nCode\nwf_risk &lt;- terra::extract(wildfire_haz, cejst_sub, fun=mean)\n\ncejst_sub$WHP_ID &lt;- wf_risk$WHP_ID\n\n\n\n\n\n\n\nCode\ncflrp &lt;- apply(st_intersects(cejst_sub, cflrp.bdry_sub, sparse = FALSE), 1, any)\n\ncejst_sub$CFLRP &lt;- cflrp\n\n\n\n\n\n\n\nCode\ncejst_mod &lt;- cejst_sub %&gt;%\n  st_drop_geometry(.) %&gt;%\n  na.omit(.)\n\ncejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")] &lt;- scale(cejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")])\n\n\n\n\n\n\nCode\nlogistic.global &lt;- glm(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID,\n                       family = binomial(link = \"logit\"),\n                       data = cejst_mod)\nsummary(logistic.global)\n\n\n\nCall:\nglm(formula = CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, family = binomial(link = \"logit\"), \n    data = cejst_mod)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.66462    0.14347  -4.632 3.62e-06 ***\nLMI_PFS      0.17284    0.16881   1.024    0.306    \nLHE         -0.25551    0.15791  -1.618    0.106    \nHBF_PFS     -0.01511    0.16081  -0.094    0.925    \nWHP_ID       0.88902    0.16785   5.297 1.18e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 330.74  on 255  degrees of freedom\nResidual deviance: 290.03  on 251  degrees of freedom\nAIC: 300.03\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\nCode\nlibrary(tree)\ncejst_mod$CFLRP &lt;- as.factor(ifelse(cejst_mod$CFLRP == 1, \"Yes\", \"No\"))\ntree.model &lt;- tree(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, cejst_mod)\n\n\n\n\n\n\n\nCode\nlibrary(randomForest)\nclass.model &lt;- CFLRP ~ .\nrf2 &lt;- randomForest(formula = class.model, cejst_mod[,-1])",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#create-traintest-split",
    "href": "example/session-23-example.html#create-traintest-split",
    "title": "Session 23 code",
    "section": "Create train/test split",
    "text": "Create train/test split\n\n\nCode\n# controls which random number generator is used so that my outputs will be consistent\n# use a different one than me and see how our results differ!\nset.seed(444)\n\nlibrary(caret)\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\n# get row numbers of training data split\n# cejst_mod$CFLRP is used to ensure ~equal yes/no split\nTrain &lt;- createDataPartition(cejst_mod$CFLRP, p = 0.6, list=FALSE)\n\n# subset of our data corresponding to Train row numbers\ntraining &lt;- cejst_mod[Train, ]\n# subset of our data NOT in the vector Train\ntesting &lt;- cejst_mod[-Train, ]",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#confusion-matrices",
    "href": "example/session-23-example.html#confusion-matrices",
    "title": "Session 23 code",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\n\nLogistic regression\n\n\nCode\n# logistic regression fit with the training data\ntrain.log &lt;- glm(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID,\n                       family = binomial(link = \"logit\"),\n                       data = training)\n\n# generate predictions for the testing data\nlog.pred &lt;- predict(logistic.global, testing, type=\"response\")\n# assign predictions Yes/No based on probability\npred &lt;- as.factor(ifelse(log.pred &gt; 0.5, \n                         \"Yes\",\n                         \"No\"))\n# print confusion matrix and its metrics\nconfusionMatrix(testing$CFLRP, pred)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  61   5\n       Yes 26   9\n                                         \n               Accuracy : 0.6931         \n                 95% CI : (0.5934, 0.781)\n    No Information Rate : 0.8614         \n    P-Value [Acc &gt; NIR] : 0.999996       \n                                         \n                  Kappa : 0.2111         \n                                         \n Mcnemar's Test P-Value : 0.000328       \n                                         \n            Sensitivity : 0.7011         \n            Specificity : 0.6429         \n         Pos Pred Value : 0.9242         \n         Neg Pred Value : 0.2571         \n             Prevalence : 0.8614         \n         Detection Rate : 0.6040         \n   Detection Prevalence : 0.6535         \n      Balanced Accuracy : 0.6720         \n                                         \n       'Positive' Class : No             \n                                         \n\n\n\n\nTree Model\n\n\nCode\n# fit tree model with training data\ntrain.tree &lt;- tree(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, training)\n# generate predictions for testing data\ntree.pred &lt;- predict(train.tree, testing, type=\"class\")\n\nconfusionMatrix(testing$CFLRP, tree.pred)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  53  13\n       Yes 18  17\n                                         \n               Accuracy : 0.6931         \n                 95% CI : (0.5934, 0.781)\n    No Information Rate : 0.703          \n    P-Value [Acc &gt; NIR] : 0.6329         \n                                         \n                  Kappa : 0.2988         \n                                         \n Mcnemar's Test P-Value : 0.4725         \n                                         \n            Sensitivity : 0.7465         \n            Specificity : 0.5667         \n         Pos Pred Value : 0.8030         \n         Neg Pred Value : 0.4857         \n             Prevalence : 0.7030         \n         Detection Rate : 0.5248         \n   Detection Prevalence : 0.6535         \n      Balanced Accuracy : 0.6566         \n                                         \n       'Positive' Class : No             \n                                         \n\n\n\n\nRandom Forest\n\n\nCode\n# fit random forest with formula and training data\ntrain.rf &lt;- randomForest(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, training[,-1]) # leave out GEOID column\n# generate predictions for testing data\nrf.pred &lt;- predict(train.rf, testing, type=\"response\")\n\nconfusionMatrix(testing$CFLRP, rf.pred)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  49  17\n       Yes 17  18\n                                          \n               Accuracy : 0.6634          \n                 95% CI : (0.5625, 0.7544)\n    No Information Rate : 0.6535          \n    P-Value [Acc &gt; NIR] : 0.4626          \n                                          \n                  Kappa : 0.2567          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.7424          \n            Specificity : 0.5143          \n         Pos Pred Value : 0.7424          \n         Neg Pred Value : 0.5143          \n             Prevalence : 0.6535          \n         Detection Rate : 0.4851          \n   Detection Prevalence : 0.6535          \n      Balanced Accuracy : 0.6284          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nBased on accuracy, perhaps logistic regression or tree is best?",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#roc-auc",
    "href": "example/session-23-example.html#roc-auc",
    "title": "Session 23 code",
    "section": "ROC / AUC",
    "text": "ROC / AUC\n\n\nCode\nlibrary(pROC)\npredict.tree &lt;- predict(train.tree, newdata=testing, type=\"vector\")[,2]\npredict.rf &lt;- predict(train.rf, newdata=testing, type=\"prob\")[,2]\n\nplot(roc(testing$CFLRP, log.pred), print.auc=TRUE)\nplot(roc(testing$CFLRP, predict.tree), print.auc=TRUE, print.auc.y = 0.45, col=\"green\", add=TRUE)\nplot(roc(testing$CFLRP, predict.rf), print.auc=TRUE, print.auc.y = 0.4, col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\n\n\nBased on AUC, perhaps logistic regression or random forest is best?",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#cross-validation",
    "href": "example/session-23-example.html#cross-validation",
    "title": "Session 23 code",
    "section": "Cross Validation",
    "text": "Cross Validation\nNote that because we are folding our data, we’re using cejst_mod, not training and testing!\n\n\nCode\n# set cross validation parameters\nfitControl &lt;- trainControl(method = \"repeatedcv\", # fast resampling method but there are many\n                           number = 10,           # number of folds\n                           repeats = 10,          # number of complete sets of folds to compute\n                           classProbs = TRUE,     # compute probabilities, not just Yes/No\n                           summaryFunction = twoClassSummary)\n\n# run logistic regression with 10 folds across our entire dataset\nlog.model &lt;- train(CFLRP ~., data = cejst_mod[,-1],\n                   # automatically uses family=binomial() for binary factor\n                   method = \"glm\",\n                   trControl = fitControl)\n\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. ROC will be used instead.\n\n\nCode\n# generate predictions for testing data\npred.log &lt;- predict(log.model, newdata = testing, type=\"prob\")[,2]\n\n# run tree model with 10 folds across our entire dataset\ntree.model &lt;- train(CFLRP ~., data = cejst_mod[,-1],\n                    method = \"rpart\",\n                    trControl = fitControl)\n\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. ROC will be used instead.\n\n\nCode\npred.tree &lt;- predict(tree.model, newdata=testing, type=\"prob\")[,2]\n\n# random forest with 10 folds across our entire dataset\nrf.model &lt;- train(CFLRP ~., data = cejst_mod[,-1],\n                  method = \"rf\",\n                  trControl = fitControl)\n\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. ROC will be used instead.\n\n\nCode\npred.rf &lt;- predict(rf.model, newdata=testing, type=\"prob\")[,2]\n\nplot(roc(testing$CFLRP, pred.log), print.auc=TRUE)\nplot(roc(testing$CFLRP, pred.tree), print.auc=TRUE, print.auc.y = 0.45, col=\"green\", add=TRUE)\nplot(roc(testing$CFLRP, pred.rf), print.auc=TRUE, print.auc.y = 0.4, col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\n\n\nBased on cross validation (which is more robust than simple test/train splitting), random forest seems best.",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "example/session-23-example.html#plotting-the-best-model",
    "href": "example/session-23-example.html#plotting-the-best-model",
    "title": "Session 23 code",
    "section": "Plotting the “best” model",
    "text": "Plotting the “best” model\nGet best models (highest AUC values came from cross-validation models):\n\n\nCode\nbest.rf &lt;- rf.model$finalModel\nbest.log &lt;- log.model$finalModel\nbest.tree &lt;- tree.model$finalModel\n\n\nWe can generate predictions for the vector data or convert to raster. I’ll show both ways here for all models.\n\nPredictions and Plotting for Vector Data\n\n\nCode\nlog.preds &lt;- predict(object=best.log, newdata=cejst_mod, type=\"response\")\ntree.preds &lt;- predict(object=best.tree, newdata=cejst_mod, type=\"class\")\nrf.preds &lt;- predict(object=best.rf, newdata=cejst_mod, type=\"prob\")[,\"Yes\"]\n\n# get geometries back for plotting predictions\ncejst_pred &lt;- left_join(cejst_mod, cejst_sub[,c(\"GEOID10\")]) %&gt;%\n  st_as_sf(., crs = st_crs(cejst_sub)) %&gt;%\n  # join predictions\n  mutate(logistic = log.preds,\n         # convert to numbers for plotting\n         tree = ifelse(tree.preds==\"Yes\", 1, 0),\n         rf = rf.preds)\n\n\nJoining with `by = join_by(GEOID10)`\n\n\n\n\nCode\n# convert data to long format for mapping\nforest.cejst.long &lt;- cejst_pred %&gt;% \n  pivot_longer(., cols =logistic:rf, names_to=\"model\", values_to = \"pred\")\n\ntm_shape(forest.cejst.long) +\n  tm_fill(col=\"pred\") +\n  tm_facets(by = c(\"model\"), free.scales.fill = TRUE) +\n  tm_shape(cflrp.bdry_sub) +\n  tm_borders(col=\"black\", lwd=1.5)\n\n\n\n\n\n\n\n\n\n\n\nPredictions and Plotting for Raster Data\nmodified 11/12/24\n\n\nCode\n# using wildfire raster as a template to rasterize all other predictors\npred.stack &lt;- c((rasterize(cejst_sub, wildfire_haz, field = \"LMI_PFS\", \"mean\") - mean(cejst_sub$LMI_PFS, na.rm=TRUE))/sd(cejst_sub$LMI_PFS, na.rm=TRUE),\n                (rasterize(cejst_sub, wildfire_haz, field = \"LHE\", \"mean\") - mean(cejst_sub$LHE, na.rm=TRUE))/sd(cejst_sub$LHE, na.rm=TRUE),\n                (rasterize(cejst_sub, wildfire_haz, field = \"HBF_PFS\", \"mean\") - mean(cejst_sub$HBF_PFS, na.rm=TRUE))/sd(cejst_sub$HBF_PFS, na.rm=TRUE),\n                (wildfire_haz - mean(cejst_sub$WHP_ID, na.rm=TRUE))/sd(cejst_sub$WHP_ID, na.rm=TRUE))\nplot(pred.stack)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlog.preds_r &lt;- terra::predict(pred.stack, best.log, type=\"response\")\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\ntree.preds_r &lt;- terra::predict(pred.stack, best.tree, type=\"class\")\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\n# random forest doesn't like NAs, so we will replace them with 0 and then mask them out later\npred.stack_rf &lt;- ifel(is.na(pred.stack), 0, pred.stack)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nrf.preds_r &lt;- terra::predict(pred.stack_rf, best.rf, type=\"prob\")[[\"Yes\"]]\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nresults &lt;- c(log.preds_r, tree.preds_r, rf.preds_r)\nnames(results) &lt;- c(\"Logistic\", \"Tree\", \"Random Forest\")\nresults &lt;- mask(results, cejst_sub)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\npar(mfrow=c(1,3))\nplot(results[[1]], main=\"Logistic\")\nplot(st_geometry(cflrp.bdry_sub), add=TRUE)\nplot(results[[2]], main=\"Tree\")\nplot(st_geometry(cflrp.bdry_sub), add=TRUE)\nplot(results[[3]], main=\"Random Forest\")\nplot(st_geometry(cflrp.bdry_sub), add=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Model Comparison"
    ]
  },
  {
    "objectID": "slides/07-slides.html#objectives",
    "href": "slides/07-slides.html#objectives",
    "title": "Areal Data: Vector Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUnderstand predicates and measures in the context of spatial operations in sf\nDefine valid geometries and approaches for assessing geometries in R\nUse st_* and sf_* to evaluate attributes of geometries and calculate measurements"
  },
  {
    "objectID": "slides/07-slides.html#revisiting-simple-features",
    "href": "slides/07-slides.html#revisiting-simple-features",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\n\n\n\nThe sf package relies on a simple feature data model to represent geometries\n\nhierarchical\nstandardized methods\ncomplementary binary and human-readable encoding\n\n\n\n\n\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above"
  },
  {
    "objectID": "slides/07-slides.html#revisiting-simple-features-1",
    "href": "slides/07-slides.html#revisiting-simple-features-1",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\nYou already know how to access some elements of a simple feature\nst_crs - returns the coordinate reference system\nst_bbox - returns the bounding box for the simple feature"
  },
  {
    "objectID": "slides/07-slides.html#standaridized-methods",
    "href": "slides/07-slides.html#standaridized-methods",
    "title": "Areal Data: Vector Data",
    "section": "Standaridized Methods",
    "text": "Standaridized Methods\n\nWe can categorize sf operations based on what they return and/or how many geometries they accept as input.\n\n\n\n\n\nOutput Categories\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\n\n\n\n\n\nInput Geometries\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries",
    "href": "slides/07-slides.html#remembering-valid-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\n\nA linestring is simple if it does not intersect\n\n\n\nlibrary(sf)\nlibrary(tidyverse)\nls = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(2,1), c(3,4)))\n\nls2 = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(0,2), c(1,1), c(2,0)))"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-1",
    "href": "slides/07-slides.html#remembering-valid-geometries-1",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\nValid polygons\n\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line)\n\nFor multipolygons, adjacent polygons touch only at points\n\nDo not repeat their own path"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-2",
    "href": "slides/07-slides.html#remembering-valid-geometries-2",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np1 = st_as_sfc(\"POLYGON((0 0, 0 10, 10 0, 10 10, 0 0))\")\np2 = st_as_sfc(\"POLYGON((0 0, 0 10, 5 5,  0 0))\")\np3 = st_as_sfc(\"POLYGON((5 5, 10 10, 10 0, 5 5))\")"
  },
  {
    "objectID": "slides/07-slides.html#remembering-valid-geometries-3",
    "href": "slides/07-slides.html#remembering-valid-geometries-3",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np4 = st_as_sfc(c(\"POLYGON((0 0, 0 10, 5 5,  0 0))\", \"POLYGON((5 5, 10 10, 10 0, 5 5))\"))\nplot(p4, col=c( \"#7C4A89\", \"blue\"))"
  },
  {
    "objectID": "slides/07-slides.html#empty-geometries",
    "href": "slides/07-slides.html#empty-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/07-slides.html#using-unary-predicates",
    "href": "slides/07-slides.html#using-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Using Unary Predicates",
    "text": "Using Unary Predicates\n\nUnary predicates accept single geometries (or geometry collections)\nProvide helpful ways to check whether your data is ready to analyze\nUse the st_ prefix and return TRUE/FALSE\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis_simple\nis the geometry self-intersecting (i.e., simple)?\n\n\nis_valid\nis the geometry valid?\n\n\nis_empty\nis the geometry column of an object empty?\n\n\nis_longlat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?"
  },
  {
    "objectID": "slides/07-slides.html#checking-geometries-with-unary-predicates",
    "href": "slides/07-slides.html#checking-geometries-with-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\nBefore conducting costly analyses, it’s worth checking for:\n\n\n\nempty geometries, using any(st_is_empty(x)))\ncorrupt geometries, using any(is.na(st_is_valid(x)))\ninvalid geometries, using any(na.omit(st_is_valid(x)) == FALSE); in case of corrupt and/or invalid geometries,\nin case of invalid geometries, query the reason for invalidity by st_is_valid(x, reason = TRUE)\n\n\nInvalid geometries will require transformation (next week!)"
  },
  {
    "objectID": "slides/07-slides.html#checking-geometries-with-unary-predicates-1",
    "href": "slides/07-slides.html#checking-geometries-with-unary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\n\n\n\n\n\n\n\n\n\n\n\nst_is_simple(ls)\n\n[1] TRUE\n\nst_is_simple(ls2)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_is_valid(p1)\n\n[1] FALSE\n\nst_is_valid(p4)\n\n[1] TRUE TRUE"
  },
  {
    "objectID": "slides/07-slides.html#unary-predicates-and-real-data",
    "href": "slides/07-slides.html#unary-predicates-and-real-data",
    "title": "Areal Data: Vector Data",
    "section": "Unary Predicates and Real Data",
    "text": "Unary Predicates and Real Data\n\n\n\nlibrary(tigris)\nid.cty &lt;- counties(\"ID\", \n                   progress_bar=FALSE)\nst_crs(id.cty)$input\n\n[1] \"NAD83\"\n\nst_is_longlat(id.cty)\n\n[1] TRUE\n\nst_is_valid(id.cty)[1:5]\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nall(st_is_valid(id.cty))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/07-slides.html#binary-predicates-1",
    "href": "slides/07-slides.html#binary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\nAccept exactly two geometries (or collections)\nAlso return logical outcomes\nBased on the Dimensionally Extended 9-Intersection Model (DE-9IM)\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B AND A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\ngiven a mask pattern, return whether A and B adhere to this pattern"
  },
  {
    "objectID": "slides/07-slides.html#binary-predicates-2",
    "href": "slides/07-slides.html#binary-predicates-2",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\n\nid &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"ID\")\nor &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"OR\")\nada.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Ada\")\n\n\n\nst_covers(id, ada.cty)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `covers'\n 1: 1\n\nst_covers(id, ada.cty, sparse=FALSE)\n\n     [,1]\n[1,] TRUE\n\nst_within(ada.cty, or)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: (empty)\n\nst_within(ada.cty, or, sparse=FALSE)\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "slides/07-slides.html#measures-1",
    "href": "slides/07-slides.html#measures-1",
    "title": "Areal Data: Vector Data",
    "section": "Measures",
    "text": "Measures\nUnary Measures\n\nReturn quantities of individual geometries\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n\nUnary Measures\n\nst_area(id)\n\n2.15994e+11 [m^2]\n\nst_area(id.cty[1:5,])\n\nUnits: [m^2]\n[1] 2858212132 3380630278 1459359818 1726660462 1223521586\n\nst_dimension(id.cty[1:5,])\n\n[1] 2 2 2 2 2"
  },
  {
    "objectID": "slides/07-slides.html#binary-measures",
    "href": "slides/07-slides.html#binary-measures",
    "title": "Areal Data: Vector Data",
    "section": "Binary Measures",
    "text": "Binary Measures\n\nst_distance returns the distance between pairs of geometries\n\n\nkootenai.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Kootenai\")\nst_distance(kootenai.cty, ada.cty)\n\nUnits: [m]\n         [,1]\n[1,] 396433.8\n\nst_distance(id.cty)[1:5, 1:5]\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]      0.0 467635.7 277227.0 132998.0      0.0\n[2,] 467635.7      0.0 319706.4 656056.0 514306.9\n[3,] 277227.0 319706.4      0.0 377105.4 336146.8\n[4,] 132998.0 656056.0 377105.4      0.0 133045.5\n[5,]      0.0 514306.9 336146.8 133045.5      0.0"
  },
  {
    "objectID": "slides/07-slides.html#practice",
    "href": "slides/07-slides.html#practice",
    "title": "Areal Data: Vector Data",
    "section": "Practice!",
    "text": "Practice!\n\nCreate a vector object for Owyhee county in Idaho (hint: use filter).\nOwyhee county is in Idaho and borders Oregon. Which two predicates communicate this information? Show your code and output.\nPrint the bounding box information for Oregon and Owyhee county. Which part of this output could clue you in that they border each other?\n\nIf you finish early, try the challenge on the next slide!"
  },
  {
    "objectID": "slides/07-slides.html#challenge",
    "href": "slides/07-slides.html#challenge",
    "title": "Areal Data: Vector Data",
    "section": "Challenge",
    "text": "Challenge\nWhich two counties in Idaho are furthest from each other? You will need to use spatial measures from this class as well as data navigation methods that you may need to look up!"
  },
  {
    "objectID": "slides/06-slides.html#objectives",
    "href": "slides/06-slides.html#objectives",
    "title": "Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the linkage between location, coordinates, coordinate reference systems, and geometry\nAccess and manipulate geometries in R with sf (and terra)\nDefine geometry in the context of vector objects and troubleshoot common problems\nChange the CRS for vectors and rasters (and understand the implications)"
  },
  {
    "objectID": "slides/06-slides.html#getting-more-acquainted-with-r",
    "href": "slides/06-slides.html#getting-more-acquainted-with-r",
    "title": "Coordinates and Geometries",
    "section": "Getting more acquainted with R",
    "text": "Getting more acquainted with R\n\nObjects, classes, functions, oh my…\nIntuition for the tidyverse\nGetting used to pipes (%&gt;% or |&gt;)\nLearning to prototype"
  },
  {
    "objectID": "slides/06-slides.html#kinds-of-errors",
    "href": "slides/06-slides.html#kinds-of-errors",
    "title": "Coordinates and Geometries",
    "section": "2 Kinds of Errors",
    "text": "2 Kinds of Errors\n\nSyntax Errors: Your code won’t actually run\nSemantic Errors: Your code runs without error, but the result is unexpected"
  },
  {
    "objectID": "slides/06-slides.html#asking-better-questions-getting-better-answers",
    "href": "slides/06-slides.html#asking-better-questions-getting-better-answers",
    "title": "Coordinates and Geometries",
    "section": "Asking better questions, getting better answers",
    "text": "Asking better questions, getting better answers\n\nPlaces to get help (Google, Slack, Stack Overflow, Github Issue pages)\nWhat are you trying to do? (the outcome you want/expect)\nWhat isn’t working? (the code and steps you’ve tried so far)\nWhy aren’t common solutions working? (proof that you’ve done your due diligence)"
  },
  {
    "objectID": "slides/06-slides.html#reproducible-examples",
    "href": "slides/06-slides.html#reproducible-examples",
    "title": "Coordinates and Geometries",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\nDon’t require someone to have your data or your computer\nMinimal amount of information and code to reproduce your error\nIncludes both code and your operating environment info\nMore info\nAn example with spatial data"
  },
  {
    "objectID": "slides/06-slides.html#reference-systems",
    "href": "slides/06-slides.html#reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Reference Systems",
    "text": "Reference Systems\n\nTo locate an object or quantity, we need:\n\nA fixed origin (or datum) to measure distances to/from\nA measurement unit (or scale) that defines the units of distance\nDatum + scale = reference system"
  },
  {
    "objectID": "slides/06-slides.html#coordinate-reference-systems",
    "href": "slides/06-slides.html#coordinate-reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nMap the location on an object to earth (geodetic) or flat (projected) surfaces\nCoordinate System - the mathematical rules that specify how coordinates are assigned to points\nDatum - the parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system\nCoordinate Reference Systems - a coordinate system that is related to an object by a datum"
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r",
    "href": "slides/06-slides.html#accessing-crs-with-r",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nsf::st_crs() for vector data\nterra::crs() for raster data\nstored in WKT, epsg, or proj4string (deprecated)\nThe EPSG website is a great reference for getting projection info"
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r-1",
    "href": "slides/06-slides.html#accessing-crs-with-r-1",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\ndir.for.files &lt;- \"C:/Users/carolynkoehn/Documents/HES505_Fall_2024/data/2023/assignment01/\"\nvector.data &lt;- sf::st_read(dsn = paste0(dir.for.files, \"cejst_nw.shp\"), quiet=TRUE)\nsf::st_crs(x = vector.data)$input\n\n[1] \"WGS 84\"\n\nsf::st_crs(x = vector.data)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nsf::st_crs(x = vector.data)$wkt\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    ID[\\\"EPSG\\\",4326]]\""
  },
  {
    "objectID": "slides/06-slides.html#accessing-crs-with-r-2",
    "href": "slides/06-slides.html#accessing-crs-with-r-2",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nraster.data &lt;- terra::rast(x = paste0(dir.for.files, \"wildfire_hazard_agg.tif\"))\nterra::crs(raster.data, describe=TRUE, proj=TRUE)\n\n     name authority code area         extent\n1 unnamed      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n                                                                                                 proj\n1 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"
  },
  {
    "objectID": "slides/06-slides.html#what-if-you-dont-know-the-crs",
    "href": "slides/06-slides.html#what-if-you-dont-know-the-crs",
    "title": "Coordinates and Geometries",
    "section": "What if you don’t know the CRS?",
    "text": "What if you don’t know the CRS?\n\n\nSometimes you receive data that is missing the projection\nYou can assign it (with caution)\nYou can guess it using crsuggest::guess_crs()\n\n\n\nlibrary(sf)\nlibrary(mapview)\nlocations &lt;- data.frame(\n  X = c(1200822.97857801, 1205015.51644983, 1202297.44383987, 1205877.68696743, \n        1194763.21511923, 1195463.42403192, 1199836.01037452, 1207081.96500368, \n        1201924.15986897),\n  Y = c(1246476.31475063, 1248612.72571423, 1241479.45996392, 1243898.58428024, \n        1246033.7550009, 1241827.7730307, 1234691.50899912, 1251125.67808482, \n        1252188.4333016),\n  id = 1:9\n)\n\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"X\", \"Y\"))"
  },
  {
    "objectID": "slides/06-slides.html#guessing-crs",
    "href": "slides/06-slides.html#guessing-crs",
    "title": "Coordinates and Geometries",
    "section": "Guessing CRS",
    "text": "Guessing CRS\n\nlibrary(crsuggest)\nguess_crs(locations_sf,\n          target_location = c(80.270721, 13.082680),\n          n_return = 5)\n\n# A tibble: 5 × 2\n  crs_code dist_km\n  &lt;chr&gt;      &lt;dbl&gt;\n1 7785        4.13\n2 24344     806.  \n3 32644     806.  \n4 32244     806.  \n5 32444     806.  \n\nst_crs(locations_sf) &lt;- 7785"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs",
    "href": "slides/06-slides.html#changing-the-crs",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS",
    "text": "Changing the CRS\n\nRequires recomputing coordinates\nCoordinate Conversion - No change to the datum; lossless\nCoordinate Transformation - New datum; relies on models; some error involved"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r",
    "href": "slides/06-slides.html#changing-the-crs-in-r",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nsf::st_transform for vectors\nterra::project for rasters\nProjecting Rasters Causes Distortion"
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r-1",
    "href": "slides/06-slides.html#changing-the-crs-in-r-1",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nvector.data.proj &lt;- vector.data %&gt;%\n  sf::st_transform(., crs = 3083)\nst_crs(vector.data.proj)$input\n\n[1] \"EPSG:3083\"\n\nvector.data.proj.rast &lt;- vector.data %&gt;%\n  sf::st_transform(., crs = crs(raster.data))\nst_crs(vector.data.proj.rast)$proj4string\n\n[1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\""
  },
  {
    "objectID": "slides/06-slides.html#changing-the-crs-in-r-2",
    "href": "slides/06-slides.html#changing-the-crs-in-r-2",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nraster.data.proj &lt;- project(x = raster.data, y = \"EPSG:3083\")\ncrs(raster.data.proj, describe=TRUE, proj=TRUE)\n\n                                     name authority code\n1 NAD83 / Texas Centric Albers Equal Area      EPSG 3083\n                         area                        extent\n1 United States (USA) - Texas -106.66, -93.50, 25.83, 36.50\n                                                                                                            proj\n1 +proj=aea +lat_0=18 +lon_0=-100 +lat_1=27.5 +lat_2=35 +x_0=1500000 +y_0=6000000 +datum=NAD83 +units=m +no_defs\n\nraster.data.proj.vect &lt;- project(x = raster.data, y = vect(vector.data))\ncrs(raster.data.proj.vect, describe=TRUE, proj=TRUE)\n\n    name authority code area         extent                                proj\n1 WGS 84      EPSG 4326 &lt;NA&gt; NA, NA, NA, NA +proj=longlat +datum=WGS84 +no_defs"
  },
  {
    "objectID": "slides/06-slides.html#the-vector-data-model",
    "href": "slides/06-slides.html#the-vector-data-model",
    "title": "Coordinates and Geometries",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nCoordinates define the Vertices (i.e., discrete x-y locations) that comprise the geometry\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons"
  },
  {
    "objectID": "slides/06-slides.html#representing-vector-data-in-r",
    "href": "slides/06-slides.html#representing-vector-data-in-r",
    "title": "Coordinates and Geometries",
    "section": "Representing vector data in R",
    "text": "Representing vector data in R\n\n\n\n\n\nFrom Lovelace et al.\n\n\n\n\n\nsf hierarchy reflects increasing complexity of geometry\n\nst_point, st_linestring, st_polygon for single features\nst_multi* for multiple features of the same type\nst_geometrycollection for multiple feature types\nst_as_sfc creates the geometry list column for many sf operations"
  },
  {
    "objectID": "slides/06-slides.html#points",
    "href": "slides/06-slides.html#points",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nlibrary(sf)\nproj &lt;- st_crs('+proj=longlat +datum=WGS84')\nlong &lt;- c(-116.7, -120.4, -116.7, -113.5, -115.5, -120.8, -119.5, -113.7, -113.7, -110.7)\nlat &lt;- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9, 36.2, 39, 41.6, 36.9)\nst_multipoint(cbind(long, lat)) %&gt;% st_sfc(., crs = proj)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -120.8 ymin: 35.7 xmax: -110.7 ymax: 45.3\nGeodetic CRS:  +proj=longlat +datum=WGS84"
  },
  {
    "objectID": "slides/06-slides.html#points-1",
    "href": "slides/06-slides.html#points-1",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nplot(st_multipoint(cbind(long, lat)) %&gt;% \n                   st_sfc(., crs = proj))"
  },
  {
    "objectID": "slides/06-slides.html#lines",
    "href": "slides/06-slides.html#lines",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nlon &lt;- c(-116.8, -114.2, -112.9, -111.9, -114.2, -115.4, -117.7)\nlat &lt;- c(41.3, 42.9, 42.4, 39.8, 37.6, 38.3, 37.6)\nlonlat &lt;- cbind(lon, lat)\npts &lt;- st_multipoint(lonlat)\n\nsfline &lt;- st_multilinestring(list(pts[1:3,], pts[4:7,]))\nstr(sfline)\n\nList of 2\n $ : num [1:3, 1:2] -116.8 -114.2 -112.9 41.3 42.9 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n $ : num [1:4, 1:2] -111.9 -114.2 -115.4 -117.7 39.8 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n - attr(*, \"class\")= chr [1:3] \"XY\" \"MULTILINESTRING\" \"sfg\""
  },
  {
    "objectID": "slides/06-slides.html#lines-1",
    "href": "slides/06-slides.html#lines-1",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nplot(st_multilinestring(list(pts[1:3,], pts[4:7,])))"
  },
  {
    "objectID": "slides/06-slides.html#polygons",
    "href": "slides/06-slides.html#polygons",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\ncoords = list(outer, hole1, hole2)\npl1 = st_polygon(coords)"
  },
  {
    "objectID": "slides/06-slides.html#polygons-1",
    "href": "slides/06-slides.html#polygons-1",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nplot(pl1)"
  },
  {
    "objectID": "slides/06-slides.html#common-problems-with-vector-data",
    "href": "slides/06-slides.html#common-problems-with-vector-data",
    "title": "Coordinates and Geometries",
    "section": "Common Problems with Vector Data",
    "text": "Common Problems with Vector Data\n\n\n\n\nVectors and scale\nSlivers and overlaps\nUndershoots and overshoots\nSelf-intersections and rings\n\n\n\n\n\n\nTopology Errors - Saylor Acad.\n\n\n\n\nWe’ll use st_is_valid() to check this, but fixing can be tricky"
  },
  {
    "objectID": "slides/06-slides.html#fixing-problematic-topology",
    "href": "slides/06-slides.html#fixing-problematic-topology",
    "title": "Coordinates and Geometries",
    "section": "Fixing Problematic Topology",
    "text": "Fixing Problematic Topology\n\nst_make_valid() for simple cases\nst_buffer with dist=0\nMore complex errors need more complex approaches"
  },
  {
    "objectID": "slides/06-slides.html#a-note-on-vectors",
    "href": "slides/06-slides.html#a-note-on-vectors",
    "title": "Coordinates and Geometries",
    "section": "A Note on Vectors",
    "text": "A Note on Vectors\n\nMoving forward we will rely primarily on the sf package for vector manipulation. Some packages require objects to be a different class. terra, for example, relies on SpatVectors. You can use as() to coerce objects from one type to another (assuming a method exists). You can also explore other packages. Many packages provide access to the ‘spatial’ backbones of R (like geos and gdal), they just differ in how the “verbs” are specified. For sf operations the st_ prefix is typical. For rgeos operations, the g prefix is common."
  },
  {
    "objectID": "slides/06-slides.html#i-want-your-feedback",
    "href": "slides/06-slides.html#i-want-your-feedback",
    "title": "Coordinates and Geometries",
    "section": "I want your feedback!",
    "text": "I want your feedback!\nPlease give me feedback on my live coding approach so I can adjust for this class’s preferences!\nhttps://forms.gle/U2acJcRABiF1TCRF7"
  },
  {
    "objectID": "example/session-20-example.html",
    "href": "example/session-20-example.html",
    "title": "Session 20 code",
    "section": "",
    "text": "Load libraries:\nCode\nlibrary(sf)\nlibrary(spdep)\nlibrary(tidyverse)\nlibrary(tmap)\nLoad data:\nCode\ncdc &lt;- read_sf(\"/opt/data/data/vectorexample/cdc_nw.shp\") %&gt;% \n  select(stateabbr, countyname, countyfips, casthma_cr)\n\nplot(cdc[\"casthma_cr\"])",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Spatial Autocorrelation and Areal Data Neighbors"
    ]
  },
  {
    "objectID": "example/session-20-example.html#find-neighbors-based-on-contiguity",
    "href": "example/session-20-example.html#find-neighbors-based-on-contiguity",
    "title": "Session 20 code",
    "section": "Find neighbors based on contiguity",
    "text": "Find neighbors based on contiguity\n\n\nCode\nnb.qn &lt;- poly2nb(cdc, queen = TRUE)\nnb.rk &lt;- poly2nb(cdc, queen = FALSE)\n\n\nvisualize\n\n\nCode\npar(mfrow=c(1,2))\nplot(st_geometry(cdc), border = 'lightgrey')\nplot(nb.qn, st_coordinates(st_centroid(cdc)), add=TRUE, col='red', main=\"Queen's case\")\nplot(st_geometry(cdc), border = 'lightgrey')\nplot(nb.rk, st_coordinates(st_centroid(cdc)), add=TRUE, col='blue', main=\"Rook's case\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\n\n\n\nGet weights\n\n\nCode\nlw.qn &lt;- nb2listw(nb.qn, style=\"W\", zero.policy = TRUE)\n\nlw.qn$weights[1:5]\n\n\n[[1]]\n[1] 0.5 0.5\n\n[[2]]\n[1] 0.25 0.25 0.25 0.25\n\n[[3]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[4]]\n[1] 0.3333333 0.3333333 0.3333333\n\n[[5]]\n[1] 1\n\n\n\n\nGet distance\n\n\nCode\nasthma.lag &lt;- lag.listw(lw.qn, cdc$casthma_cr)\n\nhead(asthma.lag)\n\n\n[1] 10.30000  9.57500  9.88000 10.26667  9.50000  9.90000\n\n\n\n\nGet Moran’s I\n\n\nCode\nM &lt;- lm(asthma.lag ~ cdc$casthma_cr)\n\nM\n\n\n\nCall:\nlm(formula = asthma.lag ~ cdc$casthma_cr)\n\nCoefficients:\n   (Intercept)  cdc$casthma_cr  \n        3.7209          0.6357  \n\n\n\n\nCode\nplot(x = cdc$casthma_cr, y = asthma.lag)\nabline(M$coefficients[1], M$coefficients[2], col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\nCompare to null hypothesis\n\n\nCode\nn &lt;- 400L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle asthma values\n  x &lt;- sample(cdc$casthma_cr, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag &lt;- lag.listw(lw.qn, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}\n\n\n\n\nMoran test code\nmoran.test uses a few key assumptions, including that the data is normally distributed\n\n\nCode\nmoran.test(cdc$casthma_cr, lw.qn)\n\n\n\n    Moran I test under randomisation\n\ndata:  cdc$casthma_cr  \nweights: lw.qn  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 40.826, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6381428057     -0.0005037783      0.0002447034",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Spatial Autocorrelation and Areal Data Neighbors"
    ]
  },
  {
    "objectID": "example/session-20-example.html#find-neighbors-based-on-distance",
    "href": "example/session-20-example.html#find-neighbors-based-on-distance",
    "title": "Session 20 code",
    "section": "Find neighbors based on distance",
    "text": "Find neighbors based on distance\n\n\nCode\ncdc.pt &lt;- st_point_on_surface(cdc)\n\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\n\nWarning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\nCode\n# another option is st_centroid\n\n# get nearest neighbor for each point\ngeog.nearnb &lt;- knn2nb(knearneigh(cdc.pt, k=1))\n\n# get list of nearest neighbors so that every tract has at least one neighbor\nnb.nearest &lt;- dnearneigh(cdc.pt,\n                         d1 = 0,\n                         d2 = max(unlist(nbdists(geog.nearnb, cdc.pt))))\n\n\n\nGet weights and distance\n\n\nCode\nlw.nearest &lt;- nb2listw(nb.nearest, style=\"W\")\nasthma.lag &lt;- lag.listw(lw.nearest, cdc$casthma_cr)\n\n\n\n\nCalculate Moran’s I\n\n\nCode\nM2 &lt;- lm(asthma.lag ~ cdc$casthma_cr)\n\nM2\n\n\n\nCall:\nlm(formula = asthma.lag ~ cdc$casthma_cr)\n\nCoefficients:\n   (Intercept)  cdc$casthma_cr  \n        8.8547          0.1578  \n\n\nvisualize:\n\n\nCode\nplot(x = cdc$casthma_cr, y = asthma.lag)\nabline(M2$coefficients[1], M2$coefficients[2], col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\nSimulate data under null hypothesis\n\n\nCode\nn &lt;- 400L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle asthma values\n  x &lt;- sample(cdc$casthma_cr, replace=FALSE)\n  # Compute new set of lagged values - use new neighbors!\n  x.lag &lt;- lag.listw(lw.nearest, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}\n\n\n\nCloser look at simulation\n\n\nCode\ni &lt;- 1\nI.r &lt;- vector(length=n)  # Create an empty vector\n\n# Randomly shuffle asthma values\nx &lt;- sample(cdc$casthma_cr, replace=FALSE)\n\nrandom_data &lt;- cbind(cdc, x)\n\nplot(random_data[\"x\"])\n\n\n\n\n\n\n\n\n\nCode\n# Compute new set of lagged values\nx.lag &lt;- lag.listw(lw.nearest, x)\n\nhead(x.lag)\n\n\n[1] 10.61818 10.50821 10.51805 10.48258 10.45769 10.51111\n\n\nCode\n# Compute the regression slope and store its value\nM.r    &lt;- lm(x.lag ~ x)\nI.r[i] &lt;- coef(M.r)[2]\n  \nhead(I.r)\n\n\n[1] -0.002479055  0.000000000  0.000000000  0.000000000  0.000000000\n[6]  0.000000000",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Spatial Autocorrelation and Areal Data Neighbors"
    ]
  },
  {
    "objectID": "example/session-20-example.html#do-it-with-new-data",
    "href": "example/session-20-example.html#do-it-with-new-data",
    "title": "Session 20 code",
    "section": "Do it with new data",
    "text": "Do it with new data\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\ncejst.id &lt;- cejst %&gt;%\n  filter(SF == \"Idaho\") %&gt;%\n  select(CF, SF, EPLR_PFS)\n\n\nAfter this, we tried our hand at tackling the homework questions with the cejst data individually.\nUse the nearest-neighbor approach that we used in class to estimate the lagged values for the cejst dataset and estimate the slope of the line describing Moran’s I statistic.\nNow use the permutation approach to compare your measured value to one generated from multiple simulations. Generate the plot of the data. Do you see more evidence of spatial autocorrelation?",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Spatial Autocorrelation and Areal Data Neighbors"
    ]
  },
  {
    "objectID": "assignment/03-vectorsolutions.html",
    "href": "assignment/03-vectorsolutions.html",
    "title": "Assignment 3 Solutions: Coordinates and Geometries",
    "section": "",
    "text": "1. Write out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\n\nThis one is probably a little tricky if you haven’t taken the time to check out the attributes of the data (which you should always do). That said, some pretty generic steps would be:\n\n\n1. Load each dataset\n2. Check geometry validity\n3. Align CRS\n4. Run Correlation\n5. Print Results\n\n\nThere are two key steps here, that you’ll repeat for any/all spatial analyses that you do: 1) checking for valid geometries and 2) making sure the data are aligned in a sensible CRS. I can add a code chunk for each now.\n\n\n1. Load each dataset\n\n\n2. Check geometry validity\n\n\n3. Align CRS\n\n\n4. Run Correlation\n\n\n5. Print Results\n\n2. Read in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\n\nHere I’m going to combine the load portion of my pseudocode with the validity since I can do that without creating additional object. I use the str() function to get a sense for what the data looks like and to understand what data classes I’m working with. Then, I use the all() function to make sure that all of the results of st_is_valid() are true. I don’t need to do that with the raster file as the geometry is implicit which means that it has to be topologically valid (this doesn’t mean that the numbers are accurate, it just means that the dataset conforms to the data model R expects). Then I’ll add another code to check the CRS of the different objects.\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\ncdc.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/cdc_nw.shp\")\nstr(cdc.nw)\nall(st_is_valid(cdc.nw))\npm.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/pm_nw.shp\")\nstr(pm.nw)\nall(st_is_valid(pm.nw))\n\nwildfire.haz &lt;- rast(\"data/opt/data/2023/assignment03/wildfire_hazard_agg.tif\")\nstr(wildfire.haz)\n\n\nNow that I’ve gotten the data into my environment, I need to make sure that the CRS are aligned. I’ll demonstrate that with a few different approaches. You can use the logical == or the identical function to check, but remember that these functions are not specific to spatial objects, they evaluate things very literally. So even if the CRS is the same, if st_crs returns the CRS in one format (WKT) and crs returns it in another, you’ll get FALSE even if they are actually the same CRS - pay attention to that. You’ll notice that they aren’t identical; we’ll deal with that in the next question.\n\n\nst_crs(cdc.nw)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nst_crs(pm.nw)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\ncrs(wildfire.haz)\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nidentical(st_crs(cdc.nw), st_crs(pm.nw))\n\n[1] FALSE\n\nst_crs(cdc.nw) == st_crs(pm.nw)\n\n[1] FALSE\n\n\n3. Re-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tfi file. Verify that all the files have the same projection.\n\nNow we’ll use st_transform to get the two shapefiles aligned with the raster (because we generally want to avoid projecting rasters if we can). We can then use the same steps above to see if they’re aligned. Note that I’m using the terra::crs() function to make sure that the output is printed in exactly the same format\n\n\ncdc.nw.proj &lt;- cdc.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\npm.nw.proj &lt;- pm.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\n\nidentical(crs(cdc.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\nidentical(crs(pm.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\n\n4. How does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\n\nNow we just want to look at the bounding box of the data before and after it was projected. We can do this using st_bbox. One of the most obvious changes is that the units for cdc.nw have changed from degrees to meters (as evidenced by the much larger numbers). For the pm.nw object we can see that the raw coordinates indicate a shift to the west; however, because the origin for this crs has also changed, the states still show up in the correct place.\n\n\nst_bbox(cdc.nw)\n\n      xmin       ymin       xmax       ymax \n-124.74918   41.98818 -111.04349   49.00232 \n\nst_bbox(cdc.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2295337  2208890 -1189292  3177425 \n\nst_bbox(pm.nw)\n\n     xmin      ymin      xmax      ymax \n-13898126   5159210 -12361307   6275276 \n\nst_bbox(pm.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2300791  2208891 -1189293  3177426 \n\n\n5. What class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\nThis one was probably a little tricky. First, to check the geometry type, we use st_geometry_type setting by_geometry to FALSE means we get the geometry type for the entire object instead of each observation. We then use a series of filter commands to get the records from Idaho and Ada county. Once we’ve narrowed the data to our correct region, we can filter again to find the row with the minimum value of PM25 (note that we have to set na.rm=TRUE so that we ignore the NA values). Then we just take the number of rows (nrow) of the result of st_coordinates to get the number of coordinates associated with that geometry.\n\n\nst_geometry_type(pm.nw, by_geometry = FALSE)\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nada.pm &lt;- pm.nw %&gt;%\n  filter(STATE_NAME==\"Idaho\" & CNTY_NAME==\"Ada\") %&gt;%\n  filter(PM25 == min(PM25, na.rm = TRUE))\n\nada.pm\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -12935300 ymin: 5329192 xmax: -12910260 ymax: 5402433\nProjected CRS: WGS 84 / Pseudo-Mercator\n# A tibble: 1 × 4\n  STATE_NAME CNTY_NAME  PM25                                            geometry\n* &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;                                  &lt;MULTIPOLYGON [m]&gt;\n1 Idaho      Ada        6.68 (((-12935301 5391002, -12934885 5391290, -12934526…\n\nnrow(st_coordinates(ada.pm))\n\n[1] 1394"
  },
  {
    "objectID": "example/session-14-example.html",
    "href": "example/session-14-example.html",
    "title": "Session 14 code",
    "section": "",
    "text": "Libraries for today:\nCode\nlibrary(tidyverse, quietly = TRUE)\nlibrary(spData)\nlibrary(sf)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#subsetting-data",
    "href": "example/session-14-example.html#subsetting-data",
    "title": "Session 14 code",
    "section": "Subsetting Data:",
    "text": "Subsetting Data:\n\n\nCode\ncolnames(world)\n\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\n\n\nCode\nhead(world)[,1:3] %&gt;% \n  st_drop_geometry()\n\n\n# A tibble: 6 × 3\n  iso_a2 name_long      continent    \n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;        \n1 FJ     Fiji           Oceania      \n2 TZ     Tanzania       Africa       \n3 EH     Western Sahara Africa       \n4 CA     Canada         North America\n5 US     United States  North America\n6 KZ     Kazakhstan     Asia         \n\n\n\n\nCode\nworld %&gt;%\n  dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.) \n\n\n# A tibble: 6 × 2\n  name_long      continent    \n  &lt;chr&gt;          &lt;chr&gt;        \n1 Fiji           Oceania      \n2 Tanzania       Africa       \n3 Western Sahara Africa       \n4 Canada         North America\n5 United States  North America\n6 Kazakhstan     Asia         \n\n\n\n\nCode\nhead(world)[1:3, 1:3] %&gt;% \n  st_drop_geometry()\n\n\n# A tibble: 3 × 3\n  iso_a2 name_long      continent\n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;    \n1 FJ     Fiji           Oceania  \n2 TZ     Tanzania       Africa   \n3 EH     Western Sahara Africa   \n\n\n\n\nCode\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n  select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n\n# A tibble: 6 × 2\n  name_long   continent\n  &lt;chr&gt;       &lt;chr&gt;    \n1 Kazakhstan  Asia     \n2 Uzbekistan  Asia     \n3 Indonesia   Asia     \n4 Timor-Leste Asia     \n5 Israel      Asia     \n6 Lebanon     Asia",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#create-new-columns",
    "href": "example/session-14-example.html#create-new-columns",
    "title": "Session 14 code",
    "section": "Create new columns",
    "text": "Create new columns\n\n\nCode\nworld_dens &lt;- world %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n  select(name_long, continent, pop, gdpPercap ,area_km2) %&gt;%\n  mutate(., dens = pop/area_km2,\n         totGDP = gdpPercap * pop) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#aggregate-summarise",
    "href": "example/session-14-example.html#aggregate-summarise",
    "title": "Session 14 code",
    "section": "Aggregate / Summarise",
    "text": "Aggregate / Summarise\n\n\nCode\nworld %&gt;%\n  st_drop_geometry(.) %&gt;% \n  group_by(continent) %&gt;%\n  summarize(pop = sum(pop, na.rm = TRUE))\n\n\n# A tibble: 8 × 2\n  continent                      pop\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#joins",
    "href": "example/session-14-example.html#joins",
    "title": "Session 14 code",
    "section": "Joins",
    "text": "Joins\n\n\nCode\nhead(coffee_data)\n\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\n\n\nCode\nworld_coffee = left_join(world, coffee_data)\n\n\nJoining with `by = join_by(name_long)`\n\n\nCode\nnrow(world_coffee)\n\n\n[1] 177\n\n\n\n\nCode\nplot(world_coffee[\"coffee_production_2016\"])\n\n\n\n\n\n\n\n\n\n\n\nCode\nworld_coffee_inner = inner_join(world, coffee_data)\n\n\nJoining with `by = join_by(name_long)`\n\n\nCode\nnrow(world_coffee_inner)\n\n\n[1] 45",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-14-example.html#practice",
    "href": "example/session-14-example.html#practice",
    "title": "Session 14 code",
    "section": "Practice:",
    "text": "Practice:\nWhat is the population density for the tracts in the cejst data? Our data sources are:\n\nTotal population in each tract (cejst$TPF)\nArea in \\(m^2\\) of each tract (tigris::tracts(), column ALAND)\n\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment06/cejst_pnw.shp\")\n\nid_tracts &lt;- tigris::tracts(state = \"ID\", year = 2015)\n\n\n\n\nCode\ncejst_id &lt;- cejst %&gt;%\n  filter(SF == \"Idaho\")\n\n\n\n\nCode\nid_tracts &lt;- id_tracts %&gt;%\n  mutate(ALAND_sqmi = ALAND/2589988.11)\nhead(id_tracts)\n\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -117.0628 ymin: 41.99601 xmax: -111.5077 ymax: 46.39597\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE       GEOID    NAME             NAMELSAD MTFCC\n1      16      041  970200 16041970200    9702    Census Tract 9702 G5020\n2      16      041  970100 16041970100    9701    Census Tract 9701 G5020\n3      16      073  950200 16073950200    9502    Census Tract 9502 G5020\n4      16      073  950101 16073950101 9501.01 Census Tract 9501.01 G5020\n5      16      073  950102 16073950102 9501.02 Census Tract 9501.02 G5020\n6      16      069  960700 16069960700    9607    Census Tract 9607 G5020\n  FUNCSTAT       ALAND   AWATER    INTPTLAT     INTPTLON\n1        S   455342589  2412885 +42.0609834 -111.7147361\n2        S  1263363258  9752226 +42.2231666 -111.8485407\n3        S 19603341439 77025612 +42.5728508 -116.1896903\n4        S   117363851  1585581 +43.5924261 -116.9602208\n5        S   132949223  2844915 +43.5247849 -116.8481343\n6        S   770216313  9225885 +46.0956517 -116.8989005\n                        geometry ALAND_sqmi\n1 POLYGON ((-111.935 42.00164...  175.80876\n2 POLYGON ((-112.1263 42.2853...  487.78728\n3 POLYGON ((-117.027 43.54418... 7568.89245\n4 POLYGON ((-117.0268 43.6465...   45.31444\n5 POLYGON ((-116.9284 43.5437...   51.33198\n6 POLYGON ((-117.0628 46.3652...  297.38218\n\n\n\n\nCode\nid_tracts &lt;- st_drop_geometry(id_tracts)\n\n\n\n\nCode\ncejst_id_join &lt;- inner_join(cejst_id, id_tracts,\n                            by = c(\"GEOID10\" = \"GEOID\")) %&gt;%\n  mutate(pop_dens = TPF/ALAND_sqmi)\n\n\n\n\nCode\nplot(cejst_id_join[\"pop_dens\"])\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\nCode\ntm_shape(cejst_id_join) +\n  tm_polygons(col = \"pop_dens\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(cejst_id_join, aes(x=pop_dens, y=IS_PFS)) +\n  geom_point()",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Attributes"
    ]
  },
  {
    "objectID": "example/session-07-example.html",
    "href": "example/session-07-example.html",
    "title": "Session 7 Live Code",
    "section": "",
    "text": "Read in the libraries we need\n\n\nCode\nlibrary(sf)\nlibrary(tigris)\n\n\n\n\nGet a sf object of ID counties (from the tigris package)\n\n\nCode\nid.cty &lt;- counties(state = \"ID\")\n\n\n\n\nCheck CRS of object\n\n\nCode\nst_crs(id.cty)$input\n\n\n[1] \"NAD83\"\n\n\n\n\nUnary predicates\n\n\nCode\nst_is_longlat(id.cty)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(id.cty)[1:5]\n\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n\nCode\nall(st_is_valid(id.cty))\n\n\n[1] TRUE\n\n\n\n\nGet some data for binary operations\n\n\nCode\nlibrary(tidyverse)\n\nid &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"ID\")\nor &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"OR\")\nada.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Ada\")\n\n\n\n\nTry some predicates\n\n\nCode\nst_covers(id, ada.cty)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `covers'\n 1: 1\n\n\nCode\nst_covers(id, ada.cty, sparse = FALSE)\n\n\n     [,1]\n[1,] TRUE\n\n\n\n\nCode\nst_within(ada.cty, or)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: (empty)\n\n\nCode\nst_within(ada.cty, or, sparse=FALSE)\n\n\n      [,1]\n[1,] FALSE\n\n\n\n\nUnary measures\n\n\nCode\nst_area(id)\n\n\n2.15994e+11 [m^2]\n\n\nCode\nst_area(id.cty)[1:5]\n\n\nUnits: [m^2]\n[1] 2858212132 3380630278 1459359818 1726660462 1223521586\n\n\n\n\nCode\nst_dimension(id.cty)[1:5]\n\n\n[1] 2 2 2 2 2\n\n\n\n\nBinary measure (distance)\n\n\nCode\nkootenai.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Kootenai\")\nst_distance(kootenai.cty, ada.cty)\n\n\nUnits: [m]\n         [,1]\n[1,] 396433.8\n\n\n\n\nCode\nst_distance(id.cty)[1:5, 1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]      0.0 467635.7 277227.0 132998.0      0.0\n[2,] 467635.7      0.0 319706.4 656056.0 514306.9\n[3,] 277227.0 319706.4      0.0 377105.4 336146.8\n[4,] 132998.0 656056.0 377105.4      0.0 133045.5\n[5,]      0.0 514306.9 336146.8 133045.5      0.0\n\n\n\n\nPractice exercise code:\n\n\nCode\n# Part 1\nowyhee.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Owyhee\")\n\n# Part 2\nst_within(owyhee.cty, id)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: 1\n\n\nCode\nst_touches(owyhee.cty, or)\n\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `touches'\n 1: 1\n\n\nCode\n# Part 3\nst_bbox(owyhee.cty)\n\n\n      xmin       ymin       xmax       ymax \n-117.02701   41.99612 -115.03751   43.68080 \n\n\nCode\nst_bbox(or)\n\n\n      xmin       ymin       xmax       ymax \n-124.70354   41.99208 -116.46326   46.29910 \n\n\n\n\nChallenge code:\n\n\nCode\n# Calculate distances between all counties\ncty.dist &lt;- st_distance(id.cty)\n\n# Label rows and columns of matrix\ncolnames(cty.dist) &lt;- id.cty$NAME\nrownames(cty.dist) &lt;- id.cty$NAME\n\n# Find where the maximum value is\nwhich(cty.dist == max(cty.dist), arr.ind = TRUE)\n\n\n         row col\nBoundary  40   4\nFranklin   4  40\n\n\nCode\n# Locate the counties at the row numbers returned by which\n# Needed if you don't label the rows and columns\nid.cty$NAME[c(4, 40)]\n\n\n[1] \"Franklin\" \"Boundary\"\n\n\n\n\nWhiteboard notes",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "slides/18-slides.html#patterns-as-realizations-of-spatial-processes",
    "href": "slides/18-slides.html#patterns-as-realizations-of-spatial-processes",
    "title": "Interpolation",
    "section": "Patterns as realizations of spatial processes",
    "text": "Patterns as realizations of spatial processes\n\nA spatial process is a description of how a spatial pattern might be generated\nGenerative models\nAn observed pattern as a possible realization of an hypothesized process\n\n\n\nBiometry/experiments – the way you collect your data gives you some right to say something about how the broader universe works\nGeostatistical – model the process that gives rise to the data or the spatial variation underlying the data to expand from our observations to a continuous field\nFrequentist vs generative:\n\nfreqentist = there is a true model where many thousands of datasets may be collected from and our model tries to capture the true model\ngenerative = we aren’t privy to the underlying process, our data is a random function of the process. We can’t know everything, so does the model we chose give plausible estimates based on our data and model estimates."
  },
  {
    "objectID": "slides/18-slides.html#deterministic-vs.-stochastic-processes",
    "href": "slides/18-slides.html#deterministic-vs.-stochastic-processes",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\nDeterministic processes: always produce the same outcome\n\n\\[\nz = 2x + 3y\n\\]\n\nResults in a spatially continuous field\n\n\nExample: old population ecology models: if you know the carrying capacity and rate of growth, you can predict how many animals there will be without error"
  },
  {
    "objectID": "slides/18-slides.html#deterministic-vs.-stochastic-processes-1",
    "href": "slides/18-slides.html#deterministic-vs.-stochastic-processes-1",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nz &lt;- x\nvalues(z) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2]"
  },
  {
    "objectID": "slides/18-slides.html#deterministic-vs.-stochastic-processes-2",
    "href": "slides/18-slides.html#deterministic-vs.-stochastic-processes-2",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\n\n\n\nStochastic processes: variation makes each realization difficult to predict\n\n\\[\nz = 2x + 3y + d\n\\]\n\nThe process is random, not the result (!!)\nMeasurement error makes deterministic processes appear stochastic\n\n\n\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nfun &lt;- function(z){\na &lt;- z\nd &lt;- runif(ncell(z), -50,50)\nvalues(a) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2] + d\nreturn(a)\n}\n\nb &lt;- replicate(n=6, fun(z=x), simplify=FALSE)\nd &lt;- do.call(c, b)\n\n\n\n\nStochasticity can come from measurement error or un-modeled terms\nImportance of simulation for model testing"
  },
  {
    "objectID": "slides/18-slides.html#deterministic-vs.-stochastic-processes-3",
    "href": "slides/18-slides.html#deterministic-vs.-stochastic-processes-3",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\n\nSix realizations of the same process\nWhat do we care about? (1) We acre a lot about d, the stochastic term in the model. (2) Multiple difference versions give you very different outcomes. You have to ask how often you would come up with the data in your hand"
  },
  {
    "objectID": "slides/18-slides.html#expected-values-and-hypothesis-testing",
    "href": "slides/18-slides.html#expected-values-and-hypothesis-testing",
    "title": "Interpolation",
    "section": "Expected values and hypothesis testing",
    "text": "Expected values and hypothesis testing\n\n\n\n\nConsidering each outcome as the realization of a process allows us to generate expected values\nThe simplest spatial process is Completely Spatial Random (CSR) process\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/18-slides.html#generating-expactations-for-csr",
    "href": "slides/18-slides.html#generating-expactations-for-csr",
    "title": "Interpolation",
    "section": "Generating expactations for CSR",
    "text": "Generating expactations for CSR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use quadrat counts to estimate the expected number of events in a given area\nThe probability of each possible count is given by:\n\n\\[\nP(n,k) = {n \\choose x}p^k(1-p)^{n-k}\n\\]\n\nGiven total coverage of quadrats, then \\(p=\\frac{\\frac{a}{x}}{a}\\) and\n\n\\[\n\\begin{equation}\nP(k,n,x) = {n \\choose k}\\bigg(\\frac{1}{x}\\bigg)^k\\bigg(\\frac{x-1}{x}\\bigg)^{n-k}\n\\end{equation}\n\\]\n\n\n\nSecond equation is a weighted average given size of quadrats"
  },
  {
    "objectID": "slides/18-slides.html#revisiting-ripleys-k",
    "href": "slides/18-slides.html#revisiting-ripleys-k",
    "title": "Interpolation",
    "section": "Revisiting Ripley’s \\(K\\)",
    "text": "Revisiting Ripley’s \\(K\\)\n\n\nProbability is tied to quadrat size – we can improve estimates with a moving window\nIf points have independent, fixed marginal densities, then they exhibit complete, spatial randomness (CSR)\nThe K function is an alternative, based on a series of circles with increasing radius\n\n\\[\n\\begin{equation}\nK(d) = \\lambda^{-1}E(N_d)\n\\end{equation}\n\\]\n\nWe can test for clustering by comparing to the expectation:\n\n\\[\n\\begin{equation}\nK_{CSR}(d) = \\pi d^2\n\\end{equation}\n\\]\n\nif \\(k(d) &gt; K_{CSR}(d)\\) then there is clustering at the scale defined by \\(d\\)\n\n\n\nCarpet example for various scales of randomness K is the rate of the point process, which we may not entirely know, times how many points in a given distance"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function",
    "href": "slides/18-slides.html#ripleys-k-function",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nWhen working with a sample the distribution of \\(K\\) is unknown\nEstimate with\n\n\\[\n\\begin{equation}\n\\hat{K}(d) = \\hat{\\lambda}^{-1}\\sum_{i=1}^n\\sum_{j=1}^n\\frac{I(d_{ij} &lt;d)}{n(n-1)}\n\\end{equation}\n\\]\nwhere:\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{n}{|A|}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-1",
    "href": "slides/18-slides.html#ripleys-k-function-1",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\nUsing the spatstat package"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-2",
    "href": "slides/18-slides.html#ripleys-k-function-2",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nkf &lt;- Kest(bramblecanes, correction-\"border\")\nplot(kf)"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-3",
    "href": "slides/18-slides.html#ripleys-k-function-3",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\naccounting for variation in \\(d\\)\n\n\nkf.env &lt;- envelope(bramblecanes, correction=\"border\", envelope = FALSE, verbose = FALSE)\nplot(kf.env)"
  },
  {
    "objectID": "slides/18-slides.html#other-functions",
    "href": "slides/18-slides.html#other-functions",
    "title": "Interpolation",
    "section": "Other functions",
    "text": "Other functions\n\n\n\n\\(L\\) function: square root transformation of \\(K\\)\n\\(G\\) function: the cumulative frequency distribution of the nearest neighbor distances\n\\(F\\) function: similar to \\(G\\) but based on randomly located points"
  },
  {
    "objectID": "slides/18-slides.html#spatial-autocorrelation",
    "href": "slides/18-slides.html#spatial-autocorrelation",
    "title": "Interpolation",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nFrom Manuel Gimond\nMoving from point patterns to areal data"
  },
  {
    "objectID": "slides/18-slides.html#one-measure-of-autocorrelation",
    "href": "slides/18-slides.html#one-measure-of-autocorrelation",
    "title": "Interpolation",
    "section": "(One) Measure of autocorrelation",
    "text": "(One) Measure of autocorrelation\n\n\n\nMoran’s I\n\n\n\n\n\n\nDifference between x at i and mean for x, x at j and mean for j, times spatial weights matrix (how many nearest neighbors to consider)"
  },
  {
    "objectID": "slides/18-slides.html#morans-i-an-example",
    "href": "slides/18-slides.html#morans-i-an-example",
    "title": "Interpolation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\n\nUse spdep package\nEstimate neighbors\nGenerate weighted average\n\n\nset.seed(2354)\n# Load the shapefile\ns &lt;- readRDS(url(\"https://github.com/mgimond/Data/raw/gh-pages/Exercises/fl_hr80.rds\"))\n\n# Define the neighbors (use queen case)\nnb &lt;- poly2nb(s, queen=TRUE)\n\n# Compute the neighboring average homicide rates\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n#estimate Moran's I\nmoran.test(s$HR80,lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  s$HR80  \nweights: lw    \n\nMoran I statistic standard deviate = 1.8891, p-value = 0.02944\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.136277593      -0.015151515       0.006425761 \n\n\n\n\n\n\n\nJust getting a Moran’s I value for housing prices in Florida in 1980"
  },
  {
    "objectID": "slides/18-slides.html#morans-i-an-example-1",
    "href": "slides/18-slides.html#morans-i-an-example-1",
    "title": "Interpolation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\nM1 &lt;- moran.mc(s$HR80, lw, nsim=9999, alternative=\"greater\")\n\n\n\n# Display the resulting statistics\nM1\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$HR80 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.13628, observed rank = 9575, p-value = 0.0425\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulations under CSR"
  },
  {
    "objectID": "slides/18-slides.html#the-challenge-of-areal-data",
    "href": "slides/18-slides.html#the-challenge-of-areal-data",
    "title": "Interpolation",
    "section": "The challenge of areal data",
    "text": "The challenge of areal data\n\nSpatial autocorrelation threatens second order randomness\nAreal data means an infinite number of potential distances\nNeighbor matrices, \\(\\boldsymbol W\\), allow different characterizations\n\n\nSimple detection of autocorrelation, level of a linear regression"
  },
  {
    "objectID": "slides/18-slides.html#interpolation-1",
    "href": "slides/18-slides.html#interpolation-1",
    "title": "Interpolation",
    "section": "Interpolation",
    "text": "Interpolation\n\nGoal: estimate the value of \\(z\\) at new points in \\(\\mathbf{x_i}\\)\nMost useful for continuous values\nNearest-neighbor, Inverse Distance Weighting, Kriging\n\n\nSometimes gaps are function of sampling, not process"
  },
  {
    "objectID": "slides/18-slides.html#nearest-neighbor",
    "href": "slides/18-slides.html#nearest-neighbor",
    "title": "Interpolation",
    "section": "Nearest neighbor",
    "text": "Nearest neighbor\n\nfind \\(i\\) such that \\(| \\mathbf{x_i} - \\mathbf{x}|\\) is minimized\nThe estimate of \\(z\\) is \\(z_i\\)\n\n\n\n\n\n# data retrieved from https://www.epa.gov/outdoor-air-quality-data/download-daily-data\n\naq &lt;- read_csv(\"/opt/data/data/classexamples/ad_viz_plotval_data_PM25_2024_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"Site Longitude\", \"Site Latitude\"), crs = \"EPSG:4326\") %&gt;% \n  st_transform(., crs = \"EPSG:8826\") %&gt;% \n  mutate(date = as_date(parse_datetime(Date, \"%m/%d/%Y\"))) %&gt;% \n  filter(., date &gt;= 2024-07-01) %&gt;% \n  filter(., date &gt; \"2024-07-01\" & date &lt; \"2024-07-31\")\naq.sum &lt;- aq %&gt;% \n  group_by(., `Site ID`) %&gt;% \n  summarise(., meanpm25 = mean(`Daily AQI Value`))\n\nnodes &lt;- st_make_grid(aq.sum,\n                      what = \"centers\")\n\ndist &lt;- distance(vect(nodes), vect(aq.sum))\nnearest &lt;- apply(dist, 1, function(x) which(x == min(x)))\naq.nn &lt;- aq.sum$meanpm25[nearest]\npreds &lt;- st_as_sf(nodes)\npreds$aq &lt;- aq.nn\n\npreds &lt;- as(preds, \"Spatial\")\nsp::gridded(preds) &lt;- TRUE\npreds.rast &lt;- rast(preds)"
  },
  {
    "objectID": "slides/18-slides.html#inverse-distance-weighting",
    "href": "slides/18-slides.html#inverse-distance-weighting",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nWeight closer observations more heavily\n\n\\[\n\\begin{equation}\n\\hat{z}(\\mathbf{x}) = \\frac{\\sum_{i=1}w_iz_i}{\\sum_{i=1}w_i}\n\\end{equation}\n\\] where\n\\[\n\\begin{equation}\nw_i = | \\mathbf{x} - \\mathbf{x}_i |^{-\\alpha}\n\\end{equation}\n\\] and \\(\\alpha &gt; 0\\) (\\(\\alpha  = 1\\) is inverse; \\(\\alpha = 2\\) is inverse square)"
  },
  {
    "objectID": "slides/18-slides.html#inverse-distance-weighting-1",
    "href": "slides/18-slides.html#inverse-distance-weighting-1",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nterra::interpolate provides flexible interpolation methods\nUse the gstat package to develop the formula\n\n\nmgsf05 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 0.5))\nmgsf2 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 2))\ninterpolate_gstat &lt;- function(model, x, crs, ...) {\n    v &lt;- st_as_sf(x, coords=c(\"x\", \"y\"), crs=crs)\n    p &lt;- predict(model, v, ...)\n    as.data.frame(p)[,1:2]\n}\nzsf05 &lt;- interpolate(preds.rast, mgsf05, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)\nzsf2 &lt;- interpolate(preds.rast, mgsf2, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)\n\n\nNo covariate effect!"
  },
  {
    "objectID": "slides/18-slides.html#inverse-distance-weighting-2",
    "href": "slides/18-slides.html#inverse-distance-weighting-2",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "slides/18-slides.html#inverse-distance-weighting-3",
    "href": "slides/18-slides.html#inverse-distance-weighting-3",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "example/session-4-example.html",
    "href": "example/session-4-example.html",
    "title": "Session 4 Live Code",
    "section": "",
    "text": "Read spreadsheet into R:\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\nfile.to.read &lt;- read_csv(file = \"/opt/data/data/assignment01/landmarks_ID.csv\", col_names = TRUE, col_type = NULL, na = c(\"\", NA))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, coords = c(\"longitude\", \"lattitude\"), crs=4326)\n\n\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 12169 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): ANSICODE, FULLNAME, MTFCC\ndbl (4): STATEFP, POINTID, longitude, lattitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRead in a shapefile:\n\n\nCode\nshapefile.inR &lt;- read_sf(dsn = \"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\nRead in a raster:\n\n\nCode\nlibrary(terra)\nraster.inR &lt;- rast(x = \"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nterra 1.7.78\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#reading-in-the-data",
    "href": "example/session-4-example.html#reading-in-the-data",
    "title": "Session 4 Live Code",
    "section": "",
    "text": "Read spreadsheet into R:\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\nfile.to.read &lt;- read_csv(file = \"/opt/data/data/assignment01/landmarks_ID.csv\", col_names = TRUE, col_type = NULL, na = c(\"\", NA))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, coords = c(\"longitude\", \"lattitude\"), crs=4326)\n\n\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 12169 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): ANSICODE, FULLNAME, MTFCC\ndbl (4): STATEFP, POINTID, longitude, lattitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRead in a shapefile:\n\n\nCode\nshapefile.inR &lt;- read_sf(dsn = \"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\nRead in a raster:\n\n\nCode\nlibrary(terra)\nraster.inR &lt;- rast(x = \"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nterra 1.7.78\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#introducing-yourself-to-vector-data",
    "href": "example/session-4-example.html#introducing-yourself-to-vector-data",
    "title": "Session 4 Live Code",
    "section": "Introducing yourself to vector data",
    "text": "Introducing yourself to vector data\nStructure of the data:\n\n\nCode\nstr(shapefile.inR)\n\n\nsf [2,590 × 124] (S3: sf/tbl_df/tbl/data.frame)\n $ GEOID10   : chr [1:2590] \"16019970700\" \"16025970100\" \"16027021700\" \"16027020700\" ...\n $ SF        : chr [1:2590] \"Idaho\" \"Idaho\" \"Idaho\" \"Idaho\" ...\n $ CF        : chr [1:2590] \"Bonneville County\" \"Camas County\" \"Canyon County\" \"Canyon County\" ...\n $ DF_PFS    : num [1:2590] 0.44 0.67 0.74 0.48 0.75 0.8 0.76 0.72 0.76 0.52 ...\n $ AF_PFS    : num [1:2590] 0.73 0.65 0.86 0.63 0.79 0.88 0.86 0.79 0.8 0.65 ...\n $ HDF_PFS   : num [1:2590] 0.57 0.79 0.69 0.47 0.72 0.55 0.53 0.71 0.74 0.45 ...\n $ DSF_PFS   : num [1:2590] 0.53 0 0.58 0.46 0.12 0.79 0.68 0.27 0.06 0.11 ...\n $ EBF_PFS   : num [1:2590] 0.63 0.95 0.54 0.3 0.63 0.81 0.81 0.44 0.73 0.52 ...\n $ EALR_PFS  : num [1:2590] 0.55 0.83 0.63 0.63 0.65 NA 0.41 0.64 0.65 0.64 ...\n $ EBLR_PFS  : num [1:2590] 0.19 0.93 0.05 0.55 0.09 0.09 0.08 0.09 0.47 0.17 ...\n $ EPLR_PFS  : num [1:2590] 0.8 0.99 0.09 0.1 0.11 0.31 0.08 0.09 0.1 0.09 ...\n $ HBF_PFS   : num [1:2590] 0.69 0.6 0.53 0.07 0.46 0.86 0.93 0.22 0.53 0.28 ...\n $ LLEF_PFS  : num [1:2590] 0.73 0.48 0.67 0.29 0.51 0.87 0.62 0.38 0.32 0.26 ...\n $ LIF_PFS   : num [1:2590] 0.54 0.54 0.53 0.12 0.48 0.61 0.88 0.46 0.41 0.32 ...\n $ LMI_PFS   : num [1:2590] 0.81 0.75 0.61 0.25 0.55 0.92 0.95 0.44 0.42 0.35 ...\n $ PM25F_PFS : num [1:2590] 0.11 0 0.68 0.63 0.42 0.66 0.69 0.57 0.26 0.4 ...\n $ HSEF      : num [1:2590] 0.14 0.06 0.2 0.08 0.21 0.24 0.31 0.18 0.18 0.09 ...\n $ P100_PFS  : num [1:2590] 0.83 0.63 0.47 0.24 0.6 0.72 0.93 0.27 0.39 0.33 ...\n $ P200_I_PFS: num [1:2590] 0.86 0.71 0.81 0.39 0.75 0.98 0.91 0.49 0.67 0.54 ...\n $ AJDLI_ET  : int [1:2590] 1 1 1 0 1 1 1 0 1 1 ...\n $ LPF_PFS   : num [1:2590] 0.75 0.52 0.26 0.3 0.65 0.62 0.63 0.4 0.51 0.36 ...\n $ KP_PFS    : num [1:2590] 0.86 0.21 0.84 0.21 0.61 0.57 0.21 0.43 0.9 0.21 ...\n $ NPL_PFS   : num [1:2590] 0.09 0.05 0.06 0.08 0.03 0.08 0.05 0.05 0.08 0.1 ...\n $ RMP_PFS   : num [1:2590] 0.49 0.01 0.56 0.84 0.26 0.96 0.58 0.36 0.1 0.2 ...\n $ TSDF_PFS  : num [1:2590] 0.22 0.01 0.14 0.42 0.07 0.55 0.13 0.11 0.1 0.19 ...\n $ TPF       : num [1:2590] 5589 1048 11701 3901 5059 ...\n $ TF_PFS    : num [1:2590] 0.55 0.06 0.54 0.53 0.28 0.71 0.83 0.24 0.05 0.09 ...\n $ UF_PFS    : num [1:2590] 0.61 0.16 0.64 0.33 0.66 0.86 0.8 0.45 0.47 0.61 ...\n $ WF_PFS    : num [1:2590] 0.03 0.11 0.98 0.24 0.79 0.98 0.98 0.85 0.42 0.04 ...\n $ UST_PFS   : num [1:2590] 0.59 0.02 0.44 0.31 0.17 0.73 0.83 0.13 0.06 0.18 ...\n $ N_WTR     : int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ N_WKFC    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ N_CLT     : int [1:2590] 0 1 0 0 0 1 1 0 0 0 ...\n $ N_ENY     : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ N_TRN     : int [1:2590] 0 0 0 0 0 0 0 0 1 0 ...\n $ N_HSG     : int [1:2590] 0 0 0 0 0 0 1 0 1 0 ...\n $ N_PLN     : int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ N_HLTH    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ SN_C      : int [1:2590] 0 1 1 0 0 1 1 0 1 0 ...\n $ SN_T      : chr [1:2590] NA NA NA NA ...\n $ DLI       : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ ALI       : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ PLHSE     : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ LMILHSE   : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ ULHSE     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ EPL_ET    : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ EAL_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ EBL_ET    : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ EB_ET     : int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ PM25_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ DS_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TP_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LPP_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ HRS_ET    : chr [1:2590] NA NA NA NA ...\n $ KP_ET     : int [1:2590] 0 0 0 0 0 0 0 0 1 0 ...\n $ HB_ET     : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ RMP_ET    : int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ NPL_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TSDF_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ WD_ET     : int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ UST_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ DB_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ A_ET      : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ HD_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LLE_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ UN_ET     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ LISO_ET   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ POV_ET    : int [1:2590] 0 0 0 0 0 0 1 0 0 0 ...\n $ LMI_ET    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ IA_LMI_ET : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IA_UN_ET  : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IA_POV_ET : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ TC        : num [1:2590] 0 3 1 0 0 4 5 0 2 0 ...\n $ CC        : num [1:2590] 0 2 1 0 0 4 4 0 2 0 ...\n $ IAULHSE   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IAPLHSE   : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IALMILHSE : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IALMIL_76 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ IAPLHS_77 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ IAULHS_78 : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ LHE       : int [1:2590] 1 0 1 0 1 1 1 1 1 0 ...\n $ IALHE     : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ IAHSEF    : num [1:2590] NA NA NA NA NA NA NA NA NA NA ...\n $ N_CLT_EOMI: int [1:2590] 0 1 0 0 0 1 1 0 0 0 ...\n $ N_ENY_EOMI: int [1:2590] 0 1 0 0 0 0 0 0 0 0 ...\n $ N_TRN_EOMI: int [1:2590] 0 0 0 0 0 0 0 1 1 0 ...\n $ N_HSG_EOMI: int [1:2590] 0 0 0 0 0 0 1 0 1 0 ...\n $ N_PLN_EOMI: int [1:2590] 0 0 0 0 0 1 0 0 0 0 ...\n $ N_WTR_EOMI: int [1:2590] 0 0 1 0 0 1 1 0 0 0 ...\n $ N_HLTH_88 : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n $ N_WKFC_89 : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ FPL200S   : int [1:2590] 1 1 1 0 1 1 1 0 1 0 ...\n $ N_WKFC_91 : int [1:2590] 1 0 1 0 1 1 1 1 1 0 ...\n $ TD_ET     : int [1:2590] 0 0 0 0 0 0 0 1 1 0 ...\n $ TD_PFS    : num [1:2590] 0.67 0.78 0.6 0.63 0.85 0.35 0.29 0.94 0.91 0.78 ...\n $ FLD_PFS   : num [1:2590] 0.83 0.88 0.43 0.24 0.82 0.93 0.97 0.44 0.49 0.71 ...\n $ WFR_PFS   : num [1:2590] 0.7 0.82 0.33 0.87 0.8 0.33 0.83 0.77 0.81 0.78 ...\n $ FLD_ET    : int [1:2590] 0 0 0 0 0 1 1 0 0 0 ...\n $ WFR_ET    : int [1:2590] 0 0 0 0 0 0 0 0 0 0 ...\n  [list output truncated]\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:123] \"GEOID10\" \"SF\" \"CF\" \"DF_PFS\" ...\n\n\nNumber of rows and columns:\n\n\nCode\nnrow(shapefile.inR)\n\n\n[1] 2590\n\n\nCode\nncol(shapefile.inR)\n\n\n[1] 124\n\n\nColumn names:\n\n\nCode\ncolnames(shapefile.inR)\n\n\n  [1] \"GEOID10\"    \"SF\"         \"CF\"         \"DF_PFS\"     \"AF_PFS\"    \n  [6] \"HDF_PFS\"    \"DSF_PFS\"    \"EBF_PFS\"    \"EALR_PFS\"   \"EBLR_PFS\"  \n [11] \"EPLR_PFS\"   \"HBF_PFS\"    \"LLEF_PFS\"   \"LIF_PFS\"    \"LMI_PFS\"   \n [16] \"PM25F_PFS\"  \"HSEF\"       \"P100_PFS\"   \"P200_I_PFS\" \"AJDLI_ET\"  \n [21] \"LPF_PFS\"    \"KP_PFS\"     \"NPL_PFS\"    \"RMP_PFS\"    \"TSDF_PFS\"  \n [26] \"TPF\"        \"TF_PFS\"     \"UF_PFS\"     \"WF_PFS\"     \"UST_PFS\"   \n [31] \"N_WTR\"      \"N_WKFC\"     \"N_CLT\"      \"N_ENY\"      \"N_TRN\"     \n [36] \"N_HSG\"      \"N_PLN\"      \"N_HLTH\"     \"SN_C\"       \"SN_T\"      \n [41] \"DLI\"        \"ALI\"        \"PLHSE\"      \"LMILHSE\"    \"ULHSE\"     \n [46] \"EPL_ET\"     \"EAL_ET\"     \"EBL_ET\"     \"EB_ET\"      \"PM25_ET\"   \n [51] \"DS_ET\"      \"TP_ET\"      \"LPP_ET\"     \"HRS_ET\"     \"KP_ET\"     \n [56] \"HB_ET\"      \"RMP_ET\"     \"NPL_ET\"     \"TSDF_ET\"    \"WD_ET\"     \n [61] \"UST_ET\"     \"DB_ET\"      \"A_ET\"       \"HD_ET\"      \"LLE_ET\"    \n [66] \"UN_ET\"      \"LISO_ET\"    \"POV_ET\"     \"LMI_ET\"     \"IA_LMI_ET\" \n [71] \"IA_UN_ET\"   \"IA_POV_ET\"  \"TC\"         \"CC\"         \"IAULHSE\"   \n [76] \"IAPLHSE\"    \"IALMILHSE\"  \"IALMIL_76\"  \"IAPLHS_77\"  \"IAULHS_78\" \n [81] \"LHE\"        \"IALHE\"      \"IAHSEF\"     \"N_CLT_EOMI\" \"N_ENY_EOMI\"\n [86] \"N_TRN_EOMI\" \"N_HSG_EOMI\" \"N_PLN_EOMI\" \"N_WTR_EOMI\" \"N_HLTH_88\" \n [91] \"N_WKFC_89\"  \"FPL200S\"    \"N_WKFC_91\"  \"TD_ET\"      \"TD_PFS\"    \n [96] \"FLD_PFS\"    \"WFR_PFS\"    \"FLD_ET\"     \"WFR_ET\"     \"ADJ_ET\"    \n[101] \"IS_PFS\"     \"IS_ET\"      \"AML_ET\"     \"FUDS_RAW\"   \"FUDS_ET\"   \n[106] \"IMP_FLG\"    \"DM_B\"       \"DM_AI\"      \"DM_A\"       \"DM_HI\"     \n[111] \"DM_T\"       \"DM_W\"       \"DM_H\"       \"DM_O\"       \"AGE_10\"    \n[116] \"AGE_MIDDLE\" \"AGE_OLD\"    \"TA_COU_116\" \"TA_COUNT_C\" \"TA_PERC\"   \n[121] \"TA_PERC_FE\" \"UI_EXP\"     \"THRHLD\"     \"geometry\"  \n\n\nFirst few rows of data:\n\n\nCode\nhead(file.as.sf, n = 6)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -116.7492 ymin: 43.18526 xmax: -111.3299 ymax: 43.80765\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n  STATEFP ANSICODE       POINTID FULLNAME        MTFCC             geometry\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;POINT [°]&gt;\n1      16 NA        110680906449 State Hospital… K1231 (-112.3351 43.18526)\n2      16 00390731 1102653699212 Snake River Ra… C3081 (-111.4147 43.45992)\n3      16 00395965 1102653699303 South Falls Ctr C3081  (-112.0222 43.4813)\n4      16 00399195 1102653700141 Swan Valley Ra… C3081 (-111.3299 43.44797)\n5      16 00382976 1102653683820 Hawley Gulch R… C3081 (-111.5744 43.65464)\n6      16 00398090 1102653708691 Sand Holw       C3081 (-116.7492 43.80765)\n\n\nSimple plotting:\n\n\nCode\nplot(st_geometry(shapefile.inR))\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(shapefile.inR[\"AGE_10\"])\n\n\n\n\n\n\n\n\n\nCRS of vector:\n\n\nCode\nst_crs(shapefile.inR)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "example/session-4-example.html#introducing-yourself-to-raster-data",
    "href": "example/session-4-example.html#introducing-yourself-to-raster-data",
    "title": "Session 4 Live Code",
    "section": "Introducing yourself to raster data",
    "text": "Introducing yourself to raster data\nDescribe raster before reading it in:\n\n\nCode\ndescribe(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\n [1] \"Driver: GTiff/GeoTIFF\"                                                                                 \n [2] \"Files: C:/Users/carolynkoehn/Documents/HES505_Fall_2024/data/2023/assignment01/wildfire_hazard_agg.tif\"\n [3] \"Size is 4607, 4016\"                                                                                    \n [4] \"Coordinate System is:\"                                                                                 \n [5] \"PROJCRS[\\\"unnamed\\\",\"                                                                                  \n [6] \"    BASEGEOGCRS[\\\"NAD83\\\",\"                                                                            \n [7] \"        DATUM[\\\"North American Datum 1983\\\",\"                                                          \n [8] \"            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\"                                          \n [9] \"                LENGTHUNIT[\\\"metre\\\",1]]],\"                                                            \n[10] \"        PRIMEM[\\\"Greenwich\\\",0,\"                                                                       \n[11] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\"                                                \n[12] \"        ID[\\\"EPSG\\\",4269]],\"                                                                           \n[13] \"    CONVERSION[\\\"Albers Equal Area\\\",\"                                                                 \n[14] \"        METHOD[\\\"Albers Equal Area\\\",\"                                                                 \n[15] \"            ID[\\\"EPSG\\\",9822]],\"                                                                       \n[16] \"        PARAMETER[\\\"Latitude of false origin\\\",23,\"                                                    \n[17] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[18] \"            ID[\\\"EPSG\\\",8821]],\"                                                                       \n[19] \"        PARAMETER[\\\"Longitude of false origin\\\",-96,\"                                                  \n[20] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[21] \"            ID[\\\"EPSG\\\",8822]],\"                                                                       \n[22] \"        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\"                                         \n[23] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[24] \"            ID[\\\"EPSG\\\",8823]],\"                                                                       \n[25] \"        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\"                                         \n[26] \"            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\"                                                 \n[27] \"            ID[\\\"EPSG\\\",8824]],\"                                                                       \n[28] \"        PARAMETER[\\\"Easting at false origin\\\",0,\"                                                      \n[29] \"            LENGTHUNIT[\\\"metre\\\",1],\"                                                                  \n[30] \"            ID[\\\"EPSG\\\",8826]],\"                                                                       \n[31] \"        PARAMETER[\\\"Northing at false origin\\\",0,\"                                                     \n[32] \"            LENGTHUNIT[\\\"metre\\\",1],\"                                                                  \n[33] \"            ID[\\\"EPSG\\\",8827]]],\"                                                                      \n[34] \"    CS[Cartesian,2],\"                                                                                  \n[35] \"        AXIS[\\\"easting\\\",east,\"                                                                        \n[36] \"            ORDER[1],\"                                                                                 \n[37] \"            LENGTHUNIT[\\\"metre\\\",1,\"                                                                   \n[38] \"                ID[\\\"EPSG\\\",9001]]],\"                                                                  \n[39] \"        AXIS[\\\"northing\\\",north,\"                                                                      \n[40] \"            ORDER[2],\"                                                                                 \n[41] \"            LENGTHUNIT[\\\"metre\\\",1,\"                                                                   \n[42] \"                ID[\\\"EPSG\\\",9001]]]]\"                                                                  \n[43] \"Data axis to CRS axis mapping: 1,2\"                                                                    \n[44] \"Origin = (-2294745.000000000000000,3172575.000000000000000)\"                                           \n[45] \"Pixel Size = (240.000000000000000,-240.000000000000000)\"                                               \n[46] \"Metadata:\"                                                                                             \n[47] \"  AREA_OR_POINT=Area\"                                                                                  \n[48] \"Image Structure Metadata:\"                                                                             \n[49] \"  COMPRESSION=LZW\"                                                                                     \n[50] \"  INTERLEAVE=BAND\"                                                                                     \n[51] \"Corner Coordinates:\"                                                                                   \n[52] \"Upper Left  (-2294745.000, 3172575.000) (127d 6'55.97\\\"W, 48d 8'21.26\\\"N)\"                             \n[53] \"Lower Left  (-2294745.000, 2208735.000) (123d27'25.80\\\"W, 39d53'32.24\\\"N)\"                             \n[54] \"Upper Right (-1189065.000, 3172575.000) (112d33'19.93\\\"W, 50d38'58.88\\\"N)\"                             \n[55] \"Lower Right (-1189065.000, 2208735.000) (110d31'22.39\\\"W, 42d 3'38.51\\\"N)\"                             \n[56] \"Center      (-1741905.000, 2690655.000) (118d26'35.33\\\"W, 45d20'39.18\\\"N)\"                             \n[57] \"Band 1 Block=4607x1 Type=Float32, ColorInterp=Gray\"                                                    \n[58] \"  Description = WHP_ID\"                                                                                \n[59] \"  Min=0.000 Max=64185.656 \"                                                                            \n[60] \"  Minimum=0.000, Maximum=64185.656, Mean=-9999.000, StdDev=-9999.000\"                                  \n[61] \"  NoData Value=nan\"                                                                                    \n[62] \"  Metadata:\"                                                                                           \n[63] \"    STATISTICS_MAXIMUM=64185.65625\"                                                                    \n[64] \"    STATISTICS_MEAN=-9999\"                                                                             \n[65] \"    STATISTICS_MINIMUM=0\"                                                                              \n[66] \"    STATISTICS_STDDEV=-9999\"                                                                           \n\n\nBasic object info:\n\n\nCode\nraster.inR\n\n\nclass       : SpatRaster \ndimensions  : 4016, 4607, 1  (nrow, ncol, nlyr)\nresolution  : 240, 240  (x, y)\nextent      : -2294745, -1189065, 2208735, 3172575  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : wildfire_hazard_agg.tif \nname        :   WHP_ID \nmin value   :     0.00 \nmax value   : 64185.66 \n\n\nSummary of values:\n\n\nCode\n# Summary of all values\nsummary(values(raster.inR))\n\n\n     WHP_ID       \n Min.   :    0    \n 1st Qu.:   85    \n Median :  242    \n Mean   :  952    \n 3rd Qu.:  668    \n Max.   :64186    \n NA's   :7349366  \n\n\nCode\n# Summary of some values\nsummary(raster.inR)\n\n\nWarning: [summary] used a sample\n\n\n     WHP_ID        \n Min.   :    0.00  \n 1st Qu.:   83.91  \n Median :  242.20  \n Mean   :  949.59  \n 3rd Qu.:  666.39  \n Max.   :57327.66  \n NA's   :39866     \n\n\nBasic plot:\n\n\nCode\nplot(raster.inR)\n\n\n\n\n\n\n\n\n\nCode\n#Change color of NA\nplot(raster.inR, colNA = \"black\")\n\n\n\n\n\n\n\n\n\nCRS of raster:\n\n\nCode\ncrs(raster.inR)\n\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\nCode\n# You can also use st_crs!\nst_crs(raster.inR)\n\n\nCoordinate Reference System:\n  User input: unnamed \n  wkt:\nPROJCRS[\"unnamed\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]",
    "crumbs": [
      "Examples",
      "Getting started",
      "Introducing yourself to spatial data"
    ]
  },
  {
    "objectID": "assignment/07-rasteropssolutions.html",
    "href": "assignment/07-rasteropssolutions.html",
    "title": "Assignment 7 Solutions: Building Spatial Databases",
    "section": "",
    "text": "1. You’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets.\n\nThe trickiest part here is just downloading the FS data (made more difficult because they updated the data on 26 Nov 2023). Once you load the function, you should be able to run it using the current link to the Administrative Forest Boundaries data. Because we already know there are empty geometries in the cejst dataset, we go ahead and drop those here.\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(terra)\nlibrary(tmap, quietly = TRUE)\n\n download_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n  }\n\nfs.shapefile &lt;- download_unzip_read(link = \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\")\n\n\ncejst.pnw &lt;- read_sf(\"data/opt/data/2023/assignment07/cejst_pnw.shp\")%&gt;% \n  filter(., !st_is_empty(.))\n\nincidents.csv &lt;- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\")\n\nland.use &lt;- rast(\"data/opt/data/2023/assignment07/land_use_pnw.tif\")\nfire.haz &lt;- rast(\"data/opt/data/2023/assignment07/wildfire_hazard_agg.tif\")\n\n2. Validate your geometries and make sure all of your data is in the same CRS.\n\nWe start by checking to make sure the rasters have the same CRS (because we’d like to avoid projecting rasters). They don’t so we’ll use the land use data as the template as the interpolation for continuous data is a little less error prone. We use the terra::project function to reproject the rasters and then use sf::st_transform to reproject all of the vector data. You’ll notice that some of the coordinates for the incidents are NA we need to get rid of those before we can convert to an sf object. We do that with a call to filter before using st_as_sf.\n\n\ncrs(land.use) \n\n[1] \"PROJCRS[\\\"Albers_Conical_Equal_Area\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4326]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\ncrs(fire.haz)\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nfire.haz.proj &lt;- project(fire.haz, land.use)\ncrs(fire.haz.proj) == crs(land.use)\n\n[1] TRUE\n\nfs.shapefile.proj &lt;- fs.shapefile %&gt;% \n  st_transform(., crs=crs(land.use))\n\ncejst.proj &lt;- cejst.pnw %&gt;% \n  st_transform(., crs=crs(land.use))\n\nincidents.proj &lt;- incidents.csv %&gt;% \n  filter(., !is.na(POO_LONGITUDE) | !is.na(POO_LATITUDE) ) %&gt;% \n  st_as_sf(., coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4269) %&gt;% \n  st_transform(., crs=crs(land.use))\n\n\nOnce we’ve gotten everything into the same CRS, we can check for valid geometries. Unfortunately, the FS boundary file has some invalid geometries, so we’ll fix those with st_make_valid.\n\n\nall(st_is_valid(fs.shapefile.proj))\n\n[1] FALSE\n\nall(st_is_valid(cejst.proj))\n\n[1] TRUE\n\nall(st_is_valid(incidents.proj))\n\n[1] TRUE\n\nfs.shapefile.proj.valid &lt;- st_make_valid(fs.shapefile.proj)\nall(st_is_valid(fs.shapefile.proj.valid))\n\n[1] TRUE\n\n\n3. Smooth the wildfire hazard and land use datasets using a 5s5 moving window; use the mean for the continuous dataset and the mode for the categorical dataset.\n\nSmoothing the two rasters is relatively straightforward using the focal function. We set the w argument to be 5 so that we get the 5x5 moving window and then specify the function for each.\n\n\nhazard.smooth &lt;- focal(fire.haz.proj, w=5, fun=\"mean\")\nland.use.smooth &lt;- focal(land.use, w=5, fun=\"modal\")\nlevels(land.use.smooth) &lt;- levels(land.use)\n\n4. Estimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\n\nThis one is a little trickier. First, we need to restrict the incidents and forests to the area for which we have data. Then, we’ve got to join the incident to the appropriate forest. Finally, we’ve got to summarize the incident cost by each forest. We use st_crop so that we can cut off the datasets at the actual bounding boxes (rather than looking for the observations that intersect). Once we do that, we need to join the incidents to the forests (using st_join). Once we’ve got the incidents linked to forests, we can use group_by, summarise, and sum to create a new, summary level variable that is the total cost. The last step is to join that value back to the original forest geometries.\n\n\nincidents.pnw &lt;- st_crop(incidents.proj, st_bbox(cejst.proj))\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nforest.pnw &lt;- st_crop(fs.shapefile.proj.valid, st_bbox(cejst.proj))\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nincidents.forest &lt;- incidents.pnw %&gt;% \n  st_join(x=., forest.pnw, join=st_within, left=FALSE)\n\nincidents.summary &lt;- incidents.forest %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(FORESTNAME) %&gt;% \n  summarise(., totcost = sum(PROJECTED_FINAL_IM_COST, na.rm=TRUE))\n\nforest.join &lt;- forest.pnw %&gt;% \n  left_join(., incidents.summary)\n\nJoining with `by = join_by(FORESTNAME)`\n\n\n5. Next join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\n\nThis step asks you to do 2 different things: join some tabular elements from the CEJST dataset to your forest datasets and extract values from the raster datasets. We first use select to keep only the total population (TPF), housing burden (HBF_PFS), and percent of population &gt;200% below the poverty level (P200_I_PFS). Then, we can use the st_join function again to attribute our forest data with the appropriate CEJST data and summarise it to the forest level. Once we’ve got our vector data attributed, we can use it to extract the appropriate values from the rasters. Because terra::extract returns the values in the order they are listed in the data, we just use cbind to join the columns to our forest data and then use rename to make the columns are little more sensible.\n\n\ncejst.select &lt;- cejst.proj %&gt;% \n  select(., c(TPF, HBF_PFS, P200_I_PFS))\n\nforest.cejst &lt;- forest.join %&gt;% \n  st_join(., y=cejst.select, join=st_intersects) %&gt;% \n  group_by(FORESTNAME) %&gt;% \n  summarise_at(vars(totcost:P200_I_PFS), mean, na.rm=TRUE)\n\n\nforest.landuse.ext &lt;- terra::extract(x=land.use.smooth, y = vect(forest.cejst), fun=\"modal\", na.rm=TRUE)\n\nforest.firehaz.ext &lt;- terra::extract(x= hazard.smooth, y = vect(forest.cejst), fun=\"mean\", na.rm=TRUE)\n\nforest.cejst.join &lt;- cbind(forest.cejst,forest.landuse.ext$category, forest.firehaz.ext$focal_mean) %&gt;% \n  rename(category = \"forest.landuse.ext.category\", hazard = \"forest.firehaz.ext.focal_mean\")\n\n6. Make a set of maps that shows the Forest-level values for all of your selected variables.\n\nYou could certainly make a map for each variable individually, but that is long and tedious. Here, we’ll use tm_facets again to create maps of all of our variables. To do that, we need the data in long format so we use pivot_longer to create a column with the variable name (taken from the column) and a column with the numeric value (take from the actual observation). Note that because a column cannot have mixed datatypes, the landcover data has to remain as the numeric code. We could fuss with that more, but for now all of the focal landcover values are forest so there’s not a ton of reason to do this. Once we’ve got the data in long format, it’s a simple call to tmap to make make all of our maps.\n\n\nforest.cejst.long &lt;- forest.cejst.join %&gt;% \n  pivot_longer(., cols =totcost:hazard, names_to=\"variable\", values_to = \"value\")\n\ntm_shape(cejst.proj) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(forest.cejst.long) +\n  tm_fill(col=\"value\") +\n  tm_facets(by = c(\"variable\"), free.scales.fill = TRUE)"
  },
  {
    "objectID": "slides/17-slides.html#what-is-a-point-pattern",
    "href": "slides/17-slides.html#what-is-a-point-pattern",
    "title": "Point Pattern Analysis",
    "section": "What is a point pattern?",
    "text": "What is a point pattern?\n\n\n\n\nPoint pattern: A set of events within a study region (i.e., a window) generated by a random process\nSet: A collection of mathematical events\nEvents: The existence of a point object of the type we are interested in at a particular location in the study region\nA marked point pattern refers to a point pattern where the events have additional descriptors\n\n\n\n\nSome notation:\n\n\\(S\\): refers to the entire set\n\\(s_i\\) refers to one event (point) in set \\(S\\)\n\\(\\mathbf{s_i}\\) denotes the vector of data describing point \\(s_i\\) in set \\(S\\)\n\\(\\#(S \\in A )\\) refers to the number of points in \\(S\\) within study area \\(A\\)"
  },
  {
    "objectID": "slides/17-slides.html#requirements-for-a-set-to-be-considered-a-point-pattern",
    "href": "slides/17-slides.html#requirements-for-a-set-to-be-considered-a-point-pattern",
    "title": "Point Pattern Analysis",
    "section": "Requirements for a set to be considered a point pattern",
    "text": "Requirements for a set to be considered a point pattern\n\nThe pattern must be mapped on a plane to preserve distance\nThe study area, \\(A\\), should be objectively determined\nThere should be a \\(1:1\\) correspondence between objects in \\(A\\) and events in the pattern (no undetected points)\nEvents must be proper i.e., refer to actual locations of the event\n\n\nQuestion: What does mapping on a plane have to do with the CRS you choose?"
  },
  {
    "objectID": "slides/17-slides.html#describing-point-patterns",
    "href": "slides/17-slides.html#describing-point-patterns",
    "title": "Point Pattern Analysis",
    "section": "Describing Point Patterns",
    "text": "Describing Point Patterns\n\n\n\n\nDensity-based metrics: the \\(\\#\\) of points within area, \\(a\\), in study area \\(A\\)\nDistance-based metrics: based on nearest neighbor distances or the distance matrix for all points\nFirst order effects reflect variation in intensity due to variation in the ‘attractiveness’ of locations\nSecond order effects reflect variation in intensity due to the presence of points themselves\n\n\n\n\n\n\nfrom Manuel Gimond\n\n\n\n\n\nFirst order effects: on-the-ground variation affects distribution (e.g. good habitat vs garbage habitat)\nSecond order effects: points themselves effect occurance of other points (e.g. flocking birds, nurse plants)"
  },
  {
    "objectID": "slides/17-slides.html#centrography",
    "href": "slides/17-slides.html#centrography",
    "title": "Point Pattern Analysis",
    "section": "Centrography",
    "text": "Centrography\n\n\n\n\nMean center: the point, \\(\\hat{\\mathbf{s}}\\), whose coordinates are the average of all events in the pattern\nStandard distance: a measure of the dispersion of points around the mean center\nStandard ellipse: dispersion in one dimension\n\n\n\n\n\n\nFrom Manuel Gimond\n\n\n\n\n\nAnalogous to summary statistics for standard data (mean, sd, interquartile range) but for space\nThese are the sort of things that show up in the first paragraph of the results – a sense for how dispersed the events are"
  },
  {
    "objectID": "slides/17-slides.html#analyzing-point-patterns",
    "href": "slides/17-slides.html#analyzing-point-patterns",
    "title": "Point Pattern Analysis",
    "section": "Analyzing Point Patterns",
    "text": "Analyzing Point Patterns\n\nModeling random processes means we are interested in probability densities of the points (first-order;density)\nAlso interested in how the presence of some events affects the probability of other events (second-order;distance)\nFinally interested in how the attributes of an event affect location (marked)\nNeed to introduce a few new packages (spatstat and gstat)\n\n\n\nFirst, are these events just randomly distributed? If they are, our covariates probably don’t matter…"
  },
  {
    "objectID": "slides/17-slides.html#density-based-methods",
    "href": "slides/17-slides.html#density-based-methods",
    "title": "Point Pattern Analysis",
    "section": "Density based methods",
    "text": "Density based methods\n\n\n\nThe overall intensity of a point pattern is a crude density estimate\n\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{\\#(S \\in A )}{a}\n\\end{equation}\n\\] * Local density = quadrat counts\n\n\n\n\n\n\n\n\n\n\n\n\nLambda is a rate, this is a quadrat"
  },
  {
    "objectID": "slides/17-slides.html#kernel-density-estimates-kde",
    "href": "slides/17-slides.html#kernel-density-estimates-kde",
    "title": "Point Pattern Analysis",
    "section": "Kernel Density Estimates (KDE)",
    "text": "Kernel Density Estimates (KDE)\n\\[\n\\begin{equation}\n\\hat{f}(x) = \\frac{1}{nh_xh_y} \\sum_{i=1}^n k\\bigg(\\frac{{x-x_i}}{h_x},\\frac{{y-y_i}}{h_y} \\bigg)\n\\end{equation}\n\\]\n\n\nAssume each location in \\(\\mathbf{s_i}\\) drawn from unknown distribution\nDistribution has probability density \\(f(\\mathbf{x})\\)\nEstimate \\(f(\\mathbf{x})\\) by averaging probability “bumps” around each location\n\n\n\n\nQuadrats are arbitrary. When we want to measure continuous density changes, these choices start to matter\nModel the shape of intensity\nThis is a normal kernal – hill that tapers off in all directions equally"
  },
  {
    "objectID": "slides/17-slides.html#kernel-density-estimates-kde-1",
    "href": "slides/17-slides.html#kernel-density-estimates-kde-1",
    "title": "Point Pattern Analysis",
    "section": "Kernel Density Estimates (KDE)",
    "text": "Kernel Density Estimates (KDE)\n\n\n\\(h\\) is the bandwidth and \\(k\\) is the kernel\nWe can use stats::density to explore\nkernel: defines the shape, size, and weight assigned to observations in the window\nbandwidth often assigned based on distance from the window center\n\n\n\n\nA density plot is a smooth of your histogram\nBandwidth: how far to look for other points (small bandwidth = rougher surface)"
  },
  {
    "objectID": "slides/17-slides.html#kernel-density-estimates-in-action",
    "href": "slides/17-slides.html#kernel-density-estimates-in-action",
    "title": "Point Pattern Analysis",
    "section": "Kernel Density Estimates in Action",
    "text": "Kernel Density Estimates in Action\n\nlibrary(spatstat)\n\nx &lt;- rpoispp(lambda =50)\nK0 &lt;- density(x)\nK1 &lt;- density(x, adjust=0.5)\nK2 &lt;- density(x, adjust=1.5)\nK3 &lt;- density(x, kernel=\"disc\")"
  },
  {
    "objectID": "slides/17-slides.html#choosing-bandwidths-and-kernels",
    "href": "slides/17-slides.html#choosing-bandwidths-and-kernels",
    "title": "Point Pattern Analysis",
    "section": "Choosing bandwidths and kernels",
    "text": "Choosing bandwidths and kernels\n\nSmall values for \\(h\\) give ‘spiky’ densities\nLarge values for \\(h\\) smooth much more\nSome kernels have optimal bandwidth detection\ntmap package provides additional functionality\n\n\n\nNo covariates here!"
  },
  {
    "objectID": "slides/17-slides.html#second-order-analysis-1",
    "href": "slides/17-slides.html#second-order-analysis-1",
    "title": "Point Pattern Analysis",
    "section": "Second-Order Analysis",
    "text": "Second-Order Analysis\n\nKDEs assume independence of points (first order randomness)\nSecond-order methods allow dependence among points (second-order randomness)\nSeveral functions for assessing second order dependence (\\(K\\), \\(L\\), and \\(G\\))"
  },
  {
    "objectID": "slides/17-slides.html#distance-based-metrics",
    "href": "slides/17-slides.html#distance-based-metrics",
    "title": "Point Pattern Analysis",
    "section": "Distance based metrics",
    "text": "Distance based metrics\n\nProvide an estimate of the second order effects\nMean nearest-neighbor distance: \\[\\hat{d}_{min} = \\frac{\\sum_{i = 1}^{m} d_{min}(\\mathbf{s_i})}{n}\\]\n\n\nAll metrics are based in neighbor distances"
  },
  {
    "objectID": "slides/17-slides.html#nearest-neighbor-distance",
    "href": "slides/17-slides.html#nearest-neighbor-distance",
    "title": "Point Pattern Analysis",
    "section": "Nearest-neighbor distance",
    "text": "Nearest-neighbor distance\n\nANN &lt;- apply(nndist(x, k=1:50),2,FUN=mean)\nplot(ANN ~ eval(1:50), type=\"b\", main=NULL, las=1, \n     xlab=\"kth nearest neighbor\", ylab=\"Mean distance\")\n\n\n\nNot very good for recognizing clumping, which is what we’re interested in with second order effects"
  },
  {
    "objectID": "slides/17-slides.html#ripleys-k-function",
    "href": "slides/17-slides.html#ripleys-k-function",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\n\nNearest neighbor methods throw away a lot of information\nIf points have independent, fixed marginal densities, then they exhibit complete, spatial randomness (CSR)\nThe K function is an alternative, based on a series of circles with increasing radius\n\n\\[\n\\begin{equation}\nK(d) = \\lambda^{-1}E(N_d)\n\\end{equation}\n\\]\n\nWe can test for clustering by comparing to the expectation:\n\n\\[\n\\begin{equation}\nK_{CSR}(d) = \\pi d^2\n\\end{equation}\n\\]\n\nif \\(k(d) &gt; K_{CSR}(d)\\) then there is clustering at the scale defined by \\(d\\)\n\n\n\n\nIncreasing radius: at which distances might we see some clustering\nK is the rate (which we may not entirely know) times how many points you have within a given distance\n“Null” hypothesis is area of a circle. When K diverges, we have more or less points than we expected given the distance. Actually making lambda (rate) equal 1."
  },
  {
    "objectID": "slides/17-slides.html#ripleys-k-function-1",
    "href": "slides/17-slides.html#ripleys-k-function-1",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nWhen working with a sample the distribution of \\(K\\) is unknown\nEstimate with\n\n\\[\n\\begin{equation}\n\\hat{K}(d) = \\hat{\\lambda}^{-1}\\sum_{i=1}^n\\sum_{j=1}^n\\frac{I(d_{ij} &lt;d)}{n(n-1)}\n\\end{equation}\n\\]\nwhere:\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{n}{|A|}\n\\end{equation}\n\\]\n\nFirst slide is ideal world (simulation). This is estimation with data. R will do it for you!"
  },
  {
    "objectID": "slides/17-slides.html#ripleys-k-function-2",
    "href": "slides/17-slides.html#ripleys-k-function-2",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\nUsing the spatstat package:"
  },
  {
    "objectID": "slides/17-slides.html#ripleys-k-function-3",
    "href": "slides/17-slides.html#ripleys-k-function-3",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nkf &lt;- Kest(bramblecanes, correction-\"border\")\nplot(kf)\n\n\n\n\nDifferent K-hats refer to different assumptions about lambda and its shape\nRadius on x-axis, K on y axis\nWhat are we missing to tell us if our data is clustered?"
  },
  {
    "objectID": "slides/17-slides.html#ripleys-k-function-4",
    "href": "slides/17-slides.html#ripleys-k-function-4",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\naccounting for variation in \\(d\\)\n\n\nkf.env &lt;- envelope(bramblecanes, correction=\"border\", envelope = FALSE, verbose = FALSE)\nplot(kf.env)\n\n\n\n\nRed line is expectation under complete spatial randomness\nWe need to include a confidence region since lines will rarely overlap entirely, so we simulate point patterns under CSR (these are still stochastic processes even if totally random)\nIs there clustering? Range of clustering shown - scale of analysis matters. What kind of distance is necessary to overcome spatial autocorrelation?"
  },
  {
    "objectID": "slides/17-slides.html#other-functions",
    "href": "slides/17-slides.html#other-functions",
    "title": "Point Pattern Analysis",
    "section": "Other functions",
    "text": "Other functions\n\n\n\n\\(L\\) function: square root transformation of \\(K\\)\n\\(G\\) function: the cummulative frequency distribution of the nearest neighbor distances\n\\(F\\) function: similar to \\(G\\) but based on randomly located points\n\n\n\n\n\n\n\n\n\n\n\n\n\nG lets you compare the marks of the points – beyond locations to attributes at locations. E.g. is productivity of eagle nests clustered?"
  },
  {
    "objectID": "lesson/getting-setup.html",
    "href": "lesson/getting-setup.html",
    "title": "Getting Set Up",
    "section": "",
    "text": "This is an intro to git and github, most of which was originally created by Jessica Taylor and Nora Honkomp (2 former students in this class!).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#lets-git-started",
    "href": "lesson/getting-setup.html#lets-git-started",
    "title": "Getting Set Up",
    "section": "Let’s “git” started",
    "text": "Let’s “git” started\nWe are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\nAccept the invitation to the assignment repo\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installload-required-package",
    "href": "lesson/getting-setup.html#installload-required-package",
    "title": "Getting Set Up",
    "section": "Install/Load Required Package",
    "text": "Install/Load Required Package\nLoad the following packages in RStudio. If you do not have them installed, you can do so using install.packages().\n\n\nCode\nlibrary(usethis)\nlibrary(gitcreds)\nlibrary(knitr)",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#create-a-github-account",
    "href": "lesson/getting-setup.html#create-a-github-account",
    "title": "Getting Set Up",
    "section": "Create a GitHub Account",
    "text": "Create a GitHub Account\nYou will need to access GitHub with an account for this tutorial. If you don’t already have an account, you can sign up for free here: https://github.com/\nYou will be asked to sign up using your email, a password you create, and a username. Your username is what will be visible to others that you collaborate with, so it’s a good idea to make it something straight forward and professional. You can skip personalization for now by scrolling to the bottom of the page.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#github-and-rstudio-server",
    "href": "lesson/getting-setup.html#github-and-rstudio-server",
    "title": "Getting Set Up",
    "section": "Github and RStudio Server",
    "text": "Github and RStudio Server\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\nBring the project into RStudio\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\nVerify that the “Git” tab is available and that your project is shown in the upper right-hand corner\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "href": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "title": "Getting Set Up",
    "section": "Installing Git (for your local machine)",
    "text": "Installing Git (for your local machine)\nYou will need the program Git for this tutorial. First, let’s check to see if Git is already installed.\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console (bottom-left panel). Enter the following:\n\n\nCode\nwhich git\ngit --version\n\n\n/c/Program Files/Git/cmd/git\ngit version 2.45.2.windows.1\n\n\nYou will get something like the above output if Git is already installed. If it is not installed, you will get something like git: command not found.\n\nWindows\nDownload the program here: https://git-scm.com/downloads\n\n\nMac\nYou may have been prompted to install command line developer tools. You should accept this offer. If you were not prompted, use this command:\n\n\nCode\nxcode-select --install\n\n\n\n\nIntroduce Yourself to Git in R\nOnce Git is successfully installed and you have a GitHub account, you will need to let R/Git know your account information to access the remote repositories.\nEnter the following in the Console tab, substituting the user name and email with your GitHub user name and email:\n\n\nCode\nusethis::use_git_config(user.name = \"Bob Barker\", user.email = \"bobbarker@thepriceisright.org\")\n\n\n\n\nGet a Personal Access Token\nA personal access token (PAT) is used for GitHub as a type of authentication. You used to be able to use your username and password, but not any more. The token-based authentication has increased security.\nBefore generating a new PAT, check to see if you already have one. Run gitcreds_set() and one of two things will happen:\n\n\nR will prompt you to enter a token. This means you don’t already have one and need to create one. Hit Esc and run the second line of code, create_github_token(). This will bring you to GitHub where you can create a token. Save this token somewhere safe - you will not be able to access it via GitHub again.\nR will show you the saved credentials (username and password) and give you options to exit without changing (1: Abort), replace the credentials, or to see the current token. This means you already have a token and can keep it or change it if it has expired. If it expired and you need a new one, run create_github_token().\n\n\n\nCode\n## Run this to see if you already have credentials, or to change them\ngitcreds::gitcreds_set()\n## Only run this if you need to generate a token\ncreate_github_token()",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#version-control",
    "href": "lesson/getting-setup.html#version-control",
    "title": "Getting Set Up",
    "section": "Version Control",
    "text": "Version Control\nThe main function of Git is version control. This means Git will track the changes you make so you can revert to (or view) previous versions of the document. In order for this to work effectively, you need to start tracking your changes (make a repository) and frequently create versions with changes you have made (committing).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-a-local-repository",
    "href": "lesson/getting-setup.html#setting-up-a-local-repository",
    "title": "Getting Set Up",
    "section": "Setting Up a Local Repository",
    "text": "Setting Up a Local Repository\nAnother function of Git is to store all your relevant files in a repository, similar to a project. You can create a repository that is located only on your device (therefore it is considered “local”). This will allow you to track changes to a project on your computer and access previous versions of the document at any time. We will look at how to create a local repository using Rstudio, but know that it is also possible to do this using the terminal. (See page 96 of Gandrud (2015))\n\nIn Rstudio, select File in the top left corner, and then New Project\nWhen the New Project menu appears, select New Directory\n\n\nOn the following screen, select New Project\nFinally, type the name you want to use for your new project, browse to the location where you want it saved on your computer, and select the Create a git repository box.\n\n\nYou now have a folder with a .Rproj object and a .gitignore file. There is also a hidden .git folder that stores all the project information, including the version history files (commit history).\n\nAdding, Staging, and Committing\nYou can create new files or move existing files (including your data) into this project folder, and Git can track changes made to them. In order to do this, you will need to “add” files that have been created or moved into the project and “commit” these changes.\nFirst, you will need to save the new file or move an existing file into the project folder. Then, you will “add” those files to your commit by checking the boxes in the Git tab in your Rstudio window. If Git is already tracking a document, you will see an M in the status column, however, if the file hasn’t been added (and is therefore not being tracked), you will see a ? in the status column.\n\nIn order to save a version of your project as it is right now (with these added files), you will need to commit. To do this, select the Commit button, above the boxes you just checked. This will open a new window where you can see the list of files that you added.\nType a useful message in the Commit message box briefly describing the changes you made in this version. Then hit Commit.\n\nYou should see a window pop up telling you what changes were successfully made. If this window ever says “failed”, “execution halted”, or “aborted”, this means the commit did not work and you should read the message closely to determine why.\nSave and commit frequently in order to get the most use out of your version control.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#branches-and-merging",
    "href": "lesson/getting-setup.html#branches-and-merging",
    "title": "Getting Set Up",
    "section": "Branches and Merging",
    "text": "Branches and Merging\n\nBranches\nA new repository will have one branch called main. You can think of this as the master version. You can create additional branches which are an exact copy of the main branch where you can make changes without committing them to the master. This is useful when several people are working on the same thing, or if you want to try multiple approaches to the same file (i.e. test run some code).\nTo create a branch, select the button with small purple shapes on the right hand side of your Git tab in Rstudio.\n\nYou can now create a new branch, for example one called “test”. This is essentially a copy of everything that is in your repository, and making changes here will not affect the main branch.\nTo switch between branches, select the drop down menu to the right of the button you used to create a new branch.\n\nYou can easily switch to a different branch by selecting it on this menu.\nNote that any files you create on a side branch will not show up in the main branch unless you merge them.\nYou can have multiple branches coming off your main branch at once, so you can try multiple different approaches (or by multiple people) simultaneously. Anytime you make a new branch it will start as a duplicate of the main branch.\nYou can see a history of your commits on different branches by selecting the clock icon (designating “History”) near the Commit button on your Git tab. You will initially see the history for only the branch you are currently in, but if you select the drop down menu of branches at the top of this window, you can select another branch or all branches and see how the branches compare to one another.\n\n\n\nMerging\nIf you find a method that works in a side branch and you want to bring it in to the main branch, this is called a merge.\nTo merge a side branch with the main branch (changing the version of your repo main branch to match that of the successful side branch), we will use the terminal.\n\nMake sure you are on the main branch\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console.\nType git merge and then the name of the branch you want to merge into the main branch.\n\n\nThe repository of your main branch should now match the repository of whatever branch you merged with it.\nOne way to check if the branches merged the way you intended is to select the history button again (make sure you go to all branches) and check that the “HEAD” (which is the main branch) is at the same level as whichever branch you merged it with.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-remote-repositories",
    "href": "lesson/getting-setup.html#setting-up-remote-repositories",
    "title": "Getting Set Up",
    "section": "Setting Up Remote Repositories",
    "text": "Setting Up Remote Repositories\nTo set up a remote repository, make sure you are able to log into GitHub. We will go over three different ways to make repositories: starting from scratch with a new repository, creating a repository for an already started project, and accessing a repository made by a collaborator.\n\nNew Repository\nTo create a brand new repository, click on the plus sign near your profile picture in the top right corner of GitHub and select New repository.\n\nOn the next page, type in a name for your repository and choose whether you want it to be public or private. It is recommended you select the box to create a README file. This will give you a place to describe the layout of your repository and the purpose of each file. Without a README file, visitors to your repository (and maybe future you) might not be able to figure out how to properly use the files in your repository, leading to your working being non-reproducible.\n\nNow you can click Create Repository.\nOnce your repository is created, you will see only the README file is present. To create other files, let’s create a directory for this repo on our computer.\n\nIn R studio, follow the instructions for creating a new project, but when you see the following menu, select Version Control this time.\n\n\n\n\nOn the next window, select Git.\nReturn to the GitHub page for the repository you created and select the green Code button.\nCopy the HTTPS link from the menu that pops up.\n\n\n\nReturn to Rstudio and paste the link in the Repository URL line at the top of the popped up window. Make sure to check the file path that is currently set and use the Browse button if you want to save the folder for this directory in a different place.\n\n\nYour repository is now set up in a new project in Rstudio, and you can begin by creating an Rmarkdown, R Script, etc.\nNote, you can edit the contents of the README file by opening it in Rstudio.\n\n\nFrom an Existing Project\nYou may find yourself wanting to create a repo on GitHub for a project you have already started working on. Fortunately, it is easy to start version control tracking on a project and add the project to a remote repository.\nHere are the steps to follow if you have a project folder with a .Rproj file in it and your directory is not already being tracked with Git:\n\nFollow the steps above for creating a New repository, but this time do not click the box to create a README file. This will bring you to a page with a “Quick Set Up” link. Keep this page open for later.\n\n\n\n\nGo to RStudio and open the .Rproj file that will be added to the GitHub repository./\nSelect the Terminal tab next to Console and enter the following lines of code.\n\n\n\nCode\n$ git init -b main\n\n\n“git” tells bash what program we want to use   “init” tells bash to initialize a Git repository fir this directory\n“-b main” is saying we want to create a branch called “main”\n\n\n\nCode\n$ git add .\n\n\nThis will add all of the files within the current folder to the repository. You may get a lot of warnings because you have a .Rproj file and potentially a .Rhistory file in this folder which are not usually ideal to track. We will take care of this by adding these file names to the .gitignore file shortly.\n\n\nCode\n$ git commit -m \"First commit\"\n\n\nThis line creates your first commit. You can change the commit message to anything that makes sense to you.\n\nGo back to GitHub and copy the Quick start link. Type the following code in the terminal, but replace &lt;REMOTE_URL&gt; with the copied link.\n\n\n\nCode\n$ git remote add origin &lt;REMOTE_URL&gt;\n# Replace \"&lt;REMOTE_URL&gt;\" with url from GitHub\n\n\n\nLast, we will push these changes to GitHub. We will further discuss what this means later, but for now run the following line in the terminal.\n\n\n\nCode\n$ git push origin main\n\n\nNow when you return to GitHub and refresh your repository page, you should see all of the files from your existing project.\nYou can add a README and .gitignore on the GitHub website by selecting “Add file” on the repository’s page. When you make a .gitignore file, it may suggest you use a template, in which case, select the R template from the drop down list and it will automatically fill the document with file types that should typically not be tracked.\n\n\nCloning a Repository\nIf you want to join a repository that has already been created, you can either find the repository by searching for it on GitHub (if it is publicly available) or contact the creator and have them add you as a collaborator to the repository. Either way, you will follow steps 1-5 in the “New Repository” section above, but this time the link is coming from the already created repository.\nIn cases where the repository is not accessible (you do not have cloning priveleges), you will have to create a pull request.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#pushing-and-pulling",
    "href": "lesson/getting-setup.html#pushing-and-pulling",
    "title": "Getting Set Up",
    "section": "“Pushing” and “Pulling”",
    "text": "“Pushing” and “Pulling”\nOnce you have your remote repository ready, you can make changes to the files and “add” and “commit” them like we did in the “Version Control” section above. However, now we must take steps to make sure our local version of the repository is up-to-date with the online version, and the versions that all other collaborators have on their computers.\nTo do this, we will need to “push” and “pull”. “Pulling” is when we bring recent and out-of-sync changes from the online version to our local device. “Pushing” is the opposite; we are sending our commits to the online version to update it with our recent changes. This will allow anyone else working in the repository to “pull” your changes onto their computer.\nTo push and pull, use the blue and green arrows on the Git tab in R studio.\n\nYou may also notice the next time you “commit” your changes, these same push and pull buttons are located above the “Commit Message” box on the pop up window.\nIt is good practice to “pull” each time you are about to start working in a repository and to push after you commit, or at least once at the end of your working period within a repository for the day. Keeping good “push” and “pull” habits will help you avoid merge conflicts with collaborators or yourself if you work on a project on more than one computer.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "href": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "title": "Getting Set Up",
    "section": "Adding and Managing Collaborators",
    "text": "Adding and Managing Collaborators\nCollaborative coding is a huge benefit of GitHub. In order to invite your collaborators to clone your remote repository, you will need to know their GitHub username, or at least their email address.\n\nOn your repository page on GitHub, select Settings in the middle of the banner near the top of the page.\n\n\n\nIn the menu on the left, select the Collaborators page.\n\n\n\nEnter your password, and then select Add People under Manage Access\n\nAdd your collaborators one at a time. This will send them a message inviting them to join the repository.\nAs the owner of the repository, you will be able to remove people from the repository at any time.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#merge-conflicts",
    "href": "lesson/getting-setup.html#merge-conflicts",
    "title": "Getting Set Up",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts\nWhen two people are working on a branch at the same time, changes were made and not pushed before someone else started working, or a collaborator forgets to pull before starting to make changes, a merge conflict may arise. Merge conflicts happen because Git is not sure how to combine the different changes that occurred in the same sections of the document. In some instances, Git is smart enough to figure it out and will merge the versions on its own. Other times, the merge conflict will be need to be fixed manually. Note that Git will not allow the push until the merge conflict is solved.\nIf a merge conflict occurs, you will see some specific things added to your code. There will be a line at the beginning that starts with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, a line at the end starting with &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a line in the middle that just has =======. The two different version are shown above and below the middle line, labeled with which branch has which version. It is your job to decide how these versions fit together. Once you have modified the code to match the final version you want to keep, you can delete the three lines of code containing the &gt;, &lt; and = symbols and “stage” and “commit” your files as usual.\n\n\nOn GitHub\nYou can also merge branches and resolve merge conflicts on GitHub. You would push your branch to GitHub, then go to the GitHub repo webpage. You will need to create a pull request to merge your branch. If there is a merge conflict, you will need to click on “Resolve conflicts” before you can merge your branch. The syntax to edit the document and resolve the conflict is the same as above.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "href": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "title": "Getting Set Up",
    "section": "Cloning, Branching, and Forking, Oh My!",
    "text": "Cloning, Branching, and Forking, Oh My!\nCloning, branching, and forking are functions that are similar, but they are not the same. When you clone a repository, you are connected to it and are working in that repository. You can commit changes and push them to the same repository. When you create a branch, you create a copy where you can work on a specific part of a document or run test code with the intent to merge it back to the main branch. Many branches are short-lived and deleted once their purpose has been served. Anyone that has access to the repository also has access to the branches in it. A fork creates a copy of the entire repository as well, however, the collaborators are disconnected from it. The intent is generally to diverge from the original repository and never be merged back into it.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#gitignore-and-large-files",
    "href": "lesson/getting-setup.html#gitignore-and-large-files",
    "title": "Getting Set Up",
    "section": ".gitignore and Large Files",
    "text": ".gitignore and Large Files\nGitHub does not allow repositories to be larger than 5GB. While we recommend keeping all of the files necessary to run your analysis together in the repo, this may not be possible if say for example your data files are larger than the size limit. If this is the case, you can manually distribute your data file a different way. (See https://git-lfs.github.com/ first though if this is an issue you are actually having.)\nOnce all collaborators (or just you) have the large data file in your Git repo, you may forget that you cannot send this to GitHub and accidentally try to commit and push it. Fortunately, Git will recognize the problem before it takes place and will give you the following warning:\n\nTo resolve this problem, you just need to tell Git to ignore the large file when you make your commit. First, determine which file(s) is/are too big by looking at their size. Then, navigate to your .gitignore document and open it. Last, add the name(s) of the file(s) that is/are too large. Now you will be able to commit and push the tracked files within your repository.\nYou can add any files you do not want to be tracked to .gitignore. An example is the html file generated by rendering this document. It is regenerated constantly, so there is no need to track the changes.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#amend-commits",
    "href": "lesson/getting-setup.html#amend-commits",
    "title": "Getting Set Up",
    "section": "Amend Commits",
    "text": "Amend Commits\nIf at any point you find you have made a commit that should not have been made (e.x. you accidentally added and committed files that are too large and now you aren’t able to push), you can easily fix this by making the necessary changes within your files, checking the “amend previous commit” box on the commit screen, and then committing as usual. This will overwrite your previous commit with the correct version you want to commit. If you need to amend an earlier commit (i.e. not the most recent commit), you will need to use the terminal (See Oh S#!*, Git!?!).",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "lesson/getting-setup.html#installing-a-git-client",
    "href": "lesson/getting-setup.html#installing-a-git-client",
    "title": "Getting Set Up",
    "section": "Installing a Git Client",
    "text": "Installing a Git Client\nWith all of the commits, branching, merging, and collaboration, it can be tricky to keep track of everything going on in your repository. Viewing the commit history in GitHub or under the Git tab in RStudio is useful, though it can still be a little hard to follow. Using a Git client software helps visualize the workflow and allows you to use commands in the graphic user interface (GUI) that you would normally need to type into the terminal.\n\nThere are a few Git Clients out there and you may want to try a few to see what works for you. I use GitKraken which you can download here.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Setup"
    ]
  },
  {
    "objectID": "example/session-10-example.html",
    "href": "example/session-10-example.html",
    "title": "Session 10 Code",
    "section": "",
    "text": "Load libraries\n\n\nCode\nlibrary(sf)\nlibrary(tidyverse)\n\n\n\n\nCentroids vs Point on Surface\n\n\nCode\nid.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE)\nid.centroid &lt;- st_centroid(id.counties)\nid.pointonsurf &lt;- st_point_on_surface(id.counties)\n\n\n\n\nCode\nplot(st_geometry(id.counties))\nplot(st_geometry(id.centroid), col=\"blue\", add=TRUE)\nplot(st_geometry(id.pointonsurf), col=\"red\", add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nPractice Example 1: Distance on Points and Polygons\n\n\nCode\nsystem.time(poly_dist &lt;- st_distance(id.counties))\n\n\n   user  system elapsed \n   4.32    0.03    4.36 \n\n\nCode\nsystem.time(cent_dist &lt;- st_distance(id.centroid))\n\n\n   user  system elapsed \n   0.00    0.00    0.01 \n\n\nCode\nsystem.time(pos_dist &lt;- st_distance(id.pointonsurf))\n\n\n   user  system elapsed \n   0.02    0.00    0.00 \n\n\n\n\nPractice Example 2: Intersections and Buffers\n\n\nCode\n# get roads data\nroads &lt;- tigris::primary_secondary_roads(\"ID\", progress_bar=FALSE)\n\n# get a polygon of Ada county\nada.cty &lt;- filter(id.counties, NAME == \"Ada\")\n\n# find all road sections within Ada county\nada.roads &lt;- st_intersection(roads, ada.cty)\n\n\n\n\nCode\n# plot result\nplot(st_geometry(ada.cty))\nplot(st_geometry(ada.roads), col=\"purple\", add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the units of the CRS\nst_crs(id.centroid)\n\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nCode\n# create a 50km buffer around the centroid of Ada county\nada.cent &lt;- filter(id.centroid, NAME == \"Ada\")\nada.buff &lt;- st_buffer(ada.cent, dist = 50000)\n\n\n\n\nCode\n# get roads within buffer zone\nroads.buff &lt;- st_intersection(roads, ada.buff)\n\n\n\n\nCode\n# plot result\nplot(st_geometry(roads.buff))\n\n\n\n\n\n\n\n\n\nIf we want to plot county boundaries, st_intersection won’t work. We’ll only get parts of each county polygon:\n\n\nCode\ncty.buff &lt;- st_intersection(id.counties, ada.buff)\nplot(st_geometry(cty.buff))\n\n\n\n\n\n\n\n\n\nBack to predicates!\n\n\nCode\n# find counties that intersect with buffer\ncty.50 &lt;- st_intersects(id.counties, ada.buff, sparse = FALSE)\n# convert to vector so filter is happy\ncty.50 &lt;- as.vector(cty.50)\n\n# get counties that intersect buffer\nmap.counties &lt;- filter(id.counties, cty.50)\n\n\n\n\nCode\n# plot result\nplot(map.counties$geometry)\nplot(st_geometry(roads.buff), col=\"purple\", add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations I"
    ]
  },
  {
    "objectID": "slides/13-slides.html#objectives",
    "href": "slides/13-slides.html#objectives",
    "title": "Raster Data: II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "slides/13-slides.html#why-use-moving-windows",
    "href": "slides/13-slides.html#why-use-moving-windows",
    "title": "Raster Data: II",
    "section": "Why use moving windows?",
    "text": "Why use moving windows?\n\nTo create new data that reflects “neighborhood” data\nTo smooth out values\nTo detect (and fill) holes or edges\nChange the thematic scale of your data (without changing resolution)"
  },
  {
    "objectID": "slides/13-slides.html#what-is-a-moving-window",
    "href": "slides/13-slides.html#what-is-a-moving-window",
    "title": "Raster Data: II",
    "section": "What is a moving window?",
    "text": "What is a moving window?"
  },
  {
    "objectID": "slides/13-slides.html#implementing-moving-windows-in-r",
    "href": "slides/13-slides.html#implementing-moving-windows-in-r",
    "title": "Raster Data: II",
    "section": "Implementing Moving Windows in R",
    "text": "Implementing Moving Windows in R\n\nUse the focal function in terra\n\nfocal(x, w=3, fun=\"sum\", ..., na.policy=\"all\", fillvalue=NA,          expand=FALSE, silent=TRUE, filename=\"\", overwrite=FALSE, wopt=list())"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters",
    "href": "slides/13-slides.html#focal-for-continuous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(spData)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-1",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-1",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-2",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-2",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continuous-rasters-3",
    "href": "slides/13-slides.html#focal-for-continuous-rasters-3",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/13-slides.html#focal-for-continous-rasters",
    "href": "slides/13-slides.html#focal-for-continous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continous Rasters",
    "text": "focal for Continous Rasters\n\ncan alter the size and shape of window by providing a weights matrix for w\nCan create different custom functions for fun (see the help file)\nna.policy for filling holes or avoiding them"
  },
  {
    "objectID": "slides/13-slides.html#reclassification-1",
    "href": "slides/13-slides.html#reclassification-1",
    "title": "Raster Data: II",
    "section": "Reclassification",
    "text": "Reclassification\n\nCreate new data based on the presence of a particular class(es) of interest\nCombine classes in a categorical map\nUseful as inputs for overlay analyses"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-rasters-in-r",
    "href": "slides/13-slides.html#reclassifying-rasters-in-r",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-rasters-in-r-1",
    "href": "slides/13-slides.html#reclassifying-rasters-in-r-1",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-categorical-rasters",
    "href": "slides/13-slides.html#reclassifying-categorical-rasters",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters\n\nNeed a classification matrix\nUse classify\n\n\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats"
  },
  {
    "objectID": "slides/13-slides.html#reclassifying-categorical-rasters-1",
    "href": "slides/13-slides.html#reclassifying-categorical-rasters-1",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters"
  },
  {
    "objectID": "slides/13-slides.html#raster-math",
    "href": "slides/13-slides.html#raster-math",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nPerforms cell-wise calculations on 1 (or more) SpatRasters\nGenerally works the same as matrix operations\nAll layers must be aligned"
  },
  {
    "objectID": "slides/13-slides.html#raster-math-1",
    "href": "slides/13-slides.html#raster-math-1",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nr &lt;- rast(ncol=5, nrow=5)\nvalues(r) &lt;- 1:ncell(r)\nr2 &lt;- r*2\nr3 &lt;- t(r)\nr4 &lt;- r + r2"
  },
  {
    "objectID": "slides/13-slides.html#cell-wise-operations",
    "href": "slides/13-slides.html#cell-wise-operations",
    "title": "Raster Data: II",
    "section": "Cell-wise operations",
    "text": "Cell-wise operations\n\nterra has a special set of apply functions\napp, lapp, tapp\napp applies a function to the values of each cell\nlapp applies a function using the layer as the value\ntapp applies the function to a subset of layers"
  },
  {
    "objectID": "slides/13-slides.html#context-specific-functions",
    "href": "slides/13-slides.html#context-specific-functions",
    "title": "Raster Data: II",
    "section": "Context-specific Functions",
    "text": "Context-specific Functions\n\ndistance and relatives are based on relationships between cells\nterrain allows calculation of slope, ruggedness, aspect using elevation rasters\nshade calculates hillshade based on terrain"
  },
  {
    "objectID": "slides/13-slides.html#practice",
    "href": "slides/13-slides.html#practice",
    "title": "Raster Data: II",
    "section": "Practice",
    "text": "Practice\nYou are tasked with the following (admittedly silly) analysis by the “Wildfire for Insurance Agents” group. They would like a map of wildfire risk in Idaho with these categories: “No worries,” “A little bit risky,” “Moderate risk,” and “Very risky,” and “Don’t move here.” They don’t want any stakeholders to feel that their properties stand out unfairly, so they don’t want pixels that are different from the pixels around them. Choose a county to present this analysis to and make a map for that audience as well as the state map."
  },
  {
    "objectID": "slides/13-slides.html#objectives-1",
    "href": "slides/13-slides.html#objectives-1",
    "title": "Raster Data: II",
    "section": "Objectives",
    "text": "Objectives\n\nYou should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "example/session-22-example.html",
    "href": "example/session-22-example.html",
    "title": "Session 22 code",
    "section": "",
    "text": "Load libraries:\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(tree)\nlibrary(randomForest)\n\n\nLoad data:\n\n\nCode\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)\n\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\", quiet=TRUE) %&gt;%\n  filter(!st_is_empty(.))\n\n\nCheck validity:\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] FALSE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] FALSE\n\n\nCode\nfs.bdry &lt;- st_make_valid(fs.bdry)\ncflrp.bdry &lt;- st_make_valid(cflrp.bdry)\n\n\nCheck alignment:\n\n\nCode\nst_crs(wildfire_haz) == st_crs(fs.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cflrp.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cejst)\n\n\n[1] FALSE\n\n\nCode\nfs.bdry_proj &lt;- st_transform(fs.bdry, crs = st_crs(wildfire_haz))\ncflrp.bdry_proj &lt;- st_transform(cflrp.bdry, crs = st_crs(wildfire_haz))\ncejst_proj &lt;- st_transform(cejst, crs = st_crs(wildfire_haz))\n\n\nSubset to relevant geographies:\n\n\nCode\nfs.bdry_sub &lt;- fs.bdry_proj[cejst_proj, ]\ncflrp.bdry_sub &lt;- cflrp.bdry_proj[cejst_proj, ]\n\ncejst_sub &lt;- cejst_proj[fs.bdry_sub, ]\n\n\nSelect relevant attributes:\n\n\nCode\ncejst_sub &lt;- cejst_sub %&gt;%\n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\n\n\nExtract wildfire risk:\n\n\nCode\nwf_risk &lt;- terra::extract(wildfire_haz, cejst_sub, fun=mean)\n\ncejst_sub$WHP_ID &lt;- wf_risk$WHP_ID\n\n\nCFLRP T or F:\n\n\nCode\ncflrp &lt;- apply(st_intersects(cejst_sub, cflrp.bdry_sub, sparse = FALSE), 1, any)\n\ncejst_sub$CFLRP &lt;- cflrp",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Classification Trees"
    ]
  },
  {
    "objectID": "example/session-22-example.html#review-from-session-16",
    "href": "example/session-22-example.html#review-from-session-16",
    "title": "Session 22 code",
    "section": "",
    "text": "Load libraries:\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(tree)\nlibrary(randomForest)\n\n\nLoad data:\n\n\nCode\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)\n\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\", quiet=TRUE) %&gt;%\n  filter(!st_is_empty(.))\n\n\nCheck validity:\n\n\nCode\nall(st_is_valid(fs.bdry))\n\n\n[1] FALSE\n\n\nCode\nall(st_is_valid(cflrp.bdry))\n\n\n[1] FALSE\n\n\nCode\nfs.bdry &lt;- st_make_valid(fs.bdry)\ncflrp.bdry &lt;- st_make_valid(cflrp.bdry)\n\n\nCheck alignment:\n\n\nCode\nst_crs(wildfire_haz) == st_crs(fs.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cflrp.bdry)\n\n\n[1] FALSE\n\n\nCode\nst_crs(wildfire_haz) == st_crs(cejst)\n\n\n[1] FALSE\n\n\nCode\nfs.bdry_proj &lt;- st_transform(fs.bdry, crs = st_crs(wildfire_haz))\ncflrp.bdry_proj &lt;- st_transform(cflrp.bdry, crs = st_crs(wildfire_haz))\ncejst_proj &lt;- st_transform(cejst, crs = st_crs(wildfire_haz))\n\n\nSubset to relevant geographies:\n\n\nCode\nfs.bdry_sub &lt;- fs.bdry_proj[cejst_proj, ]\ncflrp.bdry_sub &lt;- cflrp.bdry_proj[cejst_proj, ]\n\ncejst_sub &lt;- cejst_proj[fs.bdry_sub, ]\n\n\nSelect relevant attributes:\n\n\nCode\ncejst_sub &lt;- cejst_sub %&gt;%\n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\n\n\nExtract wildfire risk:\n\n\nCode\nwf_risk &lt;- terra::extract(wildfire_haz, cejst_sub, fun=mean)\n\ncejst_sub$WHP_ID &lt;- wf_risk$WHP_ID\n\n\nCFLRP T or F:\n\n\nCode\ncflrp &lt;- apply(st_intersects(cejst_sub, cflrp.bdry_sub, sparse = FALSE), 1, any)\n\ncejst_sub$CFLRP &lt;- cflrp",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Classification Trees"
    ]
  },
  {
    "objectID": "example/session-22-example.html#comparingpredicting-cflrp-tracts",
    "href": "example/session-22-example.html#comparingpredicting-cflrp-tracts",
    "title": "Session 22 code",
    "section": "Comparing/Predicting CFLRP tracts",
    "text": "Comparing/Predicting CFLRP tracts\n\nData preparation\n\n\nCode\ncejst_mod &lt;- cejst_sub %&gt;%\n  st_drop_geometry(.) %&gt;%\n  na.omit(.)\n\ncejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")] &lt;- scale(cejst_mod[, c(\"LMI_PFS\", \"LHE\", \"HBF_PFS\", \"WHP_ID\")])\n\n\n\n\nLogistic regression\n\n\nCode\nlogistic.global &lt;- glm(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID,\n                       family = binomial(link = \"logit\"),\n                       data = cejst_mod)\nsummary(logistic.global)\n\n\n\nCall:\nglm(formula = CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, family = binomial(link = \"logit\"), \n    data = cejst_mod)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.66462    0.14347  -4.632 3.62e-06 ***\nLMI_PFS      0.17284    0.16881   1.024    0.306    \nLHE         -0.25551    0.15791  -1.618    0.106    \nHBF_PFS     -0.01511    0.16081  -0.094    0.925    \nWHP_ID       0.88902    0.16785   5.297 1.18e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 330.74  on 255  degrees of freedom\nResidual deviance: 290.03  on 251  degrees of freedom\nAIC: 300.03\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nClassification Tree\n\n\nCode\nlibrary(tree)\ncejst_mod$CFLRP &lt;- as.factor(ifelse(cejst_mod$CFLRP == 1, \"Yes\", \"No\"))\ntree.model &lt;- tree(CFLRP ~ LMI_PFS + LHE + HBF_PFS + WHP_ID, cejst_mod)\nplot(tree.model, type = \"uniform\")\ntext(tree.model, pretty=0)\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\nCode\nlibrary(randomForest)\nclass.model &lt;- CFLRP ~ .\nrf2 &lt;- randomForest(formula = class.model, cejst_mod[,-1])\nvarImpPlot(rf2)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Classification Trees"
    ]
  },
  {
    "objectID": "slides/04-slides.html#objectives",
    "href": "slides/04-slides.html#objectives",
    "title": "Reading Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\n\nRevisit the components of spatial data\nDescribe some of the key considerations for thinking about spatial data\nIntroduce the two primary R packages for spatial workflows\nLearn to read and explore spatial objects in R"
  },
  {
    "objectID": "slides/04-slides.html#questions-from-monday",
    "href": "slides/04-slides.html#questions-from-monday",
    "title": "Reading Spatial Data in R",
    "section": "Questions from Monday",
    "text": "Questions from Monday\n\nWhy do we need a projection for calculations on a computer?\nWhat does it mean that a raster’s geometry is implicit?"
  },
  {
    "objectID": "slides/04-slides.html#reviewing-spatial-data",
    "href": "slides/04-slides.html#reviewing-spatial-data",
    "title": "Reading Spatial Data in R",
    "section": "Reviewing Spatial Data",
    "text": "Reviewing Spatial Data\nLet’s Kahoot!\nhttps://create.kahoot.it/share/isdr-session-4/888711f4-50a3-4732-a707-cbf68d9ae9dc"
  },
  {
    "objectID": "slides/04-slides.html#data-types-and-r-packages",
    "href": "slides/04-slides.html#data-types-and-r-packages",
    "title": "Reading Spatial Data in R",
    "section": "Data Types and R Packages",
    "text": "Data Types and R Packages\n\n\nData Types\n\nVector Data\n\nPoint features\nLine features\nArea features (polygons)\n\nRaster Data\n\nSpatially continuous field\nBased on pixels (not points)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "href": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: spreadsheets",
    "text": "Reading in Spatial Data: spreadsheets\n\n\nMost basic form of spatial data\nNeed x (longitude) and y (latitude) as columns\nNeed to know your CRS\nread_*** necessary to bring in the data\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\nfile.to.read &lt;- read_csv(file = \"path/to/your/file\", \n                         col_names = TRUE, col_types = NULL, \n                         na =na = c(\"\", \"NA\"))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, \n                       coords = c(\"longitude\", \"latitude\"), \n                       crs=4326)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\nALL FILES NEED TO BE IN THE SAME FOLDER\n\n\n\n\n\n.shp is the shapefile itself\n.prj contains the CRS information\n.dbf contains the attributes\n.shx contains the indices for matching attributes to geometries\nother extensions contain metadata\n\n\n\n\n\nst_read and read_sf in the sf package will read shapefiles into R\nread_sf leaves character vectors alone (often beneficial)\nst_read can handle other datatypes (like geodatabases)\nReturns slightly different classes"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\n\n\nlibrary(sf)\nshapefile.inR &lt;- read_sf(dsn = \"path/to/file.shp\")"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "href": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: rasters",
    "text": "Reading in Spatial Data: rasters\n\nrast will read rasters using the terra package\nAlso used to create rasters from scratch\nReturns SpatRaster object\n\n\n\n\nlibrary(terra)\nraster.inR &lt;- rast(x = \"path/to/file.tif\", \n                         lyrs=NULL)"
  },
  {
    "objectID": "slides/04-slides.html#introducing-the-data",
    "href": "slides/04-slides.html#introducing-the-data",
    "title": "Reading Spatial Data in R",
    "section": "Introducing the Data",
    "text": "Introducing the Data\n\nGood idea to get to know your data before manipulating it\nstr, summary, nrow, ncol are good places to start\nst_crs (for sf class objects) and crs (for SpatRaster objects)\nWe’ll practice a few of these now…"
  },
  {
    "objectID": "slides/04-slides.html#saving-your-data",
    "href": "slides/04-slides.html#saving-your-data",
    "title": "Reading Spatial Data in R",
    "section": "Saving your data",
    "text": "Saving your data\n\nwrite_sf for sf objects; writeRaster for SpatRasters\n\n\nlibrary(sf)\nlibrary(terra)\n\nwrite_sf(object = object.to.save, dsn = \"path/to/save/object\", append = FALSE)\nwriteRaster(x=object, filename = \"path/to/save\")"
  },
  {
    "objectID": "example/session-21-example.html",
    "href": "example/session-21-example.html",
    "title": "Session 21 code",
    "section": "",
    "text": "Load libraries:\nCode\nlibrary(terra)\nlibrary(spDataLarge)\nlibrary(sf)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Overlays and Logistic Regression"
    ]
  },
  {
    "objectID": "example/session-21-example.html#overlays",
    "href": "example/session-21-example.html#overlays",
    "title": "Session 21 code",
    "section": "Overlays",
    "text": "Overlays\nGet land cover data:\n\n\nCode\nnlcd &lt;-  rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\nplot(nlcd)\n\n\n\n\n\n\n\n\n\nSeparate categorical raster into Boolean layers:\n\n\nCode\nnlcd.segments &lt;- segregate(nlcd)\n# rename layers of raster stack\nnames(nlcd.segments) &lt;- levels(nlcd)[[1]][-1,2]\nplot(nlcd.segments)\n\n\n\n\n\n\n\n\n\nGet slope data:\n\n\nCode\n# elevation data\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n# get slope\nslope &lt;- terrain(srtm, v = \"slope\")\n\n\nPrepare overlay layers:\n\n\nCode\n# only slopes less than 10 are suitable\nsuit.slope &lt;- slope &lt; 10\n# only shrubland is suitable\nsuit.landcov &lt;- nlcd.segments[\"Shrubland\"]\n# make sure raster layers align\nsuit.slope.match &lt;- project(suit.slope, suit.landcov)\n\n\nRun overlay analysis:\n\n\nCode\nsuit &lt;- suit.slope.match + suit.landcov\n\nplot(suit)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Overlays and Logistic Regression"
    ]
  },
  {
    "objectID": "example/session-21-example.html#logistic-regression",
    "href": "example/session-21-example.html#logistic-regression",
    "title": "Session 21 code",
    "section": "Logistic regression",
    "text": "Logistic regression\nGet data:\n\n\nCode\n# get presence-absence simulated data\npresabs &lt;- st_read(\"/opt/data/data/presabsexample/presenceabsence.shp\", quiet = TRUE)\n\n# get predictor data file paths\npreds.list &lt;- list.files(\"/opt/data/data/presabsexample\", \"grd$\", full.names = TRUE)\n# get predictors as raster stack\npred.stack &lt;- rast(preds.list)\n# rename layers of raster stack\nnames(pred.stack) &lt;- c(\"MeanAnnTemp\", \"TotalPrecip\", \"PrecipWetQuarter\", \"PrecipDryQuarter\", \"MinTempCold\", \"TempRange\")\n\n\nExtract predictor data at each point and add presence/absence column called y:\n\n\nCode\npts.df &lt;- terra::extract(pred.stack, presabs)\npts.df[,2:7] &lt;- scale(pts.df[,2:7])\n\npts.df$y &lt;- presabs$y\n\n\nFit logistic regression model:\n\n\nCode\nlogistic.carolyn &lt;- glm(y ~ TempRange + PrecipDryQuarter, \n                        family = binomial(link = \"logit\"),\n                        data = pts.df)\n\nsummary(logistic.carolyn)\n\n\n\nCall:\nglm(formula = y ~ TempRange + PrecipDryQuarter, family = binomial(link = \"logit\"), \n    data = pts.df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.2820     0.3789  -3.384 0.000715 ***\nTempRange         -3.5156     0.7984  -4.403 1.07e-05 ***\nPrecipDryQuarter   0.4157     0.5938   0.700 0.483969    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 125.37  on 99  degrees of freedom\nResidual deviance:  59.18  on 97  degrees of freedom\nAIC: 65.18\n\nNumber of Fisher Scoring iterations: 6\n\n\nPredict model results across entire study area:\n\n\nCode\nnewpreds &lt;- predict(pred.stack, logistic.carolyn, type = \"response\")\nplot(newpreds)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Overlays and Logistic Regression"
    ]
  },
  {
    "objectID": "example/session-18-example.html",
    "href": "example/session-18-example.html",
    "title": "Session 18 code",
    "section": "",
    "text": "Load libraries:\nCode\nlibrary(terra)\nlibrary(spdep)\nlibrary(tidyverse)\nlibrary(gstat)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation I"
    ]
  },
  {
    "objectID": "example/session-18-example.html#stochastic-process",
    "href": "example/session-18-example.html#stochastic-process",
    "title": "Session 18 code",
    "section": "Stochastic Process",
    "text": "Stochastic Process\nThe deterministic \\(z = 2x + 3y\\) process becomes stochasic with the addition of the \\(d\\) term: \\(z = 2x + 3y + d\\). In the example below, \\(d\\) is pulled from a uniform distribution with range -50 to 50 and results in six very different simulated results.\n\n\nCode\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\n\nfun &lt;- function(z){\n  a &lt;- z\n  d &lt;- runif(ncell(z), -50, 50)\n  values(a) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2] + d\n  return(a)\n}\n\nb &lt;- replicate(n=6, fun(z=x), simplify=FALSE)\nd &lt;- do.call(c, b)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation I"
    ]
  },
  {
    "objectID": "example/session-18-example.html#morans-i",
    "href": "example/session-18-example.html#morans-i",
    "title": "Session 18 code",
    "section": "Moran’s I",
    "text": "Moran’s I\n\n\nCode\nset.seed(2354)\n# Load the shapefile\ns &lt;- readRDS(url(\"https://github.com/mgimond/Data/raw/gh-pages/Exercises/fl_hr80.rds\"))\n\n# Define the neighbors (use queen case)\nnb &lt;- poly2nb(s, queen=TRUE)\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nCode\n# Compute the neighboring average homicide rates\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n#estimate Moran's I\nmoran.test(s$HR80,lw, alternative=\"greater\")\n\n\n\n    Moran I test under randomisation\n\ndata:  s$HR80  \nweights: lw    \n\nMoran I statistic standard deviate = 1.8891, p-value = 0.02944\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.136277593      -0.015151515       0.006425761",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation I"
    ]
  },
  {
    "objectID": "example/session-18-example.html#nearest-neighbor-interpolation",
    "href": "example/session-18-example.html#nearest-neighbor-interpolation",
    "title": "Session 18 code",
    "section": "Nearest Neighbor Interpolation",
    "text": "Nearest Neighbor Interpolation\nEach cell takes the value of its nearest neighbor point.\n\n\nCode\n# data retrieved from https://www.epa.gov/outdoor-air-quality-data/download-daily-data\n\naq &lt;- read_csv(\"/opt/data/data/classexamples/ad_viz_plotval_data_PM25_2024_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"Site Longitude\", \"Site Latitude\"), crs = \"EPSG:4326\") %&gt;% \n  st_transform(., crs = \"EPSG:8826\") %&gt;% \n  mutate(date = as_date(parse_datetime(Date, \"%m/%d/%Y\"))) %&gt;% \n  filter(., date &gt;= 2024-07-01) %&gt;% \n  filter(., date &gt; \"2024-07-01\" & date &lt; \"2024-07-31\")\naq.sum &lt;- aq %&gt;% \n  group_by(., `Site ID`) %&gt;% \n  summarise(., meanpm25 = mean(`Daily AQI Value`))\n\nnodes &lt;- st_make_grid(aq.sum,\n                      what = \"centers\")\n\ndist &lt;- distance(vect(nodes), vect(aq.sum))\nnearest &lt;- apply(dist, 1, function(x) which(x == min(x)))\naq.nn &lt;- aq.sum$meanpm25[nearest]\npreds &lt;- st_as_sf(nodes)\npreds$aq &lt;- aq.nn\n\npreds &lt;- as(preds, \"Spatial\")\nsp::gridded(preds) &lt;- TRUE\npreds.rast &lt;- rast(preds)\n\n\n\n\nCode\nnodes_bounds &lt;- st_make_grid(aq.sum,\n                      what = \"polygons\")\n\nplot(preds.rast)\nplot(st_geometry(aq.sum), add=TRUE)\nplot(nodes_bounds, add=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation I"
    ]
  },
  {
    "objectID": "example/session-18-example.html#inverse-distance-weighting",
    "href": "example/session-18-example.html#inverse-distance-weighting",
    "title": "Session 18 code",
    "section": "Inverse Distance Weighting",
    "text": "Inverse Distance Weighting\n\n\nCode\nmgsf05 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 0.5))\nmgsf2 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 2))\n\ninterpolate_gstat &lt;- function(model, x, crs, ...) {\n    v &lt;- st_as_sf(x, coords=c(\"x\", \"y\"), crs=crs)\n    p &lt;- predict(model, v, ...)\n    as.data.frame(p)[,1:2]\n}\n\nzsf05 &lt;- interpolate(preds.rast, mgsf05, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)\nzsf2 &lt;- interpolate(preds.rast, mgsf2, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(zsf05, main=\"idp=0.5\")\nplot(zsf2, main=\"idp=2\")\n\n\n\n\n\n\n\n\n\npersp can help us visualize smoothness by creating a 3D representation of a raster.\n\n\nCode\npar(mfrow=c(1,2))\npersp(zsf05,box=FALSE, main=\"idp=0.5\")\npersp(zsf2, box=FALSE,main=\"idp=2\")\n\n\n\n\n\n\n\n\n\nWe assessed which raster cells were the most different between the two models.\n\n\nCode\ntest &lt;- zsf05 - zsf2\nplot(test)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation I"
    ]
  },
  {
    "objectID": "assignment/12-vissolutions.html",
    "href": "assignment/12-vissolutions.html",
    "title": "Assignment 10 Solutions: Data Visualization",
    "section": "",
    "text": "Get data to visualize.\n\nI chose to visualize Venus flytrap locations and soil nitrogen.\n\n\n# load libraries\nlibrary(rgbif)\nlibrary(geodata)\n\nLoading required package: terra\n\n\nterra 1.7.78\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(terra)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidycensus)\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\n\n# load data\n# get a selection of 1000 Venus flytrap observations from rgbif\nvflytrap_us &lt;- occ_search(scientificName = \"Dionaea muscipula\", \n                             country = \"US\",\n                             hasCoordinate = TRUE,\n                             limit=1000)\n\n# geodata raster\nsoils &lt;- geodata::soil_world(var = \"nitrogen\", depth=5, path=tempfile())\n\n# boundaries\nstate_boundaries &lt;- geodata::gadm(country = \"USA\", level=1, path=tempfile())\n\nWrite pseudocode for how you will prepare your data for visualization, then execute your plan. Some possible objectives might be cropping your data to an area of interest and transforming the data to tidy format.\n\nThere are a few necessary steps before I can plot. First, all vector data needs to be an sf object. rgbif returns a non-spatial dataframe, so I need to convert that to a spatial object. Next, I know Venus flytraps are endemic to just a few US states, so I will use spatial cropping tools to focus all my datasets to that area of interest. Finally, I convert my raster data to a dataframe. I always make this my last step before plotting, as I cannot perform any more spatial operations on the non-spatial dataframe.\n\n\n1. Convert Venus flytrap data to spatial object\n2. Convert state boundaries from SpatVector to sf object\n3. Filter state boundaries to only include North and South Carolina\n4. Subset Venus flytrap data to those states\n5. Crop soils raster to those states\n6. Convert soils raster to data frame\n\n\n# Convert Venus flytrap data to spatial object\nvflytrap_us &lt;- vflytrap_us$data\nvflytrap_dat_sf &lt;- vflytrap_us %&gt;%\n  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude)) %&gt;%\n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\n\n# Convert state boundaries from SpatVector to sf object\naoi &lt;- st_as_sf(state_boundaries) %&gt;%\n  # Filter state boundaries to only include North and South Carolina\n  filter(NAME_1 %in% c(\"North Carolina\", \"South Carolina\"))\n\n# Subset Venus flytrap data to those states\nvflytrap_sub &lt;- st_crop(vflytrap_dat_sf, aoi)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Crop soils raster to those states\nsoils_plot &lt;- crop(soils, aoi, mask=TRUE) %&gt;%\n  # Convert soils raster to data frame\n  as.data.frame(soils, xy=TRUE)\n\nUse ggplot2 to create a map of the raster data with the species presence points overlayed on top. Add state/province/equivalent level boundaries.\n\nFirst, I plot the raster. Then, I overlay the points and then the state polygons. coord_sf() is optional since ggplot2 can retrieve the correct coordinate system from the sf data.\n\n\nggplot() +\n  geom_raster(data = soils_plot, aes(x=x, y=y, fill=`nitrogen_0-5cm`)) +\n  geom_sf(data=vflytrap_sub) +\n  geom_sf(data=aoi, color=\"gray50\", fill=NA)\n\n\n\n\n\n\n\n\nChange the raster color scale, legend name, title, and theme from ggplot2 defaults. You can try any other ggplot customization you’d like now as well.\n\nThe color scale and legend name can be changed in the scale_fill_* function. I like the viridis scale, so I’ll use that here. The plot title can be changed in labs. I’d like to get rid of the gray background and axes, so I’ll use theme_void. I’ll also make the points slightly transparent and bright red for better visualization.\n\n\nggplot() +\n  geom_raster(data = soils_plot, aes(x=x, y=y, fill=`nitrogen_0-5cm`)) +\n  geom_sf(data=vflytrap_sub, alpha = 0.4, color=\"red\") +\n  geom_sf(data=aoi, color=\"gray50\", fill=NA) +\n  scale_fill_viridis_c(name = \"Soil N\\n(Surface)\") +\n  labs(title = \"Venus Flytrap and Soil Nitrogen in the Carolinas\") +\n  theme_void()\n\n\n\n\n\n\n\n\nUse tmap to recreate this plot with zooming functionality and any other interactive elements you’d like to add. Optionally, you can substitute the raster for tidycensus or other polygon data at this stage.\n\nFirst, I’ll use tmap to recreate the plot above.\n\n\ntm_shape(crop(soils, aoi, mask=TRUE)) +\n  tm_raster(n=8, palette = viridis::viridis(8),\n            title = \"Soil N\\n(Surface)\") +\n  tm_legend(outside=TRUE) +\n  tm_shape(vflytrap_sub) +\n  tm_dots(alpha=0.4, col=\"red\") +\n  tm_shape(aoi) +\n  tm_borders(\"gray50\") +\n  tm_layout(main.title=\"Venus Flytrap and Soil Nitrogen in the Carolinas\")\n\n\n\n\n\n\n\n\n\nZoom capabilities are possible with tmap_mode. The default hover text is the first column in vflytrap_sub, which is the specimen key. This could be helpful, but I’m going to change it to the year of observation.\n\n\ntmap_mode(\"view\")\n\ntm_shape(crop(soils, aoi, mask=TRUE)) +\n  tm_raster(n=8, palette = viridis::viridis(8),\n            title = \"Soil N\\n(Surface)\") +\n  tm_legend(outside=TRUE) +\n  tm_shape(vflytrap_sub) +\n  tm_dots(alpha=0.4, id=\"year\", col=\"red\") +\n  tm_shape(aoi) +\n  tm_borders(\"gray50\") +\n  tm_layout(main.title=\"Venus Flytrap and Soil Nitrogen in the Carolinas\")\n\n\nFor a tidycensus example, I’ll pull data on county level population to see if Vensus flytraps are in heavily populated areas.\n\n\nblock_pop &lt;- get_acs(geography = \"county\",\n                     # most recent year available\n                     year = 2022,\n                     variables = \"B01001_001\",\n                     state = c(\"NC\", \"SC\"),\n                     geometry=TRUE)\nblock_pop_proj &lt;- block_pop %&gt;%\n  st_make_valid() %&gt;%\n  filter(!st_is_empty(.)) %&gt;%\n  st_transform(crs = st_crs(vflytrap_sub))\n\ntm_shape(block_pop_proj) +\n  tm_polygons(col=\"estimate\", border.alpha = 0,\n              n=8, palette = viridis::viridis(8),\n            title = \"Population\", id=\"estimate\") +\n  tm_legend(outside=TRUE) +\n  tm_shape(vflytrap_sub) +\n  tm_dots(alpha=0.4, col=\"red\", interactive=FALSE) +\n  tm_shape(aoi) +\n  tm_borders(\"gray50\") +\n  tm_layout(main.title=\"Venus Flytrap and Population in the Carolinas\")"
  },
  {
    "objectID": "example/session-17-example.html",
    "href": "example/session-17-example.html",
    "title": "Session 17 code",
    "section": "",
    "text": "Load libraries:\n\n\nCode\nlibrary(rgbif)\nlibrary(sf, quietly = TRUE)\nlibrary(spatstat, quietly = TRUE)\nlibrary(tidyverse, quietly = TRUE)\n\n\nGet data:\n\n\nCode\nid &lt;- tigris::states(progress_bar = FALSE) %&gt;%\n  filter(NAME == \"Idaho\") %&gt;%\n  st_transform(crs=4326)\n\n\nRetrieving data for the year 2021\n\n\n\n\nCode\ngold_eag &lt;- occ_search(scientificName = \"Aquila chrysaetos\",\n                       country = \"US\",\n                       hasCoordinate = TRUE,\n                       limit = 1000)\n\n\nPrepare data for analysis:\n\n\nCode\ngold_eag_dat &lt;- gold_eag$data\n\n\n\n\nCode\n# convert to spatial data\ngold_eag_sf &lt;- gold_eag_dat %&gt;%\n  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude)) %&gt;%\n  st_as_sf(., coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs=4326)\n\nplot(st_geometry(gold_eag_sf))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# subset to study area\ngold_eag_id &lt;- gold_eag_sf[id, ]\n\nplot(st_geometry(id))\nplot(st_geometry(gold_eag_id), add=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Point Patterns"
    ]
  },
  {
    "objectID": "example/session-17-example.html#pre-processing",
    "href": "example/session-17-example.html#pre-processing",
    "title": "Session 17 code",
    "section": "",
    "text": "Load libraries:\n\n\nCode\nlibrary(rgbif)\nlibrary(sf, quietly = TRUE)\nlibrary(spatstat, quietly = TRUE)\nlibrary(tidyverse, quietly = TRUE)\n\n\nGet data:\n\n\nCode\nid &lt;- tigris::states(progress_bar = FALSE) %&gt;%\n  filter(NAME == \"Idaho\") %&gt;%\n  st_transform(crs=4326)\n\n\nRetrieving data for the year 2021\n\n\n\n\nCode\ngold_eag &lt;- occ_search(scientificName = \"Aquila chrysaetos\",\n                       country = \"US\",\n                       hasCoordinate = TRUE,\n                       limit = 1000)\n\n\nPrepare data for analysis:\n\n\nCode\ngold_eag_dat &lt;- gold_eag$data\n\n\n\n\nCode\n# convert to spatial data\ngold_eag_sf &lt;- gold_eag_dat %&gt;%\n  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude)) %&gt;%\n  st_as_sf(., coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs=4326)\n\nplot(st_geometry(gold_eag_sf))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# subset to study area\ngold_eag_id &lt;- gold_eag_sf[id, ]\n\nplot(st_geometry(id))\nplot(st_geometry(gold_eag_id), add=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Point Patterns"
    ]
  },
  {
    "objectID": "example/session-17-example.html#kernel-density-estimates",
    "href": "example/session-17-example.html#kernel-density-estimates",
    "title": "Session 17 code",
    "section": "Kernel Density Estimates",
    "text": "Kernel Density Estimates\n\nPrepare data for spatstat\n\n\nCode\n# must be mapped on a plane - projected CRS\ngold_eag_id_proj &lt;- st_transform(gold_eag_id, crs = 5070)\nid_proj &lt;- st_transform(id, crs = 5070)\n\n\n\n\nCode\n# convert to ppp object\ngold_eag.ppp &lt;- gold_eag_id_proj %&gt;%\n  as.ppp()\n\n\nWarning in as.ppp.sf(.): only first attribute column is used for marks\n\n\nCode\n# remove marks from points\nmarks(gold_eag.ppp) &lt;- NULL\n# set study area to Idaho\nWindow(gold_eag.ppp) &lt;- as.owin(id_proj)\n\nplot(gold_eag.ppp)\n\n\n\n\n\n\n\n\n\n\n\nKernel Density Estimates with different bandwidths\n\n\nCode\nkde0 &lt;- density(gold_eag.ppp)\nplot(kde0, main=\"Default KDE for ID golden eagles\")\n\n\n\n\n\n\n\n\n\nCode\nkde1 &lt;- density(gold_eag.ppp, adjust = 0.75)\nplot(kde1, main=\"Bandwidth reduced by 0.75\")",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Point Patterns"
    ]
  },
  {
    "objectID": "example/session-17-example.html#ripleys-k",
    "href": "example/session-17-example.html#ripleys-k",
    "title": "Session 17 code",
    "section": "Ripley’s K",
    "text": "Ripley’s K\n\n\nCode\nk0 &lt;- Kest(gold_eag.ppp)\nplot(k0)\n\n\n\n\n\n\n\n\n\nThe x axis shows the increasing radius of the circle tested and the y axis shows the \\(K\\) at any given radius.\nDifferent color lines show difference edge corrections. In the next step, we used border because it is the fastest. More information on edge correction can be found in the details section of the Kest help file (?Kest).\nTo test whether our points are clustered, we generate a \\(K\\) for completely spatially random points (red line below).\n\n\nCode\nk.env &lt;- envelope(gold_eag.ppp, correction=\"border\", envelope = FALSE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nCode\nplot(k.env)\n\n\n\n\n\n\n\n\n\nThe error is generated by the 99 simulations of CSR referred to in the function printout. Here, we see our data are clustered (spatially autocorrelated) from the very smallest scale (r = 1) to a radius of about 90,000 meters.",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Point Patterns"
    ]
  },
  {
    "objectID": "assignment/04-mapssolutions.html",
    "href": "assignment/04-mapssolutions.html",
    "title": "Assignment 4 Solutions: Predicates and Measures",
    "section": "",
    "text": "1. Load the cejst_nw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code). \n\nRemember that predicates return logical (i.e. TRUE or FALSE) answers so we are looking for functions with st_is_* to look for valid or empty geometries. We wrap those in the all() or any() function calls so that we get a single TRUE or FALSE for the entire geometry collection rather than returning the value for each individual observation. While those can be useful for figuring out if the entire dataset meets our criteria (i.e., all are valid or any have empty geometries), identifying which records have empty geometries takes an additional step. We use which() to return the row index of each record that returns a TRUE for st_is_empty() and then subset the original data using the [] notation keeping only the rows with empty geometries and all other columns.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\n\ncejst.nw &lt;- read_sf(\"opt/data/data/assignment04/cejst_nw.shp\")\nall(st_is_valid(cejst.nw))\nany(st_is_empty(cejst.nw))\nwhich(st_is_empty(cejst.nw))\n\ncejst.nw[which(st_is_empty(cejst.nw)),]\n\n 2. Load the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset… \n\nHere we are interested in distance which is a measure (not a predicate or transformer), but to get there we need to take a few extra steps. First, we read in the csv file and convert it to coordinates (using st_as_sf, a transformer). Then we use dplyr::filter to retain only the hospitals in the dataset. Finally, because this is a lat/long dataset, we assume a geodetic projection of WGS84 and assign it to the filtered object. Once we’ve gotten all that squared away, it’s just a matter of using the st_distance function to return the distance matrix for all objects in the dataset.\n\n\nhospitals.id &lt;- read_csv(\"opt/data/data/assignment04/landmarks_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4326\n\ndist.hospital &lt;- st_distance(hospitals.id)\n\ndist.hospital[1:5, 1:5]\n\n 3. Filter the cejst_nw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\n\nThis one should be relatively straightforward. We start with another call to dplyr::filter to get down to just the tracts in Ada County (note the use of the & to combine two logical calls). Then we use a second filter to return the row with the max value for agricultural loss. Note that we have to use the na.rm=TRUE argument to avoid having the NA values force the function to return NA.\n\n\nada.cejst &lt;- cejst.nw %&gt;% \n  filter(., SF == \"Idaho\" & CF == \"Ada County\") \n\nada.max.EALR &lt;- ada.cejst %&gt;%  \n  filter(., EALR_PFS == max(EALR_PFS, na.rm = TRUE))\n  \nada.max.EALR[, c(\"SF\", \"CF\", \"EALR_PFS\")]\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1998 ymin: 43.11297 xmax: -115.9742 ymax: 43.59134\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 4\n  SF    CF         EALR_PFS                                             geometry\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Idaho Ada County      0.7 (((-116.1371 43.55217, -116.137 43.55222, -116.1369…\n\n\n 4. Finally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\nWe can access the helpfile for adjacent by using ?terra::adjacent (I won’t do that here because I don’t want to print the entire helpfile). From that we can see that the cells argument is the place to specify which cells we are interested in. also see that the directions argument allows us to specify whether we want “rook”, “bishop”, or “queen” neighbors. Finally, we see that if we want to exclude the focal cell itself, we have to set include to FALSE. By plotting the map with the cell numbers, we can see that cells 1 and 5 are on th top row of the raster and thus do not have any neighbors for for the upper 3 categories whereas cell 55 has all 8 neighbors. If you choose cells that are in the center of the raster, you get all neighbors\n\n\nr &lt;- rast(nrows=10, ncols=10)\ncellnum &lt;- cells(r)\nr[] &lt;- cellnum\nplot(r)\n\n\n\n\n\n\n\nadjacent(r, cells=c(1, 5, 55), directions=\"queen\", include=FALSE)\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n0   NaN  NaN  NaN   10    2   20   11   12\n4   NaN  NaN  NaN    4    6   14   15   16\n54   44   45   46   54   56   64   65   66\n\nadjacent(r, cells=c(51, 52, 55), directions=\"queen\", include=FALSE)\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n50   50   41   42   60   52   70   61   62\n51   41   42   43   51   53   61   62   63\n54   44   45   46   54   56   64   65   66"
  },
  {
    "objectID": "slides/27-slides.html#categories-of-data-visualization",
    "href": "slides/27-slides.html#categories-of-data-visualization",
    "title": "Towards Interactivity",
    "section": "3 Categories of data visualization",
    "text": "3 Categories of data visualization\n\n\n\nStatic\nInteractive\nDynamic\n\n\n\n\n\ndynamic"
  },
  {
    "objectID": "slides/27-slides.html#dealing-with-complex-datasets",
    "href": "slides/27-slides.html#dealing-with-complex-datasets",
    "title": "Towards Interactivity",
    "section": "Dealing with complex datasets",
    "text": "Dealing with complex datasets\n\n\n\n\n\nIdentifying structure that might otherwise be hidden\nDiagnosing models and interpreting results\nAiding the sense-making process"
  },
  {
    "objectID": "slides/27-slides.html#clarity-in-presentation",
    "href": "slides/27-slides.html#clarity-in-presentation",
    "title": "Towards Interactivity",
    "section": "Clarity in presentation",
    "text": "Clarity in presentation\n\nZooming allows the user to determine scale of presentation\nHovering allows more information to be displayed ‘on-demand’\nSubsetting facilitates ease of interpretation"
  },
  {
    "objectID": "slides/27-slides.html#who-is-your-audience",
    "href": "slides/27-slides.html#who-is-your-audience",
    "title": "Towards Interactivity",
    "section": "Who is your audience?",
    "text": "Who is your audience?\n\nYour advisor and colleagues?\nAn external collaborator?\nThe general public?\n\nUser archetypes"
  },
  {
    "objectID": "slides/27-slides.html#iteration",
    "href": "slides/27-slides.html#iteration",
    "title": "Towards Interactivity",
    "section": "Iteration",
    "text": "Iteration\n\n\n\n\n\nFrom Usability.gov\n\n\n\n\n\nFeedback is critical\nIdeation: What specifically does the user need?\nMeaning: Are the data clearly defined and explained? Are the conclusions obvious?\nFunction: Given the usecases, will the application (visualization) actually perform?"
  },
  {
    "objectID": "slides/27-slides.html#a-note-about-apis",
    "href": "slides/27-slides.html#a-note-about-apis",
    "title": "Towards Interactivity",
    "section": "A note about APIs",
    "text": "A note about APIs\n\nAPI: Application Programming Interface\nA software intermediary that allows two applications to “communicate”\nLots of R packages rely on APIs to access data on the web (e.g.,tidycensus)\nFacilitates reproducibility and powerful web applications built on R analyses\nMay require “keys” and additional parsing (Mapbox and Google)"
  },
  {
    "objectID": "slides/27-slides.html#interactive-maps-with-mapview-and-tmap",
    "href": "slides/27-slides.html#interactive-maps-with-mapview-and-tmap",
    "title": "Towards Interactivity",
    "section": "Interactive maps with mapview and tmap",
    "text": "Interactive maps with mapview and tmap\n\nEasy extension of your existing code\nClass Demo"
  },
  {
    "objectID": "slides/27-slides.html#clarity-in-presentation-revisited",
    "href": "slides/27-slides.html#clarity-in-presentation-revisited",
    "title": "Towards Interactivity",
    "section": "Clarity in presentation (revisited)",
    "text": "Clarity in presentation (revisited)"
  },
  {
    "objectID": "slides/27-slides.html#using-plotly",
    "href": "slides/27-slides.html#using-plotly",
    "title": "Towards Interactivity",
    "section": "Using plotly",
    "text": "Using plotly\n\nSyntax is similar to ggplot\nhoverinfo describes which elements you’d like to make interactive\nOther plot elements available (see ?plot_ly)"
  },
  {
    "objectID": "slides/27-slides.html#using-plotly-1",
    "href": "slides/27-slides.html#using-plotly-1",
    "title": "Towards Interactivity",
    "section": "Using plotly",
    "text": "Using plotly\n\ng &lt;- txhousing %&gt;% \n  # group by city\n  group_by(city) %&gt;%\n  # initiate a plotly object with date on x and median on y\n   plotly::plot_ly(x = ~date, y = ~median) %&gt;%\n  # add a line plot for all texan cities\n   plotly::add_lines(name = \"Texan Cities\", hoverinfo = \"none\", \n                     type = \"scatter\", mode = \"lines\", \n                     line = list(color = 'rgba(192,192,192,0.4)')) %&gt;%\n  # plot separate lines for Dallas and Houston\n   plotly::add_lines(name = ~city, \n            data = filter(txhousing, \n                          city %in% c(\"Dallas\", \"Houston\")),\n            hoverinfo = ~city,\n            color = ~city)"
  },
  {
    "objectID": "slides/28-slides.html#what-is-an-interactive-dashboard",
    "href": "slides/28-slides.html#what-is-an-interactive-dashboard",
    "title": "Interactive Dashboards",
    "section": "What is an interactive dashboard?",
    "text": "What is an interactive dashboard?\n\nReactive to user inputs\nExamples"
  },
  {
    "objectID": "slides/28-slides.html#what-do-we-need",
    "href": "slides/28-slides.html#what-do-we-need",
    "title": "Interactive Dashboards",
    "section": "What do we need?",
    "text": "What do we need?\n\nProper YAML header\nglobal code chunk to load libraries and data\nShiny inputs and outputs\nRender results"
  },
  {
    "objectID": "slides/28-slides.html#yaml-header",
    "href": "slides/28-slides.html#yaml-header",
    "title": "Interactive Dashboards",
    "section": "YAML header",
    "text": "YAML header\n\nOutput is now a flexdashboard (instead of html)\nruntime:: shiny allows R Shiny to handle interactivity\n\n\n---\ntitle: \"Climate, social, and environmental justice markers for the Pacific Northwest\"\noutput: flexdashboard::flex_dashboard\nruntime: shiny\n---"
  },
  {
    "objectID": "slides/28-slides.html#global-code",
    "href": "slides/28-slides.html#global-code",
    "title": "Interactive Dashboards",
    "section": "global code",
    "text": "global code\nCode that only needs to run once\n\n```{r global}\n# include: false\n\nlibrary(shiny)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\ntmap_mode(\"view\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n# get column codes and meanings\ncol_choices &lt;- read_csv(\"/opt/data/data/assignment04/columns.csv\") %&gt;%\n  # make nicer column names for a display table\n  rename(\"Code\" = \"shapefile_column\", \"Description\" = \"column_name\") %&gt;%\n  # keep only \"percentile\" type columns\n  filter(str_detect(Code, \"PFS\"))\n```"
  },
  {
    "objectID": "slides/28-slides.html#inputs",
    "href": "slides/28-slides.html#inputs",
    "title": "Interactive Dashboards",
    "section": "Inputs",
    "text": "Inputs\n\nLive in a sidebar\n\n\nColumn {.sidebar}\n-----------------------------------------------------------------------"
  },
  {
    "objectID": "slides/28-slides.html#types-of-inputs",
    "href": "slides/28-slides.html#types-of-inputs",
    "title": "Interactive Dashboards",
    "section": "Types of Inputs",
    "text": "Types of Inputs\n\n\n\n\nR Function\nInput Type\n\n\n\n\nselectInput\nA box with choices to select from\n\n\nsliderInput\nA slider bar\n\n\nradioButtons\nA set of radio buttons\n\n\ntextInput\nA field to enter text\n\n\nnumericInput\nA field to enter numbers\n\n\ncheckboxInput\nA single check box\n\n\ndateInput\nA calendar to aid date selection\n\n\ndateRangeInput\nA pair of calendars for selecting a date range\n\n\nfileInput\nA file upload control wizard"
  },
  {
    "objectID": "slides/28-slides.html#adding-inputs",
    "href": "slides/28-slides.html#adding-inputs",
    "title": "Interactive Dashboards",
    "section": "Adding Inputs",
    "text": "Adding Inputs\n\n# Box with choices: which cejst column to map\nselectInput(\"column_select\", label = \"Justice Marker:\",\n            choices = col_choices$Code, selected = \"DF_PFS\")\n\n# Two sliders to select the maximum and minimum values to map\nsliderInput(\"min_threshold_adjust\", label = \"Minimum value:\",\n            min = 0, max = 1, value = 0.5, step = 0.05)\nsliderInput(\"max_threshold_adjust\", label = \"Maximum value:\",\n            min = 0, max = 1, value = 1, step = 0.05)"
  },
  {
    "objectID": "slides/28-slides.html#adding-outputs",
    "href": "slides/28-slides.html#adding-outputs",
    "title": "Interactive Dashboards",
    "section": "Adding Outputs",
    "text": "Adding Outputs\nCreate a new column with title\n\nColumn\n-----------------------------------------------------------------------\n\n### Climate, Social, and Environmental Justice"
  },
  {
    "objectID": "slides/28-slides.html#types-of-outputs",
    "href": "slides/28-slides.html#types-of-outputs",
    "title": "Interactive Dashboards",
    "section": "Types of Outputs",
    "text": "Types of Outputs\n\n\n\nR Function\nOutput Type\n\n\n\n\nrenderPlot\nR graphics output\n\n\nrenderPrint\nR printed output\n\n\nrenderTable\nData frame, matrix, other table like structures\n\n\nrenderText\nCharacter vectors"
  },
  {
    "objectID": "slides/28-slides.html#adding-reactive-output",
    "href": "slides/28-slides.html#adding-reactive-output",
    "title": "Interactive Dashboards",
    "section": "Adding Reactive Output",
    "text": "Adding Reactive Output\n\nSpecify reactive elements with input$NameOfInput\nIn this example, we use reactive filtering to only map cejst tracts that meet user criteria\n\n\n# renderTmap is a tmap special case of renderPlot\nrenderTmap({\n  # put reactively filtered data in tm_shape\n  tm_shape(subset(cejst[, input$column_select], # subset data to user's column\n                  # use the subset in the filtering steps, selecting the column of data with [[1]]\n                  cejst[, input$column_select][[1]] &lt;= input$max_threshold_adjust & # data column should be less than or equal to the user's max threhold\n                    cejst[, input$column_select][[1]] &gt;= input$min_threshold_adjust)) + #more than or equal to the min threshold\n    # add the polygons filled by the user's selected column\n    tm_polygons(col = input$column_select)\n})"
  },
  {
    "objectID": "slides/28-slides.html#add-textexplanation-to-sidebar",
    "href": "slides/28-slides.html#add-textexplanation-to-sidebar",
    "title": "Interactive Dashboards",
    "section": "Add text/explanation to sidebar",
    "text": "Add text/explanation to sidebar\n\nAdd plain text before or after code chunks\nI’ve also added a nice table with the column name meanings for reference\n\n\nknitr::kable(col_choices[,1:2])"
  },
  {
    "objectID": "slides/28-slides.html#publishing",
    "href": "slides/28-slides.html#publishing",
    "title": "Interactive Dashboards",
    "section": "Publishing",
    "text": "Publishing\n\nOn shinyapps.io (free; tutorial here)\nOn GitHub Pages (limited free pages; tutorial here)\nContact Research Computing for options"
  },
  {
    "objectID": "example/session-11-example.html",
    "href": "example/session-11-example.html",
    "title": "Session 11 Code",
    "section": "",
    "text": "Code for questions 1 and 2 can be found in Session 11, in both the slides and the Panopto recording.",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations II"
    ]
  },
  {
    "objectID": "example/session-11-example.html#question-3",
    "href": "example/session-11-example.html#question-3",
    "title": "Session 11 Code",
    "section": "Question 3",
    "text": "Question 3\n\nWhat do we need to know?\n\n\n\nPseudocode\n\n\n\nStep 1: Get cdc data and project if necessary\nWe did this in the previous questions – the dataset is cdc.idaho.\n\n\nStep 2: Get hospital data and project if necessary\nWe did this in the previous questions – the dataset is hospital.sf.proj.\n\n\nStep 3: Buffer service areas around hospitals\nWe did this in the previous questions – the dataset is hospital.buf.\n\n\nStep 4: Find intersections of service areas\nWe did this in the previous questions – the dataset is hospital.int.overlaps. However, we re-projected it, so now we need to project cdc.idaho to the same projection. A plot shows that they are aligned.\n\n\nCode\ncdc.idaho.proj &lt;- st_transform(cdc.idaho, crs=st_crs(hospital.int.overlaps))\n\nplot(st_geometry(cdc.idaho.proj), col=\"purple\")\nplot(st_geometry(hospital.int.overlaps), add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Find tracts that overlap those intersections\n\n\nCode\noverlap.tracts.matrix &lt;- st_intersects(cdc.idaho.proj, hospital.int.overlaps, sparse = FALSE)\n\noverlap.tracts.matrix[1:12, 1:5]\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]\n [1,] FALSE FALSE FALSE FALSE FALSE\n [2,] FALSE FALSE FALSE FALSE FALSE\n [3,] FALSE FALSE FALSE FALSE FALSE\n [4,] FALSE FALSE FALSE FALSE FALSE\n [5,] FALSE FALSE  TRUE FALSE FALSE\n [6,] FALSE FALSE  TRUE FALSE FALSE\n [7,] FALSE FALSE  TRUE FALSE FALSE\n [8,] FALSE FALSE  TRUE FALSE FALSE\n [9,] FALSE FALSE  TRUE FALSE FALSE\n[10,] FALSE FALSE  TRUE FALSE FALSE\n[11,] FALSE FALSE  TRUE FALSE FALSE\n[12,] FALSE FALSE  TRUE FALSE FALSE\n\n\nThis creates a logical matrix where each row corresponds to a tract, and the cells in the matrix show whether it overlaps with each overlap area (TRUE) or whether it does not (FALSE). The cool thing about logicals is that they also count as numbers (TRUE = 1, FALSE = 0). By finding the rowSums, we can see which tracts overlap with 2+ hospital areas (rowSum &gt;= 1) and which don’t (rowSum = 0).\n\n\nCode\noverlap.tracts.filter &lt;- rowSums(overlap.tracts.matrix)\n\noverlap.tracts &lt;- cdc.idaho.proj[overlap.tracts.filter&gt;=1, ]\n\nplot(st_geometry(overlap.tracts), col=\"cornflowerblue\")\nplot(st_geometry(hospital.int.overlaps), add=TRUE, col=\"orange\")\n\n\n\n\n\n\n\n\n\nFor an alternate tidyverse integration, see this Stack Overflow question.\n\n\nStep 6: Find tracts outside of service buffers\nWe’ll use the same process as step 5, but this time we’ll keep the rowSums that are equal to 0.\n\n\nCode\nnohosp.tracts.matrix &lt;- st_intersects(cdc.idaho.proj, hospital.buf, sparse = FALSE)\nnohosp.tracts.matrix[1:12, 1:5]\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]\n [1,] FALSE FALSE FALSE FALSE FALSE\n [2,] FALSE FALSE FALSE FALSE FALSE\n [3,] FALSE FALSE FALSE FALSE FALSE\n [4,] FALSE FALSE FALSE FALSE FALSE\n [5,] FALSE FALSE  TRUE FALSE FALSE\n [6,] FALSE FALSE  TRUE FALSE FALSE\n [7,] FALSE FALSE  TRUE FALSE FALSE\n [8,] FALSE FALSE  TRUE FALSE FALSE\n [9,] FALSE FALSE  TRUE FALSE FALSE\n[10,] FALSE FALSE  TRUE FALSE FALSE\n[11,] FALSE FALSE  TRUE FALSE FALSE\n[12,] FALSE FALSE  TRUE FALSE FALSE\n\n\nCode\nnohosp.tracts.filter &lt;- rowSums(nohosp.tracts.matrix)\n\nnohosp.tracts &lt;- cdc.idaho.proj[nohosp.tracts.filter==0, ]\n\nplot(st_geometry(nohosp.tracts), col=\"firebrick\")\nplot(st_geometry(hospital.buf), add=TRUE, col=\"orange\")\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Calculate average chronic heart disease rate for both groups of tracts\n\n\nCode\navg.nohosp.rate &lt;- mean(nohosp.tracts$chd_crudep)\n\navg.overlaps.rate &lt;- mean(overlap.tracts$chd_crudep)\n\n\n\n\nStep 8: Find the difference\n\n\nCode\navg.overlaps.rate - avg.nohosp.rate\n\n\n[1] 0.36875\n\n\nThe rates of chronic heart disease are on average higher in tracts with multiple hospitals than those with no hospitals. Maybe there’s less access to a diagnosis, or heart disease is more fatal to people with less hospital access…?",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Vector Operations II"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html",
    "href": "lesson/vector_intro.html",
    "title": "Intro to Vector Data",
    "section": "",
    "text": "Today we’ll build on the introductory discussion we were having about vector operations and the sf package. We’ll build a few vectors from scratch and then move on to explore a broader suite of common vector operations implemented by the sf package.",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "href": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "title": "Intro to Vector Data",
    "section": "A reminder about vector geometries in R",
    "text": "A reminder about vector geometries in R\nYou’ll recall that the sf package organizes the different types of vectors (e.g., points, lines, polygons) in to a hierarchical structure organized by complexity of geometries contained within an R object. For example, a single point will be a POINT, several points will be a MULTIPOINT, and an object containing points, polygons, and lines will be a GEOMETRYCOLLECTION. We need to be aware of what types of geometries and objects we have becasue some operations are restricted to particular types of objects or geometries as indicated by errors that read:\nError in UseMethod(\"st_crs&lt;-\") :    no applicable method for 'st_crs&lt;-' applied to an object of class \"c('XY', 'POINT', 'sfg')\"\nwhich indicates that the function (st_crs) does not have a method defined for the type of object it’s being applied to. Note that the function inside UseMethod will be replaced by whichever function you’re attempting to apply to your object and the object of class component will vary based on the function and the object class.\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\nAs is, these geometries are built on vertices with coordinates that are based on the Cartesian plane and thus are “spatial”, but not georeferenced or geographic. In order to convert these sf geometries to a geogrphic object (i.e., one with a CRS and whose location depicts and actual spot on the earth’s surface), we use st_sfc() to create a simple feature geography list column (see ?st_sfc for an example of this workflow).",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Conventions in sf and the tidyverse",
    "text": "Conventions in sf and the tidyverse\nOne of the benefits of the sf package is that it is designed to interface with the tidyverse suite of packages. One of the appealing parts of working with tidyverse packages is that they share an underlying philosophy, data structure, and grammar. This can make life a lot easier as you move from getting your data into R, constructing a set of covariates (including those derived from spatial data), analyzing, and plotting (or mapping) those data. People have strong opinions about the tidyverse, but I find it to be an (eventually) useful way for people to gain some intuition for working in R. One of the grammatical conventions used in the tidyverse suite of packages is the use _ in function calls (this is known as snake case should you ever need to know that at a dinner party). The _ is typically used to separate the verb in a function call from its predicate. For example, bind_rows() in the dplyr package “binds” (the verb) rows (the predicate) wheras bind_cols() binds columns. For the sf package it’s slightly different in that most of the functions begin with a st_ or sf_ prefix to indicate that the function is designed to work on spatial objects followed by a word (or words) describing what the operation does (e.g., st_centroid() returns a MULTIPOINT object with each point located at the centroid of a polygon). We can classify these functions based on what they are expected to return:\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\nWe can also distinguish these functions based on how many geometries that operate on:\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries\n\nWe’ll focus on the unary operators for now, but the binary and n-ary operators will become more important as we move to develop databases for spatial analysis.\n\nUnary predicates\nUnary predicates are helpful ‘checks’ to make sure the object you are working with has the properties you might expect. Are the geometries valid? Is the data projected? Because we are asking a set of TRUE/FALSE questions, these functions are specified as st_is_:\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nsimple\nis the geometry self-intersecting (i.e., simple)?\n\n\nvalid\nis the geometry valid?\n\n\nempty\nis the geometry column of an object empty?\n\n\nlonglat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?\n\n\n\n\n\nCode\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\n\nReading layer `nc' from data source \n  `C:\\Users\\carolynkoehn\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.3\\x86_64-w64-mingw32\\sf\\1.0-16\\ad57b543f7c3fca05213ba78ff63df9b\\sf\\shape\\nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nCode\nst_is_longlat(nc)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(nc)\n\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n\nUnary measures\nMeasures return a quantity that describes the geometry\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\ndistance is a binary measure that returns the distance between pairs of geometries either within a single object or between features in multiple objects\n\n\nCode\nhead(st_area(nc))\n\n\nUnits: [m^2]\n[1] 1137107793  610916077 1423145355  694378925 1520366979  967504822\n\n\nCode\nst_distance(nc)[1:5,1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]      [,4]      [,5]\n[1,]      0.0      0.0  25591.8 439493.26 299049.94\n[2,]      0.0      0.0      0.0 408416.68 268284.09\n[3,]  25591.8      0.0      0.0 366648.94 226461.23\n[4,] 439493.3 408416.7 366648.9      0.00  67066.43\n[5,] 299049.9 268284.1 226461.2  67066.43      0.00\n\n\n\n\nUnary transformers\nUnary transformations work on a per object basis and return a new geometry for each geometry. These are a few of the most common, we’ll encounter a few more as the semester continues.\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is this larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (last week)\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n\n\n\nCode\nplot(st_geometry(nc))\nplot(st_geometry(st_centroid(nc)), add=TRUE, col='red')\n\n\nWarning: st_centroid assumes attributes are constant over geometries",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Using sf and the tidyverse",
    "text": "Using sf and the tidyverse\nAs I mentioned, one of the benefits of using the sf package is that commands from the other tidyverse package have defined methods for spatial objects. The dplyr package has a ton of helpful functions for maniputlating data in R. For example, we might select a single row from a shapefile based on the value of its attributes by using the dplyr::filter() command:\n\n\nCode\ndurham.cty &lt;- nc %&gt;% \n  filter(., NAME == \"Durham\")\n## We can also use the bracket approach\ndurham.cty2 &lt;- nc[nc$NAME == \"Durham\",]\n\nplot(st_geometry(nc))\nplot(st_geometry(durham.cty), add=TRUE, col=\"blue\")\n\n\n\n\n\n\n\n\n\nOr perhaps we only want a few of the columns in the dataset (because shapefiles always have lots of extra stuff). We can use dplyr::select() to choose columns by name:\n\n\nCode\nnc.select &lt;- nc %&gt;% \n  select(., c(\"CNTY_ID\", \"NAME\", \"FIPS\"))\nplot(nc.select)\n\n\n\n\n\n\n\n\n\nNotice that the geometries are sticky, this will be important later",
    "crumbs": [
      "Lessons",
      "Spatial operations in R",
      "Vector data"
    ]
  },
  {
    "objectID": "slides/02-slides.html#checking-in",
    "href": "slides/02-slides.html#checking-in",
    "title": "Why Geographic Analysis",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "slides/02-slides.html#todays-plan",
    "href": "slides/02-slides.html#todays-plan",
    "title": "Why Geographic Analysis",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWhat can we do with geographic information?\nConceptual challenges\nAnalytical challenges\nCritiques of quantitative geography"
  },
  {
    "objectID": "slides/02-slides.html#what-is-geography",
    "href": "slides/02-slides.html#what-is-geography",
    "title": "Why Geographic Analysis",
    "section": "What is geography",
    "text": "What is geography\n\nGeo: land, earth, terrain\nGraph: writing, discourse\nTuan: Space (extent) and Place (location)\nAnalysis of the effects of extent and location on events or features"
  },
  {
    "objectID": "slides/02-slides.html#five-themes-in-geography",
    "href": "slides/02-slides.html#five-themes-in-geography",
    "title": "Why Geographic Analysis",
    "section": "Five Themes in Geography",
    "text": "Five Themes in Geography\n\n\n\nLocation\nPlace\nRegion\nMovement\nHuman-Environment Interaction\n\n\n\n\n\nWGBH Educational Foundation"
  },
  {
    "objectID": "slides/02-slides.html#location",
    "href": "slides/02-slides.html#location",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#location-1",
    "href": "slides/02-slides.html#location-1",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#place",
    "href": "slides/02-slides.html#place",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForest cover map by Robert Simmons via Wikimedia Commons; temp map from GISgeography.com"
  },
  {
    "objectID": "slides/02-slides.html#place-1",
    "href": "slides/02-slides.html#place-1",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?"
  },
  {
    "objectID": "slides/02-slides.html#region",
    "href": "slides/02-slides.html#region",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#region-1",
    "href": "slides/02-slides.html#region-1",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#movement",
    "href": "slides/02-slides.html#movement",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape."
  },
  {
    "objectID": "slides/02-slides.html#movement-1",
    "href": "slides/02-slides.html#movement-1",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth maps courtesy of High Country News"
  },
  {
    "objectID": "slides/02-slides.html#human-environment-interactions",
    "href": "slides/02-slides.html#human-environment-interactions",
    "title": "Why Geographic Analysis",
    "section": "Human-Environment Interactions",
    "text": "Human-Environment Interactions\nHow do people relate to and change the physical world to meet their needs?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmoke map courtesy of Capital Public Radio; Nightlights courtesy of NASA Earth Observatory."
  },
  {
    "objectID": "slides/02-slides.html#description",
    "href": "slides/02-slides.html#description",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nCoordinates\nDistances\nNeighbors\nSummary statistics\n\n\n\n\n\n\ncourtesy of innovative GIS"
  },
  {
    "objectID": "slides/02-slides.html#description-1",
    "href": "slides/02-slides.html#description-1",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nRange Maps\nHotspots\nIndices"
  },
  {
    "objectID": "slides/02-slides.html#explanation-and-inference",
    "href": "slides/02-slides.html#explanation-and-inference",
    "title": "Why Geographic Analysis",
    "section": "Explanation and Inference",
    "text": "Explanation and Inference\n\nCognitive Description: collection ordering and classification of data\nCause and Effect: design-based or model-based testing of the factors that give rise to geographic distributions\nSystems Analysis: describes the entire complex set of interactions that structure an activity"
  },
  {
    "objectID": "slides/02-slides.html#prediction",
    "href": "slides/02-slides.html#prediction",
    "title": "Why Geographic Analysis",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\nExtend description or explanation into unmeasured space\nStationarity: the rules governing a process do not drift over space-time"
  },
  {
    "objectID": "slides/02-slides.html#scale",
    "href": "slides/02-slides.html#scale",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nWhat do we even mean?\n\n\n\n\n\n\nGrain: the smallest unit of measurement\nExtent: the areal coverage of the measurement\n\n\n\n\nFrom Manson 2008"
  },
  {
    "objectID": "slides/02-slides.html#scale-1",
    "href": "slides/02-slides.html#scale-1",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nEven if it exists, how do we know we are measuring at the right scale?"
  },
  {
    "objectID": "slides/02-slides.html#fallacies",
    "href": "slides/02-slides.html#fallacies",
    "title": "Why Geographic Analysis",
    "section": "Fallacies",
    "text": "Fallacies\n\n\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals"
  },
  {
    "objectID": "slides/02-slides.html#measurement-error-and-mismatch",
    "href": "slides/02-slides.html#measurement-error-and-mismatch",
    "title": "Why Geographic Analysis",
    "section": "Measurement Error and Mismatch",
    "text": "Measurement Error and Mismatch"
  },
  {
    "objectID": "slides/02-slides.html#spatial-autocorrelation",
    "href": "slides/02-slides.html#spatial-autocorrelation",
    "title": "Why Geographic Analysis",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#stationarity",
    "href": "slides/02-slides.html#stationarity",
    "title": "Why Geographic Analysis",
    "section": "Stationarity",
    "text": "Stationarity\nThe rules governing a process do not drift over space-time\n\n\n\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "href": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "title": "Why Geographic Analysis",
    "section": "Not all geography needs to be quantitative",
    "text": "Not all geography needs to be quantitative\n\nAbstraction removes the interesting part\nWhat “is” may require assumptions we don’t want to accept\nWholly dependent on the military-industrial complex"
  },
  {
    "objectID": "content/29-content.html",
    "href": "content/29-content.html",
    "title": "Conclusion",
    "section": "",
    "text": "View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "29: Conclusion"
    ]
  },
  {
    "objectID": "content/27-content.html",
    "href": "content/27-content.html",
    "title": "Introduction to Interactive Maps",
    "section": "",
    "text": "Now that you’ve had a chance to practice building a few maps and learning some of the core ideas behind the Grammar of Graphics, we can extend those ideas into the development of interactive webmaps and more expansive data visualizations that can be served on the internet and accessed by collaborators and members of the public. Like the previous unit on static maps, this could be a course unto itself, but we should be able to introduce you to enough ideas to get started.",
    "crumbs": [
      "Content",
      "Course content",
      "27: Introduction to Interactive Maps"
    ]
  },
  {
    "objectID": "content/27-content.html#resources",
    "href": "content/27-content.html#resources",
    "title": "Introduction to Interactive Maps",
    "section": "Resources",
    "text": "Resources\n\n The Web-mapping section from the University Consortium for Geographic Information Science’s GIS & Technology Body of Knowledge has a nice overview of the topic and it’s origins.\n This post on User-Centered Design from Adobe provides a concise, general introduction to the core elements of User-Centered Design.\n The Maps chapter in Sievert (2020) gives a nice demonstration of using plotly to build interactive maps. More importantly, the book provides a comprehensive resource for building interactive web-based visualization in R.",
    "crumbs": [
      "Content",
      "Course content",
      "27: Introduction to Interactive Maps"
    ]
  },
  {
    "objectID": "content/27-content.html#objectives",
    "href": "content/27-content.html#objectives",
    "title": "Introduction to Interactive Maps",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine an API and their use in interactive visualization\nObtain a token for common mapping APIs\nBuild interactive maps using common packages\nRecognize other opportunities for interactive visuals with R\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "27: Introduction to Interactive Maps"
    ]
  },
  {
    "objectID": "content/25-content.html",
    "href": "content/25-content.html",
    "title": "Data Visualization and Maps I",
    "section": "",
    "text": "We’ve spent the last few weeks learning about operations to compile geographic information into databases for visualization and analysis. Because analysis requires you to know something about your data and because visualization is a great way to explore your data (especially when there’s a lot of it), we’ll turn to that next. For the next few weeks, we’ll be looking at different ways to visualize spatial data and the associated approaches in R. Note that this could be an entire course by itself, but hopefully you’ll get enough to get started making publication quality maps by the time we’re done",
    "crumbs": [
      "Content",
      "Course content",
      "25: Data Visualization and Maps I"
    ]
  },
  {
    "objectID": "content/25-content.html#resources",
    "href": "content/25-content.html#resources",
    "title": "Data Visualization and Maps I",
    "section": "Resources",
    "text": "Resources\n\n The Introduction and Visualizing Geospatial Data chapters Principles of Figure Design section in (Wilke 2019) provide a useful set of general introductions to data visualization principles and practice that is “platform agnostic” (though much of Wilke’s work is done in R).\n The Look at Data and Draw Maps chapters in (Healy 2018) revisits many of the same ideas, but focuses specifically on R and ggplot2.\n This post on making maps people want to look at from ESRI is a nice, concise depiction of some core principles for planning a cartographic project.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax.",
    "crumbs": [
      "Content",
      "Course content",
      "25: Data Visualization and Maps I"
    ]
  },
  {
    "objectID": "content/25-content.html#objectives",
    "href": "content/25-content.html#objectives",
    "title": "Data Visualization and Maps I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe some basic principles of data visualization\nExtend principles of data visualization to the development of maps\nDistinguish between several common types of spatial data visualization\nUnderstand the relationship between the Grammar of Graphics and ggplot syntax\nDescribe the various options for customizing ggplots and their syntactic conventions\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "25: Data Visualization and Maps I"
    ]
  },
  {
    "objectID": "content/22-content.html",
    "href": "content/22-content.html",
    "title": "Statistical Modelling II",
    "section": "",
    "text": "Last class we spent some time extending the idea of Favorability to build a foundation for treating overlay analysis as a logistic regression. Although logistic regression has a number of properties that make it desirable for inference, a number of recently developed statistical learning approaches have greatly improved our ability to take advantage a wide variety of available data and generate spatially explicit predictions. These methods may make interpretation and inference more challenging, but can improve the predictive ability of your models. We’ll explore some of those today.",
    "crumbs": [
      "Content",
      "Course content",
      "22: Statistical Modelling II"
    ]
  },
  {
    "objectID": "content/22-content.html#resources",
    "href": "content/22-content.html#resources",
    "title": "Statistical Modelling II",
    "section": "Resources",
    "text": "Resources\n\n An Introduction to Statistical Learning by (James et al. 2021) is a comprehensive introduction to a number of statistical learning techniques with examples in R. Although these examples are not necessarily spatial, the chapters provide a lot of the background necessary for understanding what the models are doing.\n A statistical explanation of MaxEnt for ecologists by (Elith et al. 2011) provides a relatively accessible description of the details of MaxEnt species distribution modeling.\n Random forests for Classification in Ecology by (Cutler et al. 2007) provides an introduction to the utility of Random Forests for ecologists.",
    "crumbs": [
      "Content",
      "Course content",
      "22: Statistical Modelling II"
    ]
  },
  {
    "objectID": "content/22-content.html#objectives",
    "href": "content/22-content.html#objectives",
    "title": "Statistical Modelling II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate the differences between statistical learning classifiers and logistic regression\nDescribe several classification trees and their relationship to Random Forests\nDescribe MaxEnt models for presence-only data\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "22: Statistical Modelling II"
    ]
  },
  {
    "objectID": "content/20-content.html",
    "href": "content/20-content.html",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "Now that we’ve learned about the power of spatial autocorrelation for interpolation from point data, it’s time to explore methods for spatial autocorrelation with areal data. We’ll have to define neighbors because distance is a little more ambiguous here and then look at some global and local measures of autocorrelation.",
    "crumbs": [
      "Content",
      "Course content",
      "20: Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "content/20-content.html#resources",
    "href": "content/20-content.html#resources",
    "title": "Spatial Autocorrelation",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights.\n\n Spatial Autocorrelation in R provides some easy code for working through neighbors with areal data and calculating spatial autocorrelation measures.",
    "crumbs": [
      "Content",
      "Course content",
      "20: Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "content/20-content.html#objectives",
    "href": "content/20-content.html#objectives",
    "title": "Spatial Autocorrelation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse the spdep package to identify the neighbors of a given polygon based on proximity, distance, and minimum number\nUnderstand the underlying mechanics of Moran’s I and calculate it for various neighbors\nDistinguish between global and local measures of spatial autocorrelation\nVisualize neighbors and clusters\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "20: Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "content/18-content.html",
    "href": "content/18-content.html",
    "title": "Interpolation",
    "section": "",
    "text": "Point patterns give us the foundation for beginning geostatistical analyses. In geostatistical analyses, we have observations or a spatial process from a limited sample of locations, but would like to be able to infer the values of that process across the entire study region (or at least an area larger than we initially sampled). Interpolation provides one simple way of doing this that relies on the notion that we can learn something about the process simply from our measurements and the location those measurements were taken. We can extend these approaches by adding additional covariates and model structures, but we’ll start simple for now.",
    "crumbs": [
      "Content",
      "Course content",
      "18: Interpolation"
    ]
  },
  {
    "objectID": "content/18-content.html#resources",
    "href": "content/18-content.html#resources",
    "title": "Interpolation",
    "section": "Resources",
    "text": "Resources\n\n Chapter 2: Scale in (Fletcher and Fortin 2018) provides a thorough introduction to the ecologist’s conceptualization of scale with R examples.\n This article by Steven Manson (Manson 2008) provides a more comprehensive view of conceptualizations of scale.\n The Hypothesis Testing and Autocorrelation chapters of Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provide concrete examples of attempts to find process from spatial patterns.\n Chapter 12: Spatial Interpolation in Spatial Data Science by Edzer Pebesma and Roger Bivand provides examples of different types of kriging and interpolation using sf and stars.",
    "crumbs": [
      "Content",
      "Course content",
      "18: Interpolation"
    ]
  },
  {
    "objectID": "content/18-content.html#objectives",
    "href": "content/18-content.html#objectives",
    "title": "Interpolation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDistinguish deterministic and stochastic processes\nDefine autocorrelation and describe its estimation\nArticulate the benefits and drawbacks of autocorrelation\nLeverage point patterns and autocorrelation to interpolate missing data\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "18: Interpolation"
    ]
  },
  {
    "objectID": "content/16-content.html",
    "href": "content/16-content.html",
    "title": "Combining Vectors and Rasters",
    "section": "",
    "text": "As we move towards a complete geospatial statistical workflow, we’ll need to be able to combine data from both raster and vector datasets. Sometimes that will mean simply converting from one format to another. In other cases, we’ll need to create new datasets based on calculations that integrate different data models.",
    "crumbs": [
      "Content",
      "Course content",
      "16: Combining Vectors and Rasters"
    ]
  },
  {
    "objectID": "content/16-content.html#resources",
    "href": "content/16-content.html#resources",
    "title": "Combining Vectors and Rasters",
    "section": "Resources",
    "text": "Resources\n\n The Integrating rasters and vectors chapter of Michael Dorman’s Introduction to Spatial Data Programming with R online textbook has a number of worked examples combining vector and raster data.\n Raster-vector interactions Chapter 6 in Lovelace et al., Geocomputation with R (Lovelace et al. 2019) has a great description of why you might do some of these things in your analysis.",
    "crumbs": [
      "Content",
      "Course content",
      "16: Combining Vectors and Rasters"
    ]
  },
  {
    "objectID": "content/16-content.html#objectives",
    "href": "content/16-content.html#objectives",
    "title": "Combining Vectors and Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nConvert between raster and vector datasets\nGenerate new rasters describing the spatial arrangement of vector data\nExtract raster values as attributes of vector data",
    "crumbs": [
      "Content",
      "Course content",
      "16: Combining Vectors and Rasters"
    ]
  },
  {
    "objectID": "content/16-content.html#slides",
    "href": "content/16-content.html#slides",
    "title": "Combining Vectors and Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "16: Combining Vectors and Rasters"
    ]
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Building Spatial Databases with Attributes",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R.\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#resources",
    "href": "content/14-content.html#resources",
    "title": "Building Spatial Databases with Attributes",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R.\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#objectives",
    "href": "content/14-content.html#objectives",
    "title": "Building Spatial Databases with Attributes",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nBegin building a database for spatial analysis",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "Building Spatial Databases with Attributes",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "14: Building Spatial Databases with Attributes"
    ]
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Operations with Raster Data I",
    "section": "",
    "text": "Now that we’ve learned about predicates and measures with raster data, it’s time to learn more about some of the transformations that we can conduct with terra. We’ll start with some of the basic transformations that operate on the entire dataset then move to some of the important cell-wise operations.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Operations with Raster Data I",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today.",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#objectives",
    "href": "content/12-content.html#objectives",
    "title": "Operations with Raster Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Operations with Raster Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "12: Operations with Raster Data I"
    ]
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Operations With Vector Data I",
    "section": "",
    "text": "Now that we’ve spent some time getting used to the syntax of the sf package and used it to assess some of the characteristics of vector objects (e.g., through predicates and measures), we’ll move into transformations. Transformations allow you to actually manipulate the geometries of a vector object (without necessarily changing the attributes themselves) and are a powerful tool for geting disparate data into some logical alignment. That said, transforming geometries can be complicated and often has some unanticipated consequences. That’s why we spent a little bit of time learning the mapping syntax as a means for you to be able to check yourself.",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Operations With Vector Data I",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages).",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#objectives",
    "href": "content/10-content.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Operations With Vector Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "10: Operations with Vector Data I"
    ]
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Areal Data: Rasters",
    "section": "",
    "text": "Now that we’ve learned a bit about how to assess some of the important quantities of vector-based spatial data, we’ll try to apply a bit of the same logic to raster data. We’ll be using the terra package for the majority of raster options in this course primarily because it of its speed. That said, it is not a tidyverse package and so some of the intuition we used to organize the sf functions will be a little harder to extend here. I’ll do my best to help you make the links!!",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Areal Data: Rasters",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 from Paula Moraga’s Spatial Statistics for Data Science: Theory and Practice with R provides a quick intro to using terra for raster and vector data.\n The terra reference page provides a brief overview of all of the functions and their categories. We’ll only focus on the SpatRaster methods.\n Raster Data Manipulation from the Spatial Data Science with R and terra ebook provides some nice examples of terra functions in the context of spatial workflows.",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#objectives",
    "href": "content/08-content.html#objectives",
    "title": "Areal Data: Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAccess the elements that define a raster\nBuild rasters from scratch using matrix operations and terra\nEvaluate logical conditions with raster data\nCalculate different measures of raster data",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Areal Data: Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "8: Areal Data - Raster Data"
    ]
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "",
    "text": "Now that you have a bit of the fundamentals of geographic data and have had a chance to start using R, it’s time to get into more complicated workflows. To do that, you’ll have to have a bit more of a foundation in coordinates, coordinate reference systems, and geometries and how to access those in R. We’ll start there today and move into functions that change or relate geometries in the next few classes.",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#objectives",
    "href": "content/06-content.html#objectives",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine coordinate, coordinate system, datum, and coordinate reference system\nAccess coordinate and geometry information for simple features in R\nUnderstand the rules for simple feature geometries\nAccess and transform the coordinate reference system for vector and raster data in R",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording",
    "crumbs": [
      "Content",
      "Course content",
      "6: Areal Data - Coordinates and Geometries"
    ]
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Introduction to Spatial Data in R",
    "section": "",
    "text": "Now that we’ve covered some of the conceptual bases of spatial data and geographic analysis, it’s time to get started working with actual data in R. Today’s readings and lecture are focused on the basics of getting your data into the R environment and familiarizing yourself with the different components that make up spatial data objects. We’ll do fancier things in the weeks to come!",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Introduction to Spatial Data in R",
    "section": "Readings",
    "text": "Readings\n Chapter 2 in Geocomputation with R (Lovelace et al. 2019) provides and overview of using sf for vector datasets and terra for raster data.\n Chapter 2 from Moraga (2023) explores similar topics, but provides more explanation about projections, coordinates, etc.\n Simple Features for R provides a more in-depth, technical discussion of how the sf package organizes spatial data and attributes.\n Ch 2.1-2.5 from Spatial Data Science with R and terra describes the basic functionality of terra for both vector and raster datasets. For reasons we’ll discuss in class, we will rarely use terra for vector data.",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#objectives",
    "href": "content/04-content.html#objectives",
    "title": "Introduction to Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRead spatial data into your R environment.\nDescribe the various aspects of spatial data files and objects.\nGenerate simple summaries describing the spatial data object.\nDetermine the projection, extent, and resolution of spatial data objects.",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Introduction to Spatial Data in R",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "4: Intro to Spatial Data with R"
    ]
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Why Geographic Analysis",
    "section": "",
    "text": "Before we get inundated with technical details and syntax, I think it’s important to remind ourselves why we need geographical analysis. We’ll spend some time on the various conceptualizations of place and space, how those things show up in geographic data, and their implications for the kinds of science we can do when we’re using geographic data. This session is meant to provide a bit of philosophical foundation for you to keep in mind as you work through various parts of the analytic pipeline.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Why Geographic Analysis",
    "section": "Readings",
    "text": "Readings\nThe following readings are intended to give you some sense of the discussion surrounding the role of spatial data in understanding the world. They are a mix of old favorites and relatively recent reviews. You don’t need to read all of them or memorize them, but they are worth a skim. I bet you’ll find something interesting.\n Conservation biogeography: assessment and prospects by Whitaker et al. (2005) provides an overview of the of geography in understanding ecosystem function and shaping conservation strategies.\n Economic geography, politics, and policy by Rickard (2020) provides a review of the role of geography in understanding responses to globalization.\n Revolutionary and counter revolutionary theory in geography and the problem of ghetto formation by David Harvey (1972) offers a scathing critique of quantitative geography (though Harvey was one of the founders of the field). See (Barnes 2009) for a relatively recent attempt to reconcile these views.\n Does scale exist? An epistemological scale continuum for complex human–environment systems by Steven Manson (2008) is one of my favorite summaries of the various definitions and confusion surrounding scale as a concept invoked in many disciplines.\n Spatial Scaling in Ecology by John Wiens (1989) describes the fundamental challenges of scale in Ecology.",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#objectives",
    "href": "content/02-content.html#objectives",
    "title": "Why Geographic Analysis",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine what we mean by description, explanation, and prediction in geography.\nDescribe critiques and limitations of quantitative geographical analysis\nDefine ‘scale’ and its implications for geographic analysis\nPlace your research in the broader context of geographic analysis",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Why Geographic Analysis",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "2: Why Geographic Analysis"
    ]
  },
  {
    "objectID": "example/session-25-example.html",
    "href": "example/session-25-example.html",
    "title": "Session 25 code",
    "section": "",
    "text": "Load library and data:\n\n\nCode\nlibrary(ggplot2)\n\nbikes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/z3tt/graphic-design-ggplot2/main/data/london-bikes-custom.csv\",\n                         col_types = \"Dcfffilllddddc\"\n)\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\n\nPlaying with aesthetics, scales, facets, and coordinates:\n\n\nCode\nggplot(bikes, aes(x=temp_feel, y=count)) +\n  geom_point(aes(color = day_night,\n                 shape = is_workday),\n             size=1.5) +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x=temp_feel, y=count, color = day_night)) +\n  geom_point(aes(shape = is_workday),\n             size=1.5) +\n  geom_smooth() +\n  facet_wrap(~is_workday) +\n  coord_flip()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x=count, y=temp_feel, color = day_night)) +\n  geom_point(aes(shape = is_workday),\n             size=1.5) +\n  geom_smooth() +\n  facet_wrap(~is_workday)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Examples",
      "Visualizing Spatial Data",
      "Data Visualization I"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Carolyn Koehn\n   4034-1 Environmental Research Building\n   carolynkoehn@u.boisestate.edu \n   Schedule an appointment\n\n\n\n\n\n   Mondays and Wednesdays\n   August 19–December 6, 2024\n   1:30-2:45 PM\n   Riverfront Hall Room 102B\n   Slack"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nSpatial data are ubiquitous and form the basis for many of our inquiries into social, ecological, and evolutionary processes. As such, developing the skills necessary for incorporating spatial data into reproducible statistical workflows is critical. In this course, we will introduce the core components of manipulating spatial data within the R statistical environment including managing vector and raster data, projections, extraction of data values, interpolation, and plotting. Students will also learn to prototype and benchmark different workflows to aid in applying the appropriate tools to their research questions."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course Objectives",
    "text": "Course Objectives\nStudents completing this course should be able to:\n\nArticulate the opportunities and challenges posed by geographic analysis.\nSelect the appropriate R packages and functions for manipulating different types of spatial data\nDesign statistical analyses that integrate geospatial and tabular data\n\nConstruct appropriate data visualizations for conveying geospatial data\nDevelop reproducible workflows for manipulating, visualizing, and analyzing spatial data."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "Expectations",
    "text": "Expectations\nBe nice. Be honest. Try hard.\nThe beauty of working with open source software is the community of users working on problems just like yours (and nothing like yours). Like any community, this one functions best when its members are kind, genuine, and make good-faith efforts to solve their problems along the way (more on this below).\nYou can (and should) expect me to:\n\nCreate a space where you can ask questions without fear of embarrassment or retribution\nProvide feedback on your work within 1 week of submission\nRespond to email and slack messages within 48 hours\nMake every attempt to answer your questions (when I can) or point you toward resources that may help\n\nIn turn, I expect you to:\n\nTreat all of us with respect and compassion\nMake an honest effort to work through the assignments\nDemonstrate that you have tried to solve your coding errors before asking me\nCommunicate with me when the course isn’t working for you"
  },
  {
    "objectID": "syllabus.html#prerequisite-knowledge-and-skills",
    "href": "syllabus.html#prerequisite-knowledge-and-skills",
    "title": "Syllabus",
    "section": "Prerequisite Knowledge and Skills",
    "text": "Prerequisite Knowledge and Skills\nYou can succeed in this class.\nSome familiarity with the R statistical environment is helpful, but not necessary. My goal is to foster an environment where we are all learning from each other and sharing the tips and tricks that help us along the way. Learning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. I find it helpful to remember the following:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. Even experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n— Hadley Wickham\n\n\nIf you want to start learning a few of the basics, the Resources tab has some background information to get you started. Note that this is not an exhaustive list - the number of new R tutorials available on the internet seems to be growing exponentially.\n\nGetting Help With R problems\nI am happy to help you work through your R coding challenges, but there are a lot of you and only one of me. Moreover, I may not always know exactly how to fix your problem any better than you do. In order to make sure that I am not the primary obstacle to your ability to complete the class assignments, I’m asking that you use the following steps prior to emailing/Slacking me with your coding questions.\n\n\n\n\n\n\nTip\n\n\n\nWhen you send me a question, please let me know what you searched, why the solutions you found don’t work for you, and what output you are expecting**\n\n\nWe’ll spend a bit of time on asking better questions and getting better answers so don’t worry if you aren’t quite sure how this all works.\n\nGoogle it! Searching for help with R on Google can sometimes be tricky. Google is generally smart enough to figure out what you mean when you search for “r reproject polygons”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats reproject polygons”). Also, since most of your R work will deal with the RSpatial packages, it’s often easier to just search for the package name and operation rather than the letter “r” (e.g. “sf reproject polygons”). I often paste the specific error message I get along with the spatial package I’m using to try and help Google find my solutions.\nAsk your colleagues We have an r_spatial chatroom at Slack where anyone in this class can ask questions and anyone can answer. Ask questions about code or class materials. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too. As a bonus, Slack allows you to format code to make it easy for all of us to copy and paste your code and distinguish it from the rest of your question.\nUse the forums Two of the most important sources for help with R-coding are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). If you aren’t able to find an answer to your question from the thousands of existing questions, you can post your own. You’ll need to create a reproducible example so others can figure out what you’re trying to do and what error you’re receiving, but you’d be amazed how helpful the community can be.\nAsk me! Sign up for a time to meet with me during student hours at https://calendly.com/carolynkoehn-u. I’ll want to know what searches you’ve tried (so I don’t chase down answers that you’ve already seen) and what approaches you’ve tried and why they haven’t worked. Remember, I’m here to help (but not write your code for you)."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nR and RStudio\nR is free, but it can sometimes be a pain to install and configure especially when dealing with spatial packages (we’ll talk more about why this is during class). To make life easier, I have set up an online RStudio server service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer and we should be able to avoid a number of the machine-specific issues that pop-up when 20 students have 20 different computers, operating systems (OS), etc. If you haven’t installed R on your local machine and would like some help getting that set up, there’ a useful set of instructions for installing R, RStudio, and all the tidyverse packages here.\n\n\nGit and Github Classroom\nAll assignments will be managed using GitHub classroom. This will allow each you to have your own repositories for each assignment and make it easier for me to comment on and help with your code. To use this, you should sign up for the GitHub Student Developers Pack as soon as possible and send me your github username. Once I have that, I can add you to the course and make sure that you have access to all of the necessary data and example code.\n\n\nReadings\nThe goal of this course is primarily to get you started with spatial workflows in R. That said, maps (and the spatial data that produce them) are extremely powerful and their use comes with risks and responsibilities. Although most of this course will focus on getting the code right, I’ll mix in a few readings each week to help tie the technical details of our code back to the broader contexts of spatial analysis or to illustrate new applications of the methods you are learning."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThis course is organized in 4 sections:\n\nGetting Started: What is spatial analysis and how do we do it in R?\nSpatial Data Operations in R: Prepping geospatial data for use in R\nStatistical Workflows for Spatial Data: Putting spatial data to work!\nVisualizing Spatial Data: Everyone loves a map…\n\nThe schedule page provides an overview of what to expect each week.\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the course progresses."
  },
  {
    "objectID": "syllabus.html#assignments-and-grades",
    "href": "syllabus.html#assignments-and-grades",
    "title": "Syllabus",
    "section": "Assignments and Grades",
    "text": "Assignments and Grades\nI teach this course because I believe that a) we can learn a lot about social and ecological processes by studying where they happen, b) integrating spatial analysis directly into statistical workflows makes those analyses more robust and reproducible, and c) overcoming coding challenges can provide a profound sense of accomplishment. That said, I recognize that there are many reasons that you are taking this course and that my objectives may differ from yours. In order to make sure that you get what you need out of this class, we’ll be using a mix of approaches for determining your grade in this course.\nSelf-assessment (12.5 pts x 2):  During the first week of the course, I’m going to ask you to reflect on what you want out of this course (concepts, skills, practice, etc.). This assessment will help me do a better job of aligning the content of the course to your specific needs. Grading for the self-assessment is described on the assignments page.\nExercises (5pts x 10): There are ten homework assignments. These exercises are designed to reinforce the material we cover in lecture, give you practice designing and implementing your own workflows, and build habits that promote reproducibility in science. They also allow me get a sense for your engagement in the course. Exercises are due at 11:59PM on their due date (generally Thursdays). I will post the “key” within 3 days of the due date and will not accept submissions after the key is posted. If you turn the assignment in on-time with the required number of commits, you’ll receive full credit.\nAssignment Revisions (25pts x 3): We will have three “assignment revisions” due during the course. These provide an opportunity for me to check in and see how things are going. You’ll be able to update your responses to the homeworks based on the keys and reflect on what you’ve learned throughout the course of the assignments. You’ll also be able to provide additional feedback on how the course is going for you. Rather than assign arbitrary points to each assignment, I’m going to grade your assignment revisions using the following ‘levels’ (inspired by Sarah K. Johnson’s description of her graduate data analysis course at Tufts):\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your Rmarkdown document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before you get to the next assessment. Failure to resubmit will result in no credit for the assessment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. You’re welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\nFinal Project (50pts):  The final project asks you to conduct an entire spatial analysis from layout to results. Grades on the final project are based on your objectives and your self-assessment of whether or not you achieved those objectives. Your first draft of the final project will be due December 5. I’ll make comments based on the same categories for the homework revision and you’ll have time to revise your submission prior to the final deadline of December 12.\nYou can find descriptions for all the assignments on the assignments page.\n\nGrades\nWe’ll use a form of contract grading to determine your grades in the course. Contract grading allows us to have a conversation about what you want out of the course, what you expect to put into it, and what I think you need to be successful in deploying the skills we learn here. Based on your goals for course, we’ll sign a contract that instantiates your objectives into the grade you’ll receive for the course. Complete the assignments and meet your objectives and you’ll get the grade you chose.\nThe expectations for the grades are:\n\nA You complete all of the self-assessments and at least 8 of the exercises. All of the assignment revisions achieve the “Good to Go” level. Your final project achieves the “Good to Go” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nB You complete all of the self-assessments and at least 8 of the exercises. At least one of the assignment revisions achieves the “Good to Go” level with the remainder achieving “Resubmit if you like”. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nC You complete all of the self-assessments and at least 6 of the exercises. All of your assignment revisions achieve the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nD You complete all of the self-assessments and at least 4 of the exercises. At least one of your assignment revisions achieves the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\n\n\n\nAttendance and incomplete assignments\nAttendance is an important part of this course. You are allowed to miss 2 classes without providing any justification (stuff happens). Beyond that, each additional absence will result in a 0.5 grading reduction (i.e., an A becomes and A-). Similarly, completing the assignments to a satisfactory level is vital to ensure you have a firm grip on the code and concepts. Hence, each assignment that fails to achieve a “Resubmit If You Like” will result in 0.5 grading reduction.\n\n\nLate work\nI would highly recommend staying caught up as much as possible, but if you need to turn something (other than the exercises and final project) in late, that’s fine—there’s no penalty."
  },
  {
    "objectID": "syllabus.html#student-wellbeing",
    "href": "syllabus.html#student-wellbeing",
    "title": "Syllabus",
    "section": "Student Wellbeing",
    "text": "Student Wellbeing\nIf you are struggling for any reason (COVID, relationship, family, or life’s stresses) and believe these may impact your performance in the course, I encourage you to contact the Dean of Students at (208) 426-1527 or email deanofstundents@boisestate.edu for support. If you notice a significant change in your mood, sleep, feelings of hopelessness or a lack of self worth, consider connecting immediately with Counseling Services (1529 Belmont Street, Norco Building) at (208) 426-1459 or email healthservices@boisestate.edu.\n\nLearning during a pandemic\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise."
  },
  {
    "objectID": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "href": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "title": "Syllabus",
    "section": "This course was designed with you in mind",
    "text": "This course was designed with you in mind\nI developed this course to provide a welcoming environment and effective, equitable learning experience for all students. If you encounter barriers in this course, please bring them to my attention so that I may work to address them.\n\nThis class’s community is inclusive.\nStudents in this class represent a rich variety of backgrounds and perspectives. The Human-Environment Systems group is committed to providing an atmosphere for learning that respects diversity and creates inclusive environments in our courses. While working together to build this community, we ask all members to: * share their unique experiences, values, and beliefs, if comfortable doing so.\n\nlisten deeply to one another.\nhonor the uniqueness of their peers.\nappreciate the opportunity we have to learn from each other in this community.\nuse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the campus community.\nrecognize opportunities to invite a community member to exhibit more inclusive, equitable speech or behavior—and then also invite them into further conversation. We also expect community members to respond with gratitude and to take a moment of reflection when they receive such an invitation, rather than react immediately from defensiveness.\nkeep confidential any discussions that the community has of a personal (or professional) nature, unless the speaker has given explicit permission to share what they have said.\nrespect the right of students to be addressed and referred to by the names and pronouns that correspond to their gender identities, including the use of non-binary pronouns.\n\n\n\nWe use each other’s preferred names and pronouns.\nI will ask you to let me know your preferred or adopted name and gender pronoun(s), and I will make those changes to my own records and address you that way in all cases.\nTo change to a preferred name so that it displays on all BSU sites, including Canvas and our course roster, contact the Registrar’s Office at (208) 426-4249. Note that only a legal name change can alter your name on BSU official and legal documents (e.g., your transcript).\n\n\nThis course is accessible to students with disabilities.\nI recognize that navigating your education and life can often be more difficult if you have disabilities. I want you to achieve at your highest capacity in this class. If you have a disability, I need to know if you encounter inequitable opportunities in my course related to:\n\naccessing and understanding course materials engaging with course materials and other students in the course\ndemonstrating your skills and knowledge on assignments and exams.\n\nIf you have a documented disability, you may be eligible for accommodations in all of your courses. To learn more, make an appointment with the university’s Educational Access Center.\n\n\nFor students responsible for children\nI recognize the unique challenges that can arise for students who are also parents or guardians of children. Any student needing to temporarily bring children or another dependent to class is welcome to do so to stay engaged with the class."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the principle that asks students to engage with their academic work to the fullest and to behave honestly, transparently, and ethically in every assignment and every interaction with a peer, professor, or research participant. When a strong culture of academic integrity is fostered by students and faculty in an academic program, students learn more, build positive relationships and collaborations, and can feel more confident in the value of their degrees.\nIn order to cultivate fairness and credibility, everyone must participate in upholding academic integrity. Students in this class are responsible for asking for help or clarification when it’s needed, speaking up when they see unethical behavior taking place, and understanding and adhering to the Student Code of Conduct, including the section on academic misconduct. Boise State and I take academic misconduct very seriously. It’s important to know that when a student engages in academic misconduct, I will report the incident to the Office of the Dean of Students. I also have the right to assign sanctions, which could include requirements to revise or redo work, complete educational assignments to learn about academic integrity, and grade penalties ranging from lower credit on an assignment to failing this class1. Students should learn more by reviewing the Student Code of Conduct."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo seriously, just don’t cheat or plagiarize!↩︎"
  },
  {
    "objectID": "slides/29-slides.html#storing-your-data",
    "href": "slides/29-slides.html#storing-your-data",
    "title": "Conclusion",
    "section": "Storing your data",
    "text": "Storing your data\n\nKyle’s shared Google drive folder syncs to: /opt/data/2023/project_data/\n\nRead Only\nSyncs hourly\n\nYou have write access to: /opt/projects/\n\nIntermediate files\nThings you don’t want others to see"
  },
  {
    "objectID": "slides/29-slides.html#by-the-end-of-this-course-you-should-be-able-to",
    "href": "slides/29-slides.html#by-the-end-of-this-course-you-should-be-able-to",
    "title": "Conclusion",
    "section": "By the end of this course you should be able to…",
    "text": "By the end of this course you should be able to…\n\nArticulate the opportunities and challenges posed by geographic analysis.\nSelect the appropriate R packages and functions for manipulating different types of spatial data\nDesign statistical analyses that integrate geospatial and tabular data\nConstruct appropriate data visualizations for conveying geospatial data\nDevelop reproducible workflows for manipulating, visualizing, and analyzing spatial data."
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. \n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Getting started with R Spatial"
    ]
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. \n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Getting started with R Spatial"
    ]
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to R and coding, potentially interesting data, and cool visualizations. Let me know when you find fun things to include here!",
    "crumbs": [
      "Resources",
      "Overview",
      "Resources"
    ]
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Fun datasets",
    "section": "",
    "text": "So much data, so little time… Here are some links to help you get started finding data for your geospatial projects",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#spatial-data-repositories",
    "href": "resource/data.html#spatial-data-repositories",
    "title": "Fun datasets",
    "section": "Spatial Data Repositories",
    "text": "Spatial Data Repositories\n\nDataBasin: Lots of spatial data related to conservation issues across the US.\nThe AdaptWest portal has tons of spatial data on climate change and its potential impacts.\nUS Protected Areas Database: PAD-US is America’s official national inventory of U.S. terrestrial and marine protected areas that are dedicated to the preservation of biological diversity and to other natural, recreation and cultural uses, managed for these purposes through legal or other effective means. PAD-US also includes the best available aggregation of federal land and marine areas provided directly by managing agencies, coordinated through the Federal Geographic Data Committee (FGDC) Federal Lands Working Group.\nUSGS Gap Analysis Project: A variety of datasets depicting land cover and species distributions.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#general-data-repositories",
    "href": "resource/data.html#general-data-repositories",
    "title": "Fun datasets",
    "section": "General Data Repositories",
    "text": "General Data Repositories\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#political-science-and-economics-datasets",
    "href": "resource/data.html#political-science-and-economics-datasets",
    "title": "Fun datasets",
    "section": "Political science and economics datasets",
    "text": "Political science and economics datasets\nThere’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\nInside AirBnB a Creative Commons-licensed dataset with a ton of spatially referenced info on AirBnBs in cities across the globe.",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "resource/data.html#the-30daymapchallenge",
    "href": "resource/data.html#the-30daymapchallenge",
    "title": "Fun datasets",
    "section": "The #30daymapchallenge",
    "text": "The #30daymapchallenge\nThe #30daymapchallenge is a social mapping/cartography/data visualization challenge designed to encourage experimentation with different types of datasets and mapping approaches. Searching the hashtag on social media (especially Twitter) will bring up a bunch of cool examples. Here are a few repositories to help you get started:\n\nThe Official #30DayMapChallenge Repo has an archive of past challenges and a description of what this is all about.\nBob Rudis’ 2019 bookdown project Contains both code and useful information for generating the visualizations along with sources for data.\nAlexandra Kapp’s 2020 repository makes use of some of the newer animation and interactive visualization techniques.\nThe R-Spatial list of 2020 challenge repositories",
    "crumbs": [
      "Resources",
      "Overview",
      "Fun datasets"
    ]
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section has the code and description from the live-coding exercises that we’ll do in class.",
    "crumbs": [
      "Examples",
      "Overview",
      "Examples"
    ]
  },
  {
    "objectID": "content/24-content.html",
    "href": "content/24-content.html",
    "title": "Statistical Modelling Review",
    "section": "",
    "text": "We reviewed the solutions to Assignment 9: Statistical Analyses found here.",
    "crumbs": [
      "Content",
      "Course content",
      "24: Statisitcal Modelling Review"
    ]
  },
  {
    "objectID": "content/24-content.html#objectives",
    "href": "content/24-content.html#objectives",
    "title": "Statistical Modelling Review",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nFit multiple types of models to your data\nEvaluate the performance of your model\nGenerate predictions based on your model",
    "crumbs": [
      "Content",
      "Course content",
      "24: Statisitcal Modelling Review"
    ]
  },
  {
    "objectID": "assignment/self-eval2.html",
    "href": "assignment/self-eval2.html",
    "title": "Self-reflection 2",
    "section": "",
    "text": "This is the second of three self-reflections that you’ll complete during the course. We’ll revisit your objectives and check-in on what I can do to help you get the most out of the second half of the course. This is our mid-semester “adjustment”. As such, I’m asking that you complete this in a timely fashion and turn it in by Oct 21. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval2.html#instructions",
    "href": "assignment/self-eval2.html#instructions",
    "title": "Self-reflection 2",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-2-xxx.qmd and give it a title (like M Williamson Self-Reflection 2). Make sure that you select the html output option.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-2-xx.qmd, and self-reflection-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "The main goals of this class is class is to get you comfortable with the manipulation, analysis, and visualization of spatial data using the R computing environment. These assignments are designed to help you practice those skills and reflect on your progress. All of the assignments have their own repository in our GitHub classroom, so you’ll submit them there. I’ll transfer grades over to Canvas so you’ll have an idea where you’re at in the course, but we won’t use Canvas for much else.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#self-reflections",
    "href": "assignment/index.html#self-reflections",
    "title": "Assignments",
    "section": "Self-reflections",
    "text": "Self-reflections\nThis course is collaborative. I’m hoping to provide a broad suite of information that can help you analyze spatial data for your graduate research and beyond. That said, you know more than I do about your personal and professional objectives. During the course of the semester, I’ll ask you to submit 2 self-reflections. The first should help me get to know you and get us on the same page with respect to your goals for the course. The last provides you with an opportunity to reflect on your progress in the course relative to your own objectives and give me feedback on how I can improve the course. These self-reflections are the foundation of how you’ll be ‘graded’ in this course, so they are mandatory and need to be submitted by the due date",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nI’ve created ten assignments to practice outlining a workflow, writing R code, and troubleshooting errors. The assignments rely on these datatasets and are designed to provide “bite-sized” practice to help cement the topics covered in the (generally 2) lectures that each applies. That said, they may take some and I’d encourage you to start on them early so that we have time to work on any questions you have. Moreover, long coding days are the worst so working in bite size chunks can help keep things fun! Finally, working incrementally will also to encourage you to get in the habit of using git to keep track of your progress.\nI’ll be grading these according to:\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your quarto document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before the next assignment. Failure to resubmit will result in no credit for the assignment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. You are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\n\n\n\n\n\n\nTip\n\n\n\nLate Work: There is no such thing as ‘late work’ with these assignments. Life happens, sometimes things take longer to finish than you expect. If you turn it in, I’ll give you feedback. That said, the assignments build on each other so it’s probably best to avoid falling too far behind.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your knowledge of spatial analysis workflows through a final project that requires you to integrate a variety of spatial datasets, analyze the data with respect to a question of interest, and create visuals that help you interpret the data.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignment/data.html",
    "href": "assignment/data.html",
    "title": "Assignment Datasets",
    "section": "",
    "text": "In order to reduce the fatigue of learning new datasets for each assignment, we’ll try to keep things limited to a handful of point, vector, and raster datasets. I’m going to simplify them a bit for the sake of loading times, but you can take a look at the webpages for each dataset to get a sense for what the originals look like."
  },
  {
    "objectID": "assignment/data.html#fire-occurrence-data",
    "href": "assignment/data.html#fire-occurrence-data",
    "title": "Assignment Datasets",
    "section": "Fire Occurrence Data",
    "text": "Fire Occurrence Data\nThe US Forest Service maintains a geospatial archive of historic and ongoing fire occurrence ignition locations. It’s a massive dataset, but you can play around with a web-viewer to see the sorts of things they report."
  },
  {
    "objectID": "assignment/data.html#wildfire-risk-to-communities",
    "href": "assignment/data.html#wildfire-risk-to-communities",
    "title": "Assignment Datasets",
    "section": "Wildfire Risk to Communities",
    "text": "Wildfire Risk to Communities\nThis is a recent product that is being used to determine where priorities for fire mitigation and restoration should be located. They’ve been the tool underlying the most recent round of Community Wildfire Defense Grants."
  },
  {
    "objectID": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "href": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "title": "Assignment Datasets",
    "section": "Climate and Economic Justice Screening Tool",
    "text": "Climate and Economic Justice Screening Tool\nThe CEJST combines a number of interesting datasets to identify locations where environmental justice concerns are likely to be highest. We’ll generally be using the underlying data (not the classification itself), but it’s worth checking out just to get a sense for what’s there.\nIn general, I’ll do my best to keep us using these datasets and give you a heads up and description if we need to deviate (e.g., for the networks unit)."
  },
  {
    "objectID": "assignment/12-vis.html",
    "href": "assignment/12-vis.html",
    "title": "Assignment 10: Data Visualization",
    "section": "",
    "text": "This semester we’ve spent a lot of time processing spatial datasets for the Pacific Northwest. In the past few lectures, we’ve talked about visualizing data and making maps. This assignment is a chance to practice pulling online data into R, processing unfamiliar spatial data, and making maps that combine multiple data sources and use multiple aesthetics/scales.\nBy the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assignment/12-vis.html#instructions",
    "href": "assignment/12-vis.html#instructions",
    "title": "Assignment 10: Data Visualization",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-10-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-10-xxx.qmd and give it a title (like M Williamson Assignment 10). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-10-xx.qmd, and assignment-10-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assignment/12-vis.html#the-assignment",
    "href": "assignment/12-vis.html#the-assignment",
    "title": "Assignment 10: Data Visualization",
    "section": "The Assignment",
    "text": "The Assignment\n\nGet data to visualize from the following sources:\n\n\nUse the occ_search function from rgbif to pull at least 1000 observations of a species of your choice in a country of your choice: occurance.df &lt;- occ_search(scientificName = \"Any Species Name\", country = \"Any Country\", hasCoordinate = TRUE, limit=1000)\nUse the geodata package to pull a raster of interest - some variable that you think is relevant to your species. Possible functions for different types of data can be seen here. Here’s an example: soils &lt;- geodata::soil_world(var = \"nitrogen\", depth=5, path=tempfile()). Use path=tempfile() to download a version that will be deleted when the R server restarts, but if this give you trouble, use path=getwd(). If you do this, do not commit this data and push it to GitHub.\nUse the geodata package to pull in state/province/equivalent level boundaries for your country of choice using geodata::gadm(country = \"Your Country\", level=1, path=tempfile()). The same tempfile() instructions for part b apply here. This data will be downloaded as a SpatVector and can be transformed to an sf object with st_as_sf().\n\n\nWrite pseudocode for how you will prepare your data for visualization, then execute your plan. Some possible objectives might be cropping your data to an area of interest and transforming the data to tidy format.\nUse ggplot2 to create a map of the raster data with the species presence points overlayed on top. Add state/province/equivalent level boundaries.\nChange the raster color scale, legend name, title, and theme from ggplot2 defaults. You can try any other ggplot customization you’d like now as well.\nUse tmap to recreate this plot with zooming functionality and any other interactive elements you’d like to add. Optionally, you can substitute the raster for tidycensus or other polygon data at this stage.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Data Visualization"
    ]
  },
  {
    "objectID": "assignment/10-secondrevision.html",
    "href": "assignment/10-secondrevision.html",
    "title": "Assignment Revision 2: Revisiting your code",
    "section": "",
    "text": "This is your second opportunity to reconsider your answers to the previous two assignments and evaluate what you might have done differently now that you’ve had a little more practice. Solutions (or suggestions) for how I’d approach past assignments are posted (at the end of each assignment page). Your task here is to review those solutions and your own code and answer a few questions to demonstrate what you’ve learned so far and where I need to be more clear. I’ve also asked some specific questions based on common challenges across the assignments. You’ll still be using Quarto to complete this homework.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 2"
    ]
  },
  {
    "objectID": "assignment/10-secondrevision.html#instructions",
    "href": "assignment/10-secondrevision.html#instructions",
    "title": "Assignment Revision 2: Revisiting your code",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar2_xxx.qmd and give it a title (like M Williamson Assignment Revision 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar1_xx.qmd, and ar1_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 2"
    ]
  },
  {
    "objectID": "assignment/10-secondrevision.html#questions",
    "href": "assignment/10-secondrevision.html#questions",
    "title": "Assignment Revision 2: Revisiting your code",
    "section": "Questions",
    "text": "Questions\n\nImagine you have a spatial analysis planned that uses one tabular dataset, 2 vector datasets, and 2 raster datasets. What are the basic steps that you need to do before you can build a spatial database from all the data sources in R? You can write the steps in pseudocode, bullet point, or paragraph form.\nWhat are your strategies for checking that an operation resulted in the output you expected? In general, how do you identify and correct syntax errors and semantic errors (see session 6 for definitions) in your work? If you don’t, what strategies would you like to try going forward?\nCompare your assignments 6-7 with the posted solutions. Were you able to answer all the questions? If not, correct and notate your code and push the changes to GitHub.\nAs you look back on assignments 6-7, what has been the biggest challenge? Do you feel like you know how to solve it? How can I help?\nAs you look back on the past few weeks of lectures, what is the one thing you appreciate and one thing that you wish I would do differently? (I’ll do my best!)\nTell me about your final project - how have your plans progressed since the last time you checked in with me?",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 2"
    ]
  },
  {
    "objectID": "assignment/08-combinations.html",
    "href": "assignment/08-combinations.html",
    "title": "Assignment 7: Combining Data Types",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/08-combinations.html#instructions",
    "href": "assignment/08-combinations.html#instructions",
    "title": "Assignment 7: Combining Data Types",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/06-vectorops.html",
    "href": "assignment/06-vectorops.html",
    "title": "Assignment 6: Vector Operations",
    "section": "",
    "text": "Now that you’ve been introduced to predicates, measures, and transformers in the sf package. You should be able complete a relatively simple workflow for a spatial analysis. We’ll build on this again with raster data (using terra) next week and then integrate both data models the week after that. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/06-vectorops.html#instructions",
    "href": "assignment/06-vectorops.html#instructions",
    "title": "Assignment 6: Vector Operations",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-6-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-6-xxx.qmd and give it a title (like M Williamson Assignment 6). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-6-xx.qmd, and assignment-6-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/06-vectorops.html#the-assignment",
    "href": "assignment/06-vectorops.html#the-assignment",
    "title": "Assignment 6: Vector Operations",
    "section": "The Assignment",
    "text": "The Assignment\nWe want to begin to assess the role of distance from schools in determining the education outcomes for Idahoans. We’ll use the landmarks_pnw.csv and cejst_pnw.shp datasets as the basis for this assignment. You’ll need to load the csv and convert it to an sf object. We want to compare the percentage of individuals age 25 or over with less than a high school degree (HSEF in the cejst dataset) for of counties within 50km of a school (MTFCC == K2543) to those that are more than 50km. You’ll need to follow many of the same operations in the video example from class. Your assignment is:\n\nWrite out the pseudocode for your analysis\nTranslate the pseudocode into code chunks and create the necessary code (You’ll need to use things like st_distance, st_buffer, st_sym_difference)\nMake a map for both the percentage of individuals with less than a high school degree in counties within 50km and beyond 50km (i.e. make 2 maps)\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Operations"
    ]
  },
  {
    "objectID": "assignment/04-maps.html",
    "href": "assignment/04-maps.html",
    "title": "Assignment 4: Predicates and Measures",
    "section": "",
    "text": "This is the fourth assignment of the semester for HES 505.\nNow that you’ve learned a bit about predicates and measures it’s time to practice on some vector and raster data. By the end of this assignment, you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#instructions",
    "href": "assignment/04-maps.html#instructions",
    "title": "Assignment 4: Predicates and Measures",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-4-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-4-xxx.qmd and give it a title (like M Williamson Assignment 4). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-4-xx.qmd, and assignment-4-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#the-data",
    "href": "assignment/04-maps.html#the-data",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Data",
    "text": "The Data\nWe will be using the landmarks data table and the shapefile from the Climate and Economic Justice Screening Tool from previous assignments. I’ve moved the versions for this assignment into the /opt/data/data/assignment04/ folder to make life easier.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/04-maps.html#the-assignment",
    "href": "assignment/04-maps.html#the-assignment",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Assignment",
    "text": "The Assignment\n\nLoad the cejst_nw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code).\nLoad the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset…\nFilter the cejst_nw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\nFinally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Predicates and Measures"
    ]
  },
  {
    "objectID": "assignment/02-introspatialsolutions.html",
    "href": "assignment/02-introspatialsolutions.html",
    "title": "Assignment 2 Solutions: Intro to Spatial Data",
    "section": "",
    "text": "Find a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative analysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n1. Create a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\n\nIn order to do this, you’ll need to start a new Quarto document (File -&gt; New File -&gt; Quarto Document). Once you’ve done that Rstudio will open up a Quarto document with the yaml header already in place and a bunch of example text and code. Delete that. Then use Markdown syntax to specify headings (# for top level headings, ## for second level headings, etc.) So in this case, once you’ve gotten all of the example stuff deleted, you can use # Introduction to create a section header with the correct title. Adding citations requires you to create a separate, .bib file that lives in the same directory as your document and has your citation info in BIBTEX format. You can find more on that here.\n\n2. Create a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in class (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\n\nNow I’ll add # Methods as my next header and start to write out the steps I’d like the analysis to follow. There are lots of ways you might do this, depending on whether you want the pseudocode to be part of your final product. For now we’ll keep it simple and just use numbered steps like:\n\n\nLoad data\nFilter the correct rows\nSelect the right variables\nModel y as a function of x1, x2, x3…\nPlot\n\n3. Add in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\n\nAdding in code blocks and giving is accomplished by setting up your code fence (```), identifying the language you want to use ({r}), and then setting code options with the hash-pipe (#|). You can see my example here\n\n4. Based on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\n\nIn order to ensure that the code prints, you need to add the #| echo: true execution option. In order to prevent it from running, you want to set #| eval: false. You can see this in my example.\n\n5. Add a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose. \n\nYou can add images in markdown by using the ![]() syntax where the file location is pasted in the parentheses and any caption is placed in the brackets."
  },
  {
    "objectID": "assignment/01-intro.html",
    "href": "assignment/01-intro.html",
    "title": "Assignment 1: Introductory material",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of geographic thought, core technical details of working with spatial data, and introduce R as a tool for end-to-end spatial workflows. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/01-intro.html#instructions",
    "href": "assignment/01-intro.html#instructions",
    "title": "Assignment 1: Introductory material",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/01-intro.html#questions-for-the-assignment",
    "href": "assignment/01-intro.html#questions-for-the-assignment",
    "title": "Assignment 1: Introductory material",
    "section": "Questions for the Assignment",
    "text": "Questions for the Assignment\n\nHow does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\nWhat are the primary components that describe spatial data?\nWhat is the coordinate reference system and why is it important\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Introducing the course"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html",
    "href": "assignment/02-introspatial.html",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "",
    "text": "This is the second assignment of the semester for HES 505.\nFor the rest of the course, I’ll be asking you to use pseudocode to plan your analysis steps before you start using any functions (or writing your own). Pseudocode allows you to think about the important steps of your process and identify your desired results before your start down the path of coding. You can think of pseudocode as an outline for syntax, much like the one you might use for writing an manuscript or report. Quarto documents are designed to let you both outline your report and plan your analysis all in the same place! This assingment is meant to give you some practice setting up your outlines before you start coding. By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html#instructions",
    "href": "assignment/02-introspatial.html#instructions",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-2-xxx.qmd and give it a title (like M Williamson Assignment 2). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-2-xx.qmd, and assignment-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/02-introspatial.html#the-assignment",
    "href": "assignment/02-introspatial.html#the-assignment",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "The Assignment",
    "text": "The Assignment\nFind a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative analysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n\nCreate a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\nCreate a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in clase (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\nAdd in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\nBased on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\nAdd a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Data and Quarto"
    ]
  },
  {
    "objectID": "assignment/03-vector.html",
    "href": "assignment/03-vector.html",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "",
    "text": "This is the third assignment of the semester for HES 505. The last few lectures have focused on coordinates and geometries. In this assignment, we’ll use the different functions for accessing and transforming the crs of different spatial objects. We’ll also use a little of the tidyverse to subset the data and access some of the geometry information for one of the observations in our dataset. You’ll need to use both the lectures and the recorded examples (or check out the tidyverse tutorials linked in the lectures). This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#instructions",
    "href": "assignment/03-vector.html#instructions",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-3-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-3-xxx.qmd and give it a title (like M Williamson Assignment 3). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-3-xx.qmd, and assignment-3-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#the-data",
    "href": "assignment/03-vector.html#the-data",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Data",
    "text": "The Data\nFor this assignment, you’ll be looking at 3 different datasets. One from the Center for Disease Control’s PLACES data describing the distribution of chronic health risks, one from the EPA describing exposure to PM2.5 (an important air pollutant), and one describing wildfire risk. You might imagine that as we become increasingly concerned with the environmental justice concerns associated with fire, we might be concerned about whether more smoke increases the risk of chronic respiratory diseases. We won’t totally answer that question this week, but you’ll start to develop the workflow necessary to move towards that type of analysis. All of the data are on the server in the opt/data/data/assignment03/ folder.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/03-vector.html#the-assignment",
    "href": "assignment/03-vector.html#the-assignment",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Assignment",
    "text": "The Assignment\n\nWrite out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\nRead in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\nRe-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tif file. Verify that all the files have the same projection.\nHow does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\nWhat class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Vector Data"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html",
    "href": "assignment/05-firstrevision.html",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "",
    "text": "Now that you’ve had some practice with R and the format of the course, it’s time to pause and take a moment to check in on what you’ve learned. Because we haven’t had a ton of coding yet, this review is a little more conceptual (rather than focusing on particular pieces you may have done incorrectly or inefficiently). My soluionts (or suggestions) for how I’d approach the first four assignments are posted (at the end of each assignment page). Your task here is to review those solutions and your own code and answer a few questions to demonstrate what you’ve learned so far and where I need to be more clear. You’ll still be using Quarto to complete this homework.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html#instructions",
    "href": "assignment/05-firstrevision.html#instructions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar1_xxx.qmd and give it a title (like M Williamson Assignment Revision 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar1_xx.qmd, and ar1_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/05-firstrevision.html#questions",
    "href": "assignment/05-firstrevision.html#questions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Questions",
    "text": "Questions\n\nHow do you introduce yourself to git? (look at some of the initial lectures for the course) How often do you need to introduce yourself when you are on the RStudio server? What is the difference between a “commit” and a “push”?\nHow do you check the Coordinate Reference System of a vector file using the sf package? What about for a raster file using the terra package? Why is the CRS important?\nWhat is the difference between a predicate and a measure?\nAs you look back on your first few assignments, what has been the biggest challenge? Do you feel like you know how to solve it? How can I help?\nAs you look back on the first few weeks of lectures, what is the one thing that you wish I would do differently (I’ll do my best!)\nTell me about your final project - what topic are you hoping to explore? What data are going to use? (Remember, this assignment works better if you’re using data that isn’t part of your thesis) If there are things I can do to help you find data or a topic, please let me know here.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 1"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html",
    "href": "assignment/07-rasterops.html",
    "title": "Assignment 7: Building spatial databases",
    "section": "",
    "text": "It’s time to put together the various vector, raster, and tabular code we’ve learned in order to build a dataframe that you can use for a subsequent statistical analysis. For this assignment we’ll be using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters from 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder, a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Your goal is to create a dataframe that includes the total cost of all disasters that have occurred within a National Forest Boundary along with several social and ecological variables that might help explain the difference in dollars expended to contain the hazard (typically fire). By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html#instructions",
    "href": "assignment/07-rasterops.html#instructions",
    "title": "Assignment 7: Building spatial databases",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-7-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-7-xxx.qmd and give it a title (like M Williamson Assignment 7). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-7-xx.qmd, and assignment-7-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/07-rasterops.html#the-assignment",
    "href": "assignment/07-rasterops.html#the-assignment",
    "title": "Assignment 7: Building spatial databases",
    "section": "The Assignment",
    "text": "The Assignment\n\nYou’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets.\nValidate your geometries and make sure all of your data is in the same CRS.\nSmooth the wildfire hazard and land use datasets using a 5x5 moving window; use the mean for the continuous dataset and the mode for the categorical dataset.\nEstimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\nNext join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\nMake a set of maps that shows the Forest-level values for all of your selected variables.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Spatial Databases"
    ]
  },
  {
    "objectID": "assignment/09-pointpatterns.html",
    "href": "assignment/09-pointpatterns.html",
    "title": "Assignment 8: Spatial Autocorrelation and Interpolation",
    "section": "",
    "text": "Exploring and exploiting spatial autocorrelation in our data is an important tool for interpolating missing values in geographic data. It’s also an important diagnostic to check before additional statistical analyses. You’ll get to try some of that out here. We’ll the cost of natural disasters dataset from 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder and focusing on the data describing the structures lost to natural disaster (STR_DESTROYED_TOTAL). For this assignment, we’ll use this data to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Point Patterns and Interpolation"
    ]
  },
  {
    "objectID": "assignment/09-pointpatterns.html#instructions",
    "href": "assignment/09-pointpatterns.html#instructions",
    "title": "Assignment 8: Spatial Autocorrelation and Interpolation",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-8-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-8-xxx.qmd and give it a title (like M Williamson Assignment 8). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-8-xx.qmd, and assignment-8-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Point Patterns and Interpolation"
    ]
  },
  {
    "objectID": "assignment/09-pointpatterns.html#the-assignment",
    "href": "assignment/09-pointpatterns.html#the-assignment",
    "title": "Assignment 8: Spatial Autocorrelation and Interpolation",
    "section": "The Assignment",
    "text": "The Assignment\n\nRead in the disasters dataset, convert it to points, filter it to those disasters in Idaho, and select any relevant columns. You’ll also need to use tigris::county() to download a county shapefile for Idaho. Make sure your data are projected correctly\nGenerate the Ripley’s K curves for the disaster dataset. What do you think? Is there evidence that the data is spatially autocorrelated?\nUse the nearest-neighbor approach that we used in class to estimate the lagged values for the disaster dataset and estimate the slope of the line describing Moran’s I statistic.\nNow use the permutation approach to compare your measured value to one generated from multiple simulations. Generate the plot of the data. Do you see more evidence of spatial autocorrelation?\nGenerate the 0th, 1st, and 2nd order spatial trend surfaces for the data. Is there evidence for a second order trend? How can you tell?\nNow use the spatial trend surface to perform some ordinary kriging. You’ll want to have a grid of ~15,000 points, fit 3 different experimental variogram functions (see the vgm function helpfile to learn more about the shapes available to you). Plot your variogram fits. Which one would you choose? Why?\nUsing your spatial trend model and your fitted variogram, krige the data and generate a map of the interpolated value and a map of the error.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Point Patterns and Interpolation"
    ]
  },
  {
    "objectID": "assignment/11-statmod.html",
    "href": "assignment/11-statmod.html",
    "title": "Assignment 9: Statistical Analyses",
    "section": "",
    "text": "You built a dataframe for analysis in Assignment 7 using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters fromm 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder. a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Now it’s time to fit some models to explain the cost of natural disasters. By the end of this assignment you should be able to:",
    "crumbs": [
      "Assignments",
      "Homework",
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "assignment/11-statmod.html#instructions",
    "href": "assignment/11-statmod.html#instructions",
    "title": "Assignment 9: Statistical Analyses",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of an R project named assignment-9-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-9-xxx.qmd and give it a title (like M Williamson Assignment 9). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-9-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "assignment/11-statmod.html#the-assignment",
    "href": "assignment/11-statmod.html#the-assignment",
    "title": "Assignment 9: Statistical Analyses",
    "section": "The Assignment",
    "text": "The Assignment\n\nUse the variables that you chose from assignment 7 along with the wildfire hazard and land use dataset to attribute each disaster in the disaster dataset.\nFit a Poisson regression using your covariates and the cost of the incident data (using glm with family=poisson())\nFit a regression tree using your covariates and the cost of the incident data (using caret package method='rpart')\nFit a random forest model using your covariates and the cost of the incident data (using caret package method= 'rf')\nUse cross-validation to identify the best performing model of the 3 that you fit\nConvert all of your predictors into rasters of the same resolution and generate a spatial prediction based on your best performing model\nPlot your result\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here",
    "crumbs": [
      "Assignments",
      "Homework",
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "assignment/13-thirdrevision.html",
    "href": "assignment/13-thirdrevision.html",
    "title": "Assignment Revision 3: Revisiting your code",
    "section": "",
    "text": "This is your final opportunity to reconsider your answers to the last few assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete this homework.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 3"
    ]
  },
  {
    "objectID": "assignment/13-thirdrevision.html#instructions",
    "href": "assignment/13-thirdrevision.html#instructions",
    "title": "Assignment Revision 3: Revisiting your code",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-3-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar3_xxx.qmd and give it a title (like M Williamson Assignment Revision 3). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar3_xx.qmd, and ar3_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 3"
    ]
  },
  {
    "objectID": "assignment/13-thirdrevision.html#questions",
    "href": "assignment/13-thirdrevision.html#questions",
    "title": "Assignment Revision 3: Revisiting your code",
    "section": "Questions",
    "text": "Questions\n\nAutocorrelation metrics:\n\n\nWhat can a Ripley’s K curve tell you about spatial autocorrelation?\nWhat about Moran’s I?\nLook at the posted answers for assignment 8. What do the Ripley’s K curve and the Moran’s I slope shown there tell you about the spatial autocorrelation of the data?\n\n\nWhen kriging, you need both a spatial trend surface and an experimental variogram. What is the role of each of these components in kriging?\nWhy are confusion matrices and ROC/AUC plots only useful for categorical dependent/outcome variables? What might you use instead for continuous dependent/outcome variables?\nWhat is the difference between a training/testing data split and k-fold or leave-one-out cross validation?\nNotate the changes you made to assignment 9 during class on 11/11 and push them to GitHub.\nWe’ve covered 3 of the 4 sections of this course so far:\n\n\nGetting Started: What is spatial analysis and how do we do it in R?\nSpatial Data Operations in R: Prepping geospatial data for use in R\nStatistical Workflows for Spatial Data: Putting spatial data to work!\n\nAs you think back across these sections, what is one topic/workflow you feel very confident doing? What topic/workflow that you expect to use in your research are you least confident in? What strategies do you think will help you feel more confident? What support could I or another spatial data expert provide that might help?\n\nWhich statistical approach do you plan to use for the final project? What challenges do you anticipate working through during your final project?",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Assignment Revision 3"
    ]
  },
  {
    "objectID": "assignment/final-proj.html",
    "href": "assignment/final-proj.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation techniques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#overview",
    "href": "assignment/final-proj.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation techniques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#requirements",
    "href": "assignment/final-proj.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\nDatasets. The ability to manipulate and integrate a variety of data types, resolutions, and formats is a key component of this course. Your analysis should incorporate at least 5 datasets. The ultimate compostion of your database is up to you, but I’d like you to include 1 tabular dataset, 1 vector dataset, and 1 raster dataset. You should choose the other 2 (or more) to give you practice with the data types that are most relevant to your objectives and/or research.\nAnalyses. You’ve learned several classes of analyses (e.g., overlays, point-pattern, multivariate regression, and statistical learning). Apply at least one (preferably the one most tied to your own objectives and research) of these analyses to address your question. In the course of doing so, you’ll need to justify your choice, assess whether your data meets appropriate assumptions, and evaluate the implications of key assumptions you make. For example, if you’re conducting an overlay analysis, how does your choice of threshold affect the ultimate result? If you’ve fit a statistical model based on summary statistics (e.g, mean, median), how well does the model fit? How does the model change if you use different slices of the data?\nVisualizations. You should produce a minimum of 3 visualizations to accompany your analysis. One of these should be a publication quality location map. The others are up to you, but should a) help you tell the story of your analysis and b) help you meet your objectives for the course and your own research. These can be additional maps of results, figures that summarize your data or results in non-spatial ways, or interactive graphics that allow you to explore parts of your analysis.\nReporting You can generate a ‘manuscript’ style document (using Quarto) or a flexdashboard (using Quarto and shiny) as the final product. Your report should include:\n\nA brief (1-2 paragraphs) description of your question and why you’re interested in it.\nA Methods section with subsections describing the data sources, any processing steps you took and why, and your process for the analysis. Show your code and provide annotation to describe what you are attempting do with the various steps.\nA Results section that includes tabular results as well as any relevant visualizations that describe your data and analysis.\nA Discussion of your results that puts your results in the context of your question, considers alternative analysis strategies and why they may or may not be better than the approach you chose, describes additional data that might be important for your question, and considers the role of extent and resolution in your analysis.",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/final-proj.html#assessment",
    "href": "assignment/final-proj.html#assessment",
    "title": "Final Project",
    "section": "Assessment",
    "text": "Assessment\nYou’ll submit a draft of the final report on December 5. I’ll give you feedback based on your project and on your objectives for the course. You’ll then have a chance to address my feedback before turning in your final draft on December 12. Your final self-assessment will ask you to reflect on your objectives for the course and evaluate the degree to which your final project demonstrates that you achieved your objectives. Thus, when you are designing your project, make sure that you have your initial objectives in mind.\n\n\n\n\n\n\nTip\n\n\n\nA note on grades: You will be responsible for assessing how well your assignment demonstrates that you achieved your objectives. I reserve the right to change the grade you’ve given yourself, but will provide clear justification for why I’m doing that. Without a completed self-assessment there is no grade for your final project, so please make sure you complete it.",
    "crumbs": [
      "Assignments",
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "assignment/self-eval1.html",
    "href": "assignment/self-eval1.html",
    "title": "Self-reflection 1",
    "section": "",
    "text": "This is the first of three self-reflections that you’ll complete during the course. These provide an opportunity for me to learn more about you, check-in on how the course is going, and make sure that getting what you need from the course materials. This first reflection also helps us set the standards for your assessment in the course. As such, I’m asking that you complete this in a timely fashion and turn it in by Aug. 23. You’ll need to accept this link to access the questions.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Self-reflection 1"
    ]
  },
  {
    "objectID": "assignment/self-eval1.html#instructions",
    "href": "assignment/self-eval1.html#instructions",
    "title": "Self-reflection 1",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.",
    "crumbs": [
      "Assignments",
      "Self Assessments",
      "Self-reflection 1"
    ]
  },
  {
    "objectID": "assignment/self-eval3.html",
    "href": "assignment/self-eval3.html",
    "title": "Self-reflection 3",
    "section": "",
    "text": "This is the final self-reflection for the course and provides a way of evaluating your final project relative to your course learning objectives. This final self-reflection is critical for assigning your grades on the final project and the course, in general. As such, I’m asking that you complete this in a timely fashion and turn it in by Dec. 16. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval3.html#instructions",
    "href": "assignment/self-eval3.html#instructions",
    "title": "Self-reflection 3",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings and slides",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture. On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).",
    "crumbs": [
      "Content",
      "Overview",
      "Readings and slides"
    ]
  },
  {
    "objectID": "lesson/index.html",
    "href": "lesson/index.html",
    "title": "Lessons",
    "section": "",
    "text": "This section has some worked examples demonstrating the use of different packages and giving some ‘roadmaps’ for completing different spatial operations. These are mostly my opinions, your mileage may vary, but I’ll try to justify why I do things the way that I do so that you can make an informed choice when you decide to deviate from that path.",
    "crumbs": [
      "Lessons",
      "Overview",
      "Lessons"
    ]
  },
  {
    "objectID": "resource/git.html",
    "href": "resource/git.html",
    "title": "Helpful git links",
    "section": "",
    "text": "Getting in the habit of using version control can be challenging, especially if you are collaborating with others. The challenge gets worse when some of those collaborators are not familiar with the importance of version control. Here are a few links to try and make your (and their) transition a little smoother.",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "href": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "title": "Helpful git links",
    "section": "Installing Git and making it play nice with R",
    "text": "Installing Git and making it play nice with R\nHappy git with R is Jenny Bryan’s extremely helpful introduction to git and incorporating it into your R workflow.",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "resource/git.html#getting-the-hang-of-git",
    "href": "resource/git.html#getting-the-hang-of-git",
    "title": "Helpful git links",
    "section": "Getting the hang of git",
    "text": "Getting the hang of git\nUnderstanding the logic of git: provides a relatively accessible explanation of the various operations in git and links that to commonly used syntax.\nOh Sh@t, Git?!?: A less technical, more irreverant introduction to git workflows and fixing the inevitable challenges of version control. (G-rated version available at Dang it, Git?!?).",
    "crumbs": [
      "Resources",
      "Guides",
      "Helpful git links"
    ]
  },
  {
    "objectID": "resource/lastyear.html",
    "href": "resource/lastyear.html",
    "title": "Past years’ classes",
    "section": "",
    "text": "2021 was the first time this class was taught in its current format. Matt Williamson built a number of worked examples to try to clarify how different parts of a spatial workflow come together. Although the examples may diverge a bit from the examples we do this year, the page does have a number of potentially useful pieces. Check out the examples to access these.",
    "crumbs": [
      "Resources",
      "Overview",
      "Past years' classes"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Authoring in Rmarkdown and Quarto",
    "section": "",
    "text": "[Rmarkdown] and [Quarto] are two powerful ways to combine writing (or presentations) and analysis into a single reproducible workflow. Although they can be a little more cumbersome to use than traditional word processors (e.g., Word or Pages) or presentation software (e.g., PowerPoint or Keynotes), they have the benefit of allowing you to keep all of the pieces of your manuscripts or presentations in one place. Change some data or analysis? The whole manuscript or presentation should update without forcing you to try and find all of the places where that change might alter your writing or slides. It may take a little getting used to, but the fact that both Rmarkdown and Quarto can utilize the power of LaTeX typesetting means that you’ll ultimately be able to produce publication quality equations, tables, and figures all in one place.\nThis webpage and all of my slides were built with Quarto. Having gone through the process of learning how make that work, I’m convinced that having a working knowledge of one or both of these is useful. As such, you’ll be using Rmarkdown or Quarto (your choice) to complete your assignments and render them to html.",
    "crumbs": [
      "Resources",
      "Guides",
      "Authoring in Rmarkdown and Quarto"
    ]
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.2.1) and download it.\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nThe tidyverse consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nYou can install packages manually in RStudio, but this can be a bit fragile, especially for some of the spatial packages. Instead of using the RStudio GUI we’ll just install thins at the prompt. To install the tidyverse pacakge (and all of its associated dependencies) run the following: install.packages(\"tidyverse\").",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎",
    "crumbs": [
      "Resources",
      "Guides",
      "Installing R, RStudio, and tidyverse"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Intro to Spatial Data in R\n        ",
    "section": "",
    "text": "Intro to Spatial Data in R\n        \n        \n            Use R to load, visualize, and analyze spatial data\n        \n        \n            HES 505 • Fall 2024Human-Environment SystemsBoise State University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n   Carolyn Koehn\n   4034-1 Environmental Research Building\n   carolynkoehn@u.boisestate.edu \n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   August 19–December 6, 2024\n   1:30-2:45 PM\n   Riverfront Hall Room 102B\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 48 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Getting Started",
    "section": "",
    "text": "Today we’ll focus on getting oriented to the course and the tools we’ll be using throughout the semester. Readings are designed to help understand some of the ‘rules’ of R syntax and develop an understanding for manipulating different types of data in R. I’ve added a few on open science and reproducibility because I think they help make the case for learning to build code-based workflows.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Getting Started",
    "section": "Readings",
    "text": "Readings\n\nThe syllabus, content, examples, and assignments pages for this class\n Chapter 1 - 6 in Venables et al., An Introduction to R (Venables et al. 2009) - for a quick refresher on data types in R (it’s only 30 pages)\n Chapters 1-2 in Douglas et al., An Introduction to R - provides another intro to R that’s been updated and is an open-source book.\n Happy Git and GitHub for the useR - all you really need to know to be a proficient user of git for version control and reproducible workflows.\n Open science, reproducibility, and transparency in ecology by Powers and Hampton - discusses the importance of open science for ecologists.\n Practical Reproducibility in Geography and Geosciences by Nilst and Pebesma - describes the importance of reproducibility for geospatial analysis.",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#objectives",
    "href": "content/01-content.html#objectives",
    "title": "Getting Started",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should:\n\nBe able to articulate the organization of the course, the approach to grading, and the requirements for the final project\nBe able to access the RStudio Server and Github classroom\nBe able to clone the first self-reflection and know the process for submitting assignments",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Getting Started",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "1: Introduction to the Course"
    ]
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Introduction to Spatial Data",
    "section": "",
    "text": "Now that you have a little more background in the breadth of philosophies, methods, and questions that geography encompasses, it’s time to start familiarizing yourself with the nature of spatial data. This lecture will be a little more conceptual, but is designed to help you make some sense for how R thinks about spatial data (or at least how the package developers have been thinking about it).",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "Introduction to Spatial Data",
    "section": "Readings",
    "text": "Readings\n Types of Spatial Data from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R provides a nice overview of the types of spatial data from the perspective of a statistical analyst.\n Attributes and Support from Pebesma and Bivand’s Spatial Data Science with Applications in R gives more info and examples on the nature of the relationship between geometries and support.\n Scale and Projections from Mapping, Society, and Technology by Laura Matson and Melinda Kernik gives a nice overview of the challenges associated with representing location on Earth’s surface.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#objectives",
    "href": "content/03-content.html#objectives",
    "title": "Introduction to Spatial Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nContrast the different “views” of spatial data and their incorporation in GIS.\nIdentify key elements that make data “spatial”.\nArticulate the importance of coordinate reference systems.\nRecognize the relationship between geometries and support.",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Introduction to Spatial Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "3: Introduction to Spatial Data"
    ]
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "",
    "text": "Today we’ll focus on some of the tools for reproducible workflows using R. We’ll introduce Quarto as a means of authoring different kinds of documents. We’ll talk about literate programming and leaving breadcrumbs for yourself (and others). Finally, we’ll begin to work through the ideas of workflow planning",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Readings",
    "text": "Readings\n\n Authoring in Quarto - an intro to Quarto for developing different kinds of documents. Lots of other resources linked here!!\n Pseudocode: what it is and how to write it - A nice blogpost by Sara Metawalli the sketches out the logic of pseudocode and why it can be helpful.\n The Whole Game - from Wickham et al., R for Data Science (Wickham and Grolemund 2016). Focus on the sections that begin with “Workflow” to get a sense for how we’ll start putting the pieces together.\n Scripts, algorithms, and functions - chapter 11 in in Lovelace et al., Geocomputation with R (Lovelace et al. 2019) introduces some concepts behind geospatial programming. A few of these pieces will make more sense in the next few weeks, but the general advice on constructing code and planning analyses is useful now.",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#objectives",
    "href": "content/05-content.html#objectives",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDevelop basic docs with Quarto\nUnderstand the basics of creating readable code\nUse pseudocode to sketch out a computational problem",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "5: Literate Programming, Quarto, Workflows"
    ]
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Areal Data: Vectors",
    "section": "",
    "text": "Now that you have started working with the various components of coordinates and coordinate reference systems, it’s time to start learning the fundamental aspects of working with vector data in sf and R. The syntax is a little confusing at first, but once you’ve gotten a sense for the logic behind it you should be able to start piecing together the functions necessary to implement the pseudocode you write for an analysis. We’ll spend more time on vector manipulation in the coming weeks so you’ll get plenty of practice with the ideas we introduce today.",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Areal Data: Vectors",
    "section": "Readings",
    "text": "Readings\nSame as last class really, but hopefully you’ll begin to understand them better…\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#objectives",
    "href": "content/07-content.html#objectives",
    "title": "Areal Data: Vectors",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nImplement approaches for checking and repairing geometries in R\nUnderstand predicates and measures in the context of spatial operations in sf\nUse st_* to evaluate attributes of geometries and calculate measurements",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Areal Data: Vectors",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "7: Areal Data - Vector Data"
    ]
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Introduction to Mapping Geographic Data",
    "section": "",
    "text": "Now that we’re getting into actual operations on spatial data and beginning to actually modify the geometries and attributes of spatial data, it’ll be important for you to be able to visualize the results. At this point, we’ll be focusing on rough visualization as a way of “gut-checking” the outcomes of your code. We’ll focus more on creating informative, aesthetically pleasing, publication quality visualizations in section 4 of this course.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Readings",
    "text": "Readings\n\n Ch.3 Tmap in a nutshell from “Elegant and informative maps with tmap” by Martijn Tennekes and Jakub Nowosad provides a great “quick start” for using the tmap package for visualizing spatial data.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax.\n Making maps in R by Emily Burchfield illustrates some quick mapping syntax with base plot, ggplot, and tmap. For now, just focus on the base plot and tmap sections as we’ll take on the ggplot stuff later in the course.",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#objectives",
    "href": "content/09-content.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "9: Intro to Mapping Geographic Data"
    ]
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Operations With Vector Data II",
    "section": "",
    "text": "Now that you have the complete picture of predicates, measures, and transformers; it’s time to use them on some actual data. This lecture is meant to be the “practical” application of the ideas you’ve learned in our previous discussions of vector data and give you enough tools to begin to subset your data to the records and attributes of interest, calculate new spatial metrics, and generate new geometries based on existing data.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Operations With Vector Data II",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages).",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#objectives",
    "href": "content/11-content.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data.",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Operations With Vector Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "11: Operations with Vector Data II"
    ]
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Operations with Raster Data II",
    "section": "",
    "text": "Now that we’ve done some “global” transformations of raster data using terra, we’ll look at some of the options for cell-wise transformations. Rather than manipulating the extent, resolution, or CRS of the raster data; we’ll actually be using functions to change the values of the cells themselves.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Operations with Raster Data II",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#objectives",
    "href": "content/13-content.html#objectives",
    "title": "Operations with Raster Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset.",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Operations with Raster Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "13: Operations with Raster Data II"
    ]
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Building Databases with Location",
    "section": "",
    "text": "Today we’ll continue our development of attributes (or covariates) in our spatial databases. We’ll look at developing attributes that describe various geographic properties along with joining and subsetting based on locations.",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/15-content.html#resources",
    "href": "content/15-content.html#resources",
    "title": "Building Databases with Location",
    "section": "Resources",
    "text": "Resources\n\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R.\n Attributes and Support of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/15-content.html#objectives",
    "href": "content/15-content.html#objectives",
    "title": "Building Databases with Location",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nGenerate new features using geographic data\nUse topological subsetting to reduce features based on geography\nUse spatial joins to add attributes based on location",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/15-content.html#slides",
    "href": "content/15-content.html#slides",
    "title": "Building Databases with Location",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto recording",
    "crumbs": [
      "Content",
      "Course content",
      "15: Building Databases with Location"
    ]
  },
  {
    "objectID": "content/17-content.html",
    "href": "content/17-content.html",
    "title": "Combining Data and Point Patterns",
    "section": "",
    "text": "Today we’ll finish up our example of combining data for analysis and introduce point process models as a first version of spatial analysis. We’ll need a few new packages here, but many of the key data management processes will remain the same.",
    "crumbs": [
      "Content",
      "Course content",
      "17: Point Patterns"
    ]
  },
  {
    "objectID": "content/17-content.html#resources",
    "href": "content/17-content.html#resources",
    "title": "Combining Data and Point Patterns",
    "section": "Resources",
    "text": "Resources\n\n The Chapters 17 and 18 on Spatial Point Processes and the spatstat package in Paula Moraga’s book Spatial Statistics for Data Science: Theory and Practice with R.\n Rings, circles, and null-models for point pattern analysis in ecology by (Wiegand and A. Moloney 2004) provides an introduction to metrics for spatial clustering with applications in ecology.\n Improving the usability of spatial point process methodology: an interdisciplinary dialogue between statistics and ecology by Janine Illian (a major contributor to modern point pattern analyses) and David Burslem (a Scottish plant ecologist) (Illian and Burslem 2017) is a fairly modern take on the challenges associated with point process modeling in ecology.\n Chapter 11: Point Pattern Analysis in Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provides a nice (and free) introduction to some of these introductory point process methods.",
    "crumbs": [
      "Content",
      "Course content",
      "17: Point Patterns"
    ]
  },
  {
    "objectID": "content/17-content.html#objectives",
    "href": "content/17-content.html#objectives",
    "title": "Combining Data and Point Patterns",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine a point process and their utility for ecological applications\nDefine first and second-order Complete Spatial Randomness\nUse several common functions to explore point patterns",
    "crumbs": [
      "Content",
      "Course content",
      "17: Point Patterns"
    ]
  },
  {
    "objectID": "content/17-content.html#slides",
    "href": "content/17-content.html#slides",
    "title": "Combining Data and Point Patterns",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "17: Point Patterns"
    ]
  },
  {
    "objectID": "content/19-content.html",
    "href": "content/19-content.html",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Last class we started to explore ways to leverage spatial autocorrelation as a means of using interpolation to generate values at unobserved locations. We’ll continue that discussion using variograms and kriging. We then move to a discussion of areal data and the need to identify “neighbors” as a means of understanding how to weight observations when the actual point location of the observation may be unknown or impossible to assign.",
    "crumbs": [
      "Content",
      "Course content",
      "19: Interpolation II"
    ]
  },
  {
    "objectID": "content/19-content.html#resources",
    "href": "content/19-content.html#resources",
    "title": "Proximity and Areal Data",
    "section": "Resources",
    "text": "Resources\n\n Ch. 14: Spatial Interpolation in Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provides more examples of interpolation, trend surfaces, and kriging.\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights.",
    "crumbs": [
      "Content",
      "Course content",
      "19: Interpolation II"
    ]
  },
  {
    "objectID": "content/19-content.html#objectives",
    "href": "content/19-content.html#objectives",
    "title": "Proximity and Areal Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe and implement statistical approaches to interpolation\n\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "19: Interpolation II"
    ]
  },
  {
    "objectID": "content/21-content.html",
    "href": "content/21-content.html",
    "title": "Statistical Modelling I",
    "section": "",
    "text": "Now that we’ve spent some time building dataframes and assessing the spatial correlation (or covariation) for different data, we can move beyond just describing the nature of the data we have or interpolating based on simple predictions. We’ll introduce two fairly simple spatial analysis approaches - overlays and logistic regression - and talk about some of the key assumptions and extensions of these approaches.",
    "crumbs": [
      "Content",
      "Course content",
      "21: Statistical Modelling I"
    ]
  },
  {
    "objectID": "content/21-content.html#resources",
    "href": "content/21-content.html#resources",
    "title": "Statistical Modelling I",
    "section": "Resources",
    "text": "Resources\n\n Overlay analysis provides an overview of the logic of overlay analysis.\n Predicting site location with simple additive raster sensitivity analysis using R from Ben Markwick has a complete example of using a weights of evidence approach to overlays.\n Logistic regression: a brief primer by (Stoltzfus 2011) is a nice introduction to logistic regression.\n Is my species distribution model fit for purpose? Matching data and models to applications by (Guillera-Arroita et al. 2015) is an excellent, concise description of the relations between data collection, statistical models, and inference.\n Predicting species distributions for conservation decisions by (Guisan et al. 2013) is a foundational paper describing some of the challenges with making conservation decisions based on the outcomes of species distribution models.",
    "crumbs": [
      "Content",
      "Course content",
      "21: Statistical Modelling I"
    ]
  },
  {
    "objectID": "content/21-content.html#objectives",
    "href": "content/21-content.html#objectives",
    "title": "Statistical Modelling I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe and implement overlay analyses\nExtend overlay analysis to statistical modeling\nGenerate spatial predictions from statistical models\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "21: Statistical Modelling I"
    ]
  },
  {
    "objectID": "content/23-content.html",
    "href": "content/23-content.html",
    "title": "Statistical Modelling III",
    "section": "",
    "text": "In our last class on multivariate analysis, we’ll take on one of the more underappreciated elements of modeling: understanding if your model is good enough for prediction or inference. We’ll spend a bit of time differentiating the uses of models as a means of understanding what it means to be a “good” model.",
    "crumbs": [
      "Content",
      "Course content",
      "23: Statistical Modelling III"
    ]
  },
  {
    "objectID": "content/23-content.html#resources",
    "href": "content/23-content.html#resources",
    "title": "Statistical Modelling III",
    "section": "Resources",
    "text": "Resources\n\n A practical guide to selecting models for exploration, inference, and prediction in ecology by (Tredennick et al. 2021) highlights the importance of understanding model performance before making inference on predictor effects.\n Model selection using information criteria, but is the “best” model any good? by (Mac Nally et al. 2018) highlights the importance of understanding model performance before making inference on predictor effects.\n Standards for distribution models in biodiversity assessments by (Araújo et al. 2019) highlights the importance of understanding model performance before making inference on predictor effects.",
    "crumbs": [
      "Content",
      "Course content",
      "23: Statistical Modelling III"
    ]
  },
  {
    "objectID": "content/23-content.html#objectives",
    "href": "content/23-content.html#objectives",
    "title": "Statistical Modelling III",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate three different reasons for modeling and how they link to assessments of fit\nDescribe and implement several test statistics for assessing model fit\nDescribe and implement several assessments of classification\nDescribe and implement resampling techniques to estimate predictive performance\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "23: Statistical Modelling III"
    ]
  },
  {
    "objectID": "content/26-content.html",
    "href": "content/26-content.html",
    "title": "Data Visualization and Maps II",
    "section": "",
    "text": "R provides a number of different packages for generating plots of your data, but ggplot2 is probably the most common owing to its ability to use consistent syntax to produce a variety of different graphics. In addition to plots of data and model objects, ggplot2 can also be used with sf objects and some raster datasets to generate publication quality maps. We’ll also take a little more time to understand some of the options for building static maps in R and look at a few packages that can help you build publication-quality maps without having to move into a new software.",
    "crumbs": [
      "Content",
      "Course content",
      "26: Data Visualization and Maps II"
    ]
  },
  {
    "objectID": "content/26-content.html#resources",
    "href": "content/26-content.html#resources",
    "title": "Data Visualization and Maps II",
    "section": "Resources",
    "text": "Resources\n\n The Data Visualization: A Practical Introduction by Healy (2018) provides a lot of examples of Tufte-style graphics built with ggplot2.\n Graphic design with ggplot2 is an entire course devoted to making beautiful visualizations with ggplot2. If nothing else, check out some of the examples!\n The patchwork package website provides a lot of examples of building complicated layouts with ggplot2 objects with intuitive syntax.\n The Drawing Beautiful Maps Programatically with R, sf, and ggplot2 chapter by Mel Moreno and Mathieu Basille provides a nice series of blog posts designed to help you build maps with sf objects.\n Creating beautiful demographic maps in R with the tidycensus and tmap packages by Zev Ross illustrates the simplicity of mapping with tmap.\n Displaying time series, spatial and space-time data with R by Oscar Perpiñán Lamigueiro has a bunch of interesting code for producing maps contained in the book by the same name.",
    "crumbs": [
      "Content",
      "Course content",
      "26: Data Visualization and Maps II"
    ]
  },
  {
    "objectID": "content/26-content.html#objectives",
    "href": "content/26-content.html#objectives",
    "title": "Data Visualization and Maps II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUnderstand the relationship between the Grammar of Graphics and ggplot syntax\nDescribe the various options for customizing ggplots and their syntactic conventions\nGenerate complicated plot layouts without additional pre-processing\nConstruct a map using ggplot2 and tmap\nCombine vector and raster data in the same map\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "26: Data Visualization and Maps II"
    ]
  },
  {
    "objectID": "content/28-content.html",
    "href": "content/28-content.html",
    "title": "Interactive Dashboards",
    "section": "",
    "text": "Now that you’ve had a chance to practice building a few maps and learning some of the core ideas behind the Grammar of Graphics, we can extend those ideas into the development of interactive webmaps and more expansive data visualizations that can be served on the internet and accessed by collaborators and members of the public. Like the previous unit on static maps, this could be a course unto itself, but we should be able to introduce you to enough ideas to get started.",
    "crumbs": [
      "Content",
      "Course content",
      "28: Interactive Dashboards"
    ]
  },
  {
    "objectID": "content/28-content.html#resources",
    "href": "content/28-content.html#resources",
    "title": "Interactive Dashboards",
    "section": "Resources",
    "text": "Resources\n\n The flexdashboard vignette has a simple working example of an interactive dashboard and links to other examples with source code.",
    "crumbs": [
      "Content",
      "Course content",
      "28: Interactive Dashboards"
    ]
  },
  {
    "objectID": "content/28-content.html#objectives",
    "href": "content/28-content.html#objectives",
    "title": "Interactive Dashboards",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nList the necessary elements of an interactive dashboard\nOutline the structure of code needed to build a flexdashboard\nBuild a simple interactive dashboard with spatial data\n\n\n View all slides in new window  Download PDF of all slides",
    "crumbs": [
      "Content",
      "Course content",
      "28: Interactive Dashboards"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the week. Read and watch these before our in-person class.\nLesson (): This page contains additional annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here, but it will be helpful as you work on your assignments.\nExample (): This page the scripts that we work on in class as a reminder of some of the live-coding exercises. These are provided as a reference to help you link your notes to the syntax we use in class.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nAugust 19(Session 1)\n\n\nIntroduction to the course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 21(Session 2)\n\n\nWhy Geographic Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 23\n\n\n Self-Evaluation 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 26(Session 3)\n\n\nIntroduction to Spatial Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 28(Session 4)\n\n\nIntroduction to Spatial Data with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 29\n\n\n Homework 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 2\n\n\nNo Class(Labor Day)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 4(Session 5)\n\n\nLiterate Programming, Quarto, Workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 5\n\n\n Homework 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Operations in R\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nSeptember 9(Session 6)\n\n\nAreal Data: Coordinates and Geometries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 11(Session 7)\n\n\nAreal Data: Vector Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 12\n\n\n Homework 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 16(Session 8)\n\n\nAreal Data: Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18(Session 9)\n\n\nMapping Geographic Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 19\n\n\n Homework 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 23(Session 10)\n\n\nOperations With Vectors I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 25(Session 11)\n\n\nOperations With Vectors II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 26\n\n\n Assignment Revision 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 30(Session 12)\n\n\nOperations With Rasters I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2(Session 13)\n\n\nOperations With Rasters II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 3\n\n\n Homework 6  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 7(Session 14)\n\n\nBuilding Spatial Databases with Attributes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 9(Session 15)\n\n\nBuilding Spatial Databases with Location\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 14(Session 16)\n\n\nCombining Vectors and Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 15\n\n\n Homework 7  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Workflows for Spatial Data\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nOctober 16(Session 17)\n\n\nPoint Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 21(Session 18)\n\n\nInterpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 23(Session 19)\n\n\nProximity and Areal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 24\n\n\n Assignment Revision 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 28(Session 20)\n\n\nSpatial Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 30(Session 21)\n\n\nStatistical Modelling I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 31\n\n\n Homework 8  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 4(Session 22)\n\n\nStatistical Modelling II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 6(Session 23)\n\n\nStatistical Modelling III\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 7\n\n\n Homework 9  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 11(Session 24)\n\n\nStatistical Modelling Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 13(Session 25)\n\n\nData Visualization and Maps I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 14\n\n\n Assignment Revision 3  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Spatial Data\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nNovember 18(Session 26)\n\n\nData Visualization and Maps II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 20(Session 27)\n\n\nIntroduction to Interactive Maps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 21\n\n\n Homework 10  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 25\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 27\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 2(Session 28)\n\n\nInteractive Dashboards\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrapup\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nDecember 4(Session 29)\n\n\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 5\n\n\n Final Project Draft  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 11\n\n\nFinal Project Workday(optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 12\n\n\n Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 13\n\n\n Final Self-Evaluation Due  (submit by 11:59 PM)"
  },
  {
    "objectID": "slides/05-slides.html#for-today",
    "href": "slides/05-slides.html#for-today",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "For today",
    "text": "For today\n\nIntroduce literate programming\nDescribe pseudocode and its utility for designing an analysis\nIntroduce Quarto as a means of documenting your work\nPractice workflow"
  },
  {
    "objectID": "slides/05-slides.html#why-do-we-need-reproducibility",
    "href": "slides/05-slides.html#why-do-we-need-reproducibility",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why Do We Need Reproducibility?",
    "text": "Why Do We Need Reproducibility?\n\n\n\nNoise!!\nConfirmation bias\nHindsight bias\n\n\n\n\n\nMunafo et al. 2017. Nat Hum Beh."
  },
  {
    "objectID": "slides/05-slides.html#reproducibility-and-your-code",
    "href": "slides/05-slides.html#reproducibility-and-your-code",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducibility and your code",
    "text": "Reproducibility and your code\n\nScripts: may make your code reproducible (but not your analysis)\nCommenting and formatting can help!\n\n\n```{r}\n#| eval: false\n#|\n## load the packages necessary\nlibrary(tidyverse)\n## read in the data\nlandmarks.csv &lt;- read_csv(\"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2023/assignment01/landmarks_ID.csv\")\n\n## How many in each feature class\ntable(landmarks.csv$MTFCC)\n```"
  },
  {
    "objectID": "slides/05-slides.html#reproducible-scripts",
    "href": "slides/05-slides.html#reproducible-scripts",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducible scripts",
    "text": "Reproducible scripts\n\nComments explain what the code is doing\nOperations are ordered logically\nOnly relevant commands are presented\nUseful object and function names\nScript runs without errors (on your machine and someone else’s)"
  },
  {
    "objectID": "slides/05-slides.html#toward-efficient-reproducible-analyses",
    "href": "slides/05-slides.html#toward-efficient-reproducible-analyses",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Toward Efficient Reproducible Analyses",
    "text": "Toward Efficient Reproducible Analyses\n\nScripts can document what you did, but not why you did it!\nScripts separate your analysis products from your report/manuscript"
  },
  {
    "objectID": "slides/05-slides.html#what-is-literate-programming",
    "href": "slides/05-slides.html#what-is-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\n\n— Donald Knuth, CSLI, 1984"
  },
  {
    "objectID": "slides/05-slides.html#what-is-literate-programming-1",
    "href": "slides/05-slides.html#what-is-literate-programming-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!\nMultiple “scales” possible"
  },
  {
    "objectID": "slides/05-slides.html#why-literate-programming",
    "href": "slides/05-slides.html#why-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why literate programming?",
    "text": "Why literate programming?\n\nYour analysis scripts are computer software\nIntegrate math, figures, code, and narrative in one place\nExplaining something helps you learn it"
  },
  {
    "objectID": "slides/05-slides.html#planning-an-analysis",
    "href": "slides/05-slides.html#planning-an-analysis",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Planning an analysis",
    "text": "Planning an analysis\n\n\n\nOutline your project\nWrite pseudocode\nIdentify potential packages\nBorrow (and attribute) code from others (including yourself!)"
  },
  {
    "objectID": "slides/05-slides.html#pseudocode-and-literate-programming",
    "href": "slides/05-slides.html#pseudocode-and-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode and literate programming",
    "text": "Pseudocode and literate programming\n\nAn informal way of writing the ‘logic’ of your program\nBalance between readability and precision\nAvoid syntactic drift"
  },
  {
    "objectID": "slides/05-slides.html#writing-pseudocode",
    "href": "slides/05-slides.html#writing-pseudocode",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Writing pseudocode",
    "text": "Writing pseudocode\n\n\n\nFocus on statements\nMathematical operations\nConditionals\nIteration\nExceptions"
  },
  {
    "objectID": "slides/05-slides.html#pseudocode-1",
    "href": "slides/05-slides.html#pseudocode-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nStart function\nInput information\nLogical test: if TRUE\n  (what to do if TRUE)\nelse\n  (what to do if FALSE)\nEnd function"
  },
  {
    "objectID": "slides/05-slides.html#what-is-quarto",
    "href": "slides/05-slides.html#what-is-quarto",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nA multi-language platform for developing reproducible documents\nA ‘lab notebook’ for your analyses\nAllows transparent, reproducible scientific reports and presentations"
  },
  {
    "objectID": "slides/05-slides.html#key-components",
    "href": "slides/05-slides.html#key-components",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Key components",
    "text": "Key components\n\nMetadata and global options: YAML\nText, figures, and tables: Markdown and LaTeX\nCode: knitr (or jupyter if you’re into that sort of thing)"
  },
  {
    "objectID": "slides/05-slides.html#yaml---yet-another-markup-language",
    "href": "slides/05-slides.html#yaml---yet-another-markup-language",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "YAML - Yet Another Markup Language",
    "text": "YAML - Yet Another Markup Language\n\nAllows you to set (or change) output format\nProvide options that apply to the entire document\nSpacing matters!"
  },
  {
    "objectID": "slides/05-slides.html#formatting-text",
    "href": "slides/05-slides.html#formatting-text",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Formatting Text",
    "text": "Formatting Text\n\nBasic formatting via Markdown\nFancier options using Divs and spans via Pandoc\nFenced Divs start and end with ::: (can be any number &gt;3 but must match)"
  },
  {
    "objectID": "slides/05-slides.html#adding-code-chunks",
    "href": "slides/05-slides.html#adding-code-chunks",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Adding Code Chunks",
    "text": "Adding Code Chunks\n\nUse 3x ``` on each end\nInclude the engine {r} (or python or Julia)\nInclude options beneath the “fence” using a hashpipe (#|)"
  },
  {
    "objectID": "slides/05-slides.html#additional-considerations",
    "href": "slides/05-slides.html#additional-considerations",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Additional considerations",
    "text": "Additional considerations\n\nFile locations and Quarto\nCaching for slow operations\nModularizing code and functional programming"
  },
  {
    "objectID": "lesson/dataclasses.html",
    "href": "lesson/dataclasses.html",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Data Classes"
    ]
  },
  {
    "objectID": "lesson/dataclasses.html#data-types-and-structures",
    "href": "lesson/dataclasses.html#data-types-and-structures",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Data Classes"
    ]
  },
  {
    "objectID": "example/session-5-example.html",
    "href": "example/session-5-example.html",
    "title": "Session 5 Live Code",
    "section": "",
    "text": "This is the html output of the Quarto options we tested in class today.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#code-chunk-output-options",
    "href": "example/session-5-example.html#code-chunk-output-options",
    "title": "Session 5 Live Code",
    "section": "Code chunk output options",
    "text": "Code chunk output options\nCommon code chunk options (all true by default):\n\ninclude\neval\necho\nwarning\nmessage\n\n\n\n\nThis table shows what occurs when these options are set to false.\n\n\nWe used the following code to test code chunk options to generate this table. We also added code chunk labels.\n\n\nCode\n```{r}\n#| output: false\n#| label: \"read libraries\"\n\nlibrary(tidyverse)\nlibrary(sf)\n```\n\n\nI’m getting data from this source. In literate programming, we would add more detail!\n\n\nCode\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\n\n\nCode\n```{r}\n#| warning: false\n\nggplot(data = cejst, aes(x = AGE_10, y = AGE_MIDDLE)) +\n  geom_point()\n```\n\n\n\n\n\n\n\n\n\nYou can find more information on code chunk options here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#inline-code",
    "href": "example/session-5-example.html#inline-code",
    "title": "Session 5 Live Code",
    "section": "Inline code",
    "text": "Inline code\nWe implemented inline code by with the formula: `, r, a single space, some code, `.\nThe mean proportion of children 10 and under in the Northwest is 0.1140576.\nYou can find more information on inline code here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#heading-styles",
    "href": "example/session-5-example.html#heading-styles",
    "title": "Session 5 Live Code",
    "section": "Heading Styles",
    "text": "Heading Styles\n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n##### Heading 5\n###### Heading 6\nrenders as:",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#heading-2",
    "href": "example/session-5-example.html#heading-2",
    "title": "Session 5 Live Code",
    "section": "Heading 2",
    "text": "Heading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#text-formatting",
    "href": "example/session-5-example.html#text-formatting",
    "title": "Session 5 Live Code",
    "section": "Text formatting",
    "text": "Text formatting\n**bold** renders as bold\n_italics_ renders as italics",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#lists",
    "href": "example/session-5-example.html#lists",
    "title": "Session 5 Live Code",
    "section": "Lists",
    "text": "Lists\n\nBullet List\n- bullet 1\n- bullet 2\n- another sub-bullet\n- bullet 3\nrenders as:\n\nbullet 1\n\nbullet 2\n\nanother sub-bullet\n\n\nbullet 3\n\n\n\nNumbered List\n1. number 1\n2. number 2\nrenders as:\n\nnumber 1\nnumber 2",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "example/session-5-example.html#commenting",
    "href": "example/session-5-example.html#commenting",
    "title": "Session 5 Live Code",
    "section": "Commenting",
    "text": "Commenting\nTo comment out text outside code chunks, use &lt;!-- at the beginning of a comment and --&gt; at the end.\nMore information about text formatting in Quarto can be found here.",
    "crumbs": [
      "Examples",
      "Getting started",
      "Quarto formatting"
    ]
  },
  {
    "objectID": "assignment/01-introsolutions.html",
    "href": "assignment/01-introsolutions.html",
    "title": "Assignment 1 Solutions: Introductory material",
    "section": "",
    "text": "How does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\n\nMy research is based almost entirely on geographic analysis, between estimating ecosystem services and studying agricultural change through time in different Idaho locations. One issue I face is that I integrate ecosystem services data collected and estimated at multiple scales into one analysis. I have to be careful I don’t fall into any pitfalls or fallacies when I combine multiple sources of data. In other cases, I have county level data and have to acknowledge the measurement bias that comes with data collection along those policial boundaries.\n\nWhat are the primary components that describe spatial data?\n\nI would say that the primary components are the coordinate reference system (because it helps us understand where we actually are on Earth), the extent of the data (because that helps me know what scale we’re working with and the size of the computational problem), the resolution (same reason as extent), the geometry, and spatial support. I don’t think about this last one often enough, but it really is the key to honest interpretation of the spatial data that you have.\n\nWhat is the coordinate reference system and why is it important\n\nThe CRS consists of the information necessary to locate points in 2 or 3 dimensional space. Coordinates are only meaningful in the context of a CRS (i.e., (2,2) could describe any number of places in the world - we need to know the origin and the datum to actually know where that is). The CRS becomes particularly important when we need to align datasets that were not collected in the same CRS originally or when we need to transfer locations from the globe to a flat surface (e.g., map, screen, etc).\n\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\n\nHere’s a fun article on projections that shows what I’m talking about!\n\nRead in the cejst_nw.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\nI can read in the data using st_read or read_sf\n\nlibrary(sf)\n\ncejst.sf &lt;- read_sf(\"/opt/data/data/assignment01/cejst_nw.shp\")\ncejst.st &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\nYou can inspect the differences between the resulting object classes by calling class\n\n```{r}\n#| message: false\n\nclass(cejst.sf)\nclass(cejst.st)\n```\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n[1] \"sf\"         \"data.frame\"\n\n\nYou’ll notice that using st_read assigns the object to an sf and data.frame class meaning that functions defined for those two classes will work. Alternatively, read_sf assigns the object to sf, tbl_df, tbl, and data.frame classes meaning that a much broader set of functions can be run on the cejst.sf object.\nBecause the data are in wide format, we can assume that there is only 1 observation for each location (because sf requires that there is a geometry entry for every observation (even if it’s empty)). Probably the easiest way to get the number of observations is:\n\n```{r}\nnrow(cejst.sf)\n```\n\n[1] 2590\n\n\nSimilarly, if we wanted to know how many attributes are collected for each observation we could use ncol:\n\n```{r}\nncol(cejst.sf)\n```\n\n[1] 124\n\n\nNote that these are really only approximate estimates. There’s usually a lot of extra ID-style columns in spatial data such that the number of columns with useful information is less than the total number of columns, but we won’t worry about that for now."
  },
  {
    "objectID": "assignment/02-example.html",
    "href": "assignment/02-example.html",
    "title": "Quarto Example",
    "section": "",
    "text": "Introduction\nI want to reproduce the figure from (Dash Nelson 2016) depicting commute networks in the United States based on US Census data.\n\n\nMethods\nSome pseudocode:\n\n1. Retrieve ACS commute data\n2. Identify the source and destination networks\n3. Calculate the edge density for each source-destination pair\n4. Thin to a manageable number of nodes based on edge densities\n5. plot\n\nas code chunks\n\n```{r}\n#| eval: false\n#| label: getacs\n\n1. Retrieve ACS commute data\n```\n\n\n```{r}\n#| eval: false\n#| label: buildnet\n\n2. Identify the source and destination networks\n```\n\n\n```{r}\n#| eval: false\n#| label: edgedens\n\n3. Calculate the edge density for each source-destination pair\n```\n\n\n```{r}\n#| eval: false\n#| label: thinnodes\n\n4. Thin to a manageable number of nodes based on edge densities\n```\n\n\n```{r}\n#| eval: false\n#| label: buildplot\n\n5. Build plot\n```\n\n\n\nResults\n\n\n\nCommute Networks from Dash Nelson and Rae 2016\n\n\n\n\n\n\n\nReferences\n\nDash Nelson, A., Garrett AND Rae. 2016. An economic geography of the united states: From commutes to megaregions. PLOS ONE 11:1–23."
  },
  {
    "objectID": "example/session-15-example.html",
    "href": "example/session-15-example.html",
    "title": "Session 15 Code",
    "section": "",
    "text": "Please install and load these packages:\nCode\n# install.packages('rgbif')\n\nlibrary(rgbif)\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(spData)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Location"
    ]
  },
  {
    "objectID": "example/session-15-example.html#create-new-spatial-attributes",
    "href": "example/session-15-example.html#create-new-spatial-attributes",
    "title": "Session 15 Code",
    "section": "Create new spatial attributes",
    "text": "Create new spatial attributes\n\n\nCode\nnz.sf &lt;- nz %&gt;% \n  mutate(area = st_area(nz))\nhead(nz.sf$area, 3)\n\n\nUnits: [m^2]\n[1] 12890576439  4911565037 24588819863\n\n\n\n\nCode\n# generate random points\nrandom_long_lat &lt;- \n  data.frame(\n    long = sample(runif(2000, min = 1090144, max = 2089533), replace = F),\n    lat = sample(runif(2000, min = 4748537, max = 6191874), replace = F)\n  )\n\nrandom_long_lat_sf &lt;- random_long_lat %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(nz))\n\nrandom_nz &lt;- random_long_lat_sf[nz.sf,]\n\n# code from slides\nnz.df &lt;- nz %&gt;% \nmutate(counts = lengths(st_intersects(., random_nz)),\n       area = st_area(nz),\n       density = counts/area)\nhead(st_drop_geometry(nz.df[,7:10]))\n\n\n  counts              area              density\n1     20 12890576439 [m^2] 1.551521e-09 [1/m^2]\n2      4  4911565037 [m^2] 8.144044e-10 [1/m^2]\n3     40 24588819863 [m^2] 1.626756e-09 [1/m^2]\n4     21 12271015945 [m^2] 1.711350e-09 [1/m^2]\n5      9  8364554416 [m^2] 1.075969e-09 [1/m^2]\n6     18 14242517871 [m^2] 1.263821e-09 [1/m^2]\n\n\n\n\nCode\ncanterbury = nz %&gt;% filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\nco = filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n\n\nUnits: [m]\n          [,1]     [,2]\n[1,] 123537.16 15497.72\n[2,]  94282.77     0.00\n[3,]  93018.56     0.00\n\n\n\n\nCode\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nCode\nua &lt;- urban_areas(cb = FALSE, progress_bar = FALSE) %&gt;% \n  filter(., UATYP10 == \"U\") %&gt;% \n  filter(., str_detect(NAME10, \"ID\")) %&gt;% \n  st_transform(., crs=2163)\n\n\nRetrieving data for the year 2022\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: CRS EPSG:2163 is deprecated.\nIts non-deprecated replacement EPSG:9311 will be used instead. To use the\noriginal CRS, set the OSR_USE_NON_DEPRECATED configuration option to NO.\n\n\nCode\n#get index of nearest ID city\nnearest &lt;-  st_nearest_feature(ua)\n#estimate distance\n(dist = st_distance(ua, ua[nearest,], by_element=TRUE))\n\n\nUnits: [m]\n[1]  61373.575  61373.575   1647.128   1647.128 136917.546 136917.546\n\n\nCode\nua$dist_to_neighbor &lt;- dist\n\n\n\nTopological Subsetting\n\n\nCode\nctby_height &lt;-  nz_height[canterbury, ]\n\nctby_height_diffpred &lt;- nz_height[canterbury, ,op=st_touches]\n\n\n\n\nCode\ncanterbury_height3 = nz_height %&gt;%\n  filter(st_intersects(x = ., y = canterbury, sparse = FALSE))\n\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\n\n\nCode\n# to fix warning\ncanterbury_height3 = nz_height %&gt;%\n  filter(as.vector(st_intersects(x = ., y = canterbury, sparse = FALSE)))\n\n\n\n\nSpatial Joins\n\n\nCode\nset.seed(2018)\n(bb = st_bbox(world)) # the world's bounds\n\n\n      xmin       ymin       xmax       ymax \n-180.00000  -89.90000  179.99999   83.64513 \n\n\nCode\n#&gt;   xmin   ymin   xmax   ymax \n#&gt; -180.0  -89.9  180.0   83.6\n\nrandom_df = data.frame(\n  x = runif(n = 10, min = bb[1], max = bb[3]),\n  y = runif(n = 10, min = bb[2], max = bb[4])\n)\nrandom_points &lt;- random_df %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\")) %&gt;% # set coordinates\n  st_set_crs(\"EPSG:4326\") # set geographic CRS\n\nrandom_joined = st_join(random_points, world[\"name_long\"])\n\n\n\n\nCode\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n\n\n[1] FALSE\n\n\n\n\nCode\nz = st_join(cycle_hire, cycle_hire_osm, st_is_within_distance, dist = 20)\nnrow(cycle_hire)\n\n\n[1] 742\n\n\n\n\nCode\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\")) %&gt;%\n  st_transform(crs=4326)\n\n\nReading layer `nc' from data source \n  `C:\\Users\\carolynkoehn\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.3\\x86_64-w64-mingw32\\sf\\1.0-16\\ad57b543f7c3fca05213ba78ff63df9b\\sf\\shape\\nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nCode\nbb &lt;- st_bbox(nc)\nnc_points &lt;- data.frame(\n  x = runif(n = 2, min = bb[1], max = bb[3]),\n  y = runif(n = 2, min = bb[2], max = bb[4])\n) %&gt;%\n  st_as_sf(., coords = c(\"x\", \"y\"), crs=4326)\ntr_buff &lt;- st_buffer(nc_points, units::set_units(100, \"km\")) %&gt;%\n  st_transform(crs=4326)\n\nintersect_pct &lt;- st_intersection(nc, tr_buff) %&gt;% \n   mutate(intersect_area = st_area(.)) %&gt;%   # create new column with shape area\n   dplyr::select(NAME, intersect_area) %&gt;%   # only select columns needed to merge\n   st_drop_geometry()\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nCode\nnc &lt;- mutate(nc, county_area = st_area(nc))\n\n# Merge by county name\nnc &lt;- left_join(nc, intersect_pct)\n\n\nJoining with `by = join_by(NAME)`\n\n\nCode\n# Calculate coverage\nnc &lt;- nc %&gt;% \n   mutate(coverage = as.numeric(intersect_area/county_area)) %&gt;%\n  mutate(coverage = ifelse(test = is.na(coverage),\n                           yes = 0,\n                           no = coverage))\n\nplot(nc[\"coverage\"])\n\n\n\n\n\n\n\n\n\n\n\nPractice\nWhat is the relationship between observations of golden eagles and distance to a main road?\n\n\nCode\nidaho &lt;- tigris::states(progress_bar = FALSE) %&gt;%\n  filter(NAME == \"Idaho\")\n\n\nRetrieving data for the year 2021\n\n\nCode\n# get a selection of 1000 observations from rgbif\ngold_eagles_us &lt;- occ_search(scientificName = \"Aquila chrysaetos\", \n                    country = \"US\",\n                    hasCoordinate = TRUE,\n                    limit=1000)\n\n# This step has to be separate, it can't be piped into the next bit of code\ngold_eagles_us &lt;- gold_eagles_us$data\n\n# convert to spatial object\ngold_eagle_dat_sf &lt;- gold_eagles_us %&gt;%\n  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude)) %&gt;%\n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\n\n# get roads\nroads &lt;- tigris::primary_secondary_roads(\"ID\", progress_bar = FALSE) %&gt;%\n  st_transform(crs=4326)\n\n\nRetrieving data for the year 2022\n\n\nCode\n# check plot\nplot(st_geometry(idaho))\nplot(st_geometry(roads), col=\"red\", add=TRUE)\nplot(st_geometry(gold_eagle_dat_sf), add=TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# spatial subset to points in Idaho\nidaho &lt;- st_transform(idaho, crs=4326)\n\ngold_eagle_dat_sf_id &lt;- gold_eagle_dat_sf[idaho, ]\n\n# check plot\nplot(st_geometry(idaho))\nplot(st_geometry(roads), col=\"red\", add=TRUE)\nplot(st_geometry(gold_eagle_dat_sf_id), add=TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# get nearest road for each point\nnearest_road &lt;- st_nearest_feature(gold_eagle_dat_sf_id, roads)\n\n# get distance to nearest road for each point\ngold_eagle_dat_sf_id &lt;- gold_eagle_dat_sf_id %&gt;%\n  mutate(dist_to_road = st_distance(., roads[nearest_road,], by_element = TRUE))\n\n# now we can investigate\n# for example...\nhist(gold_eagle_dat_sf_id$dist_to_road)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Combining Databases with Location"
    ]
  },
  {
    "objectID": "assignment/11-statmodsolutions.html",
    "href": "assignment/11-statmodsolutions.html",
    "title": "Assignment 9 Solutions: Fitting models to your dataframe",
    "section": "",
    "text": "1. Use the variables that you chose from assignment 7 along with the wildfire hazard and land use dataset to attribute each disaster in the disaster dataset.\n\nHere I am following the same procedure from assignment 7 for creating the spatial database. The only real change is that we are attributing point data (in the incidents dataset) instead of summarizing to polygons (like we did with the Forest Service data). We drop any incomplete cases to avoid problems with NAs (though this may not be the best thing to do in practice) and store that data for later. We then set up our model dataframe by making sure that the cost variable is an integer (for Poisson modeling) and that we drop levels from the land use dataset that don’t appear in our incident locations.\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(terra)\nlibrary(tmap, quietly = TRUE)\nlibrary(caret)\n\n \ncejst.pnw &lt;- read_sf(\"/opt/data/data/assignment07/cejst_pnw.shp\")%&gt;% \n  filter(., !st_is_empty(.))\n\nincidents.csv &lt;- read_csv(\"/opt/data/data/assignment07/ics209-plus-wf_incidents_1999to2020.csv\")\n\nland.use &lt;- rast(\"/opt/data/data/assignment07/land_use_pnw.tif\")\nfire.haz &lt;- rast(\"/opt/data/data/assignment07/wildfire_hazard_agg.tif\")\n\n\nfire.haz.proj &lt;- project(fire.haz, land.use)\n\n\ncejst.proj &lt;- cejst.pnw %&gt;% \n  st_transform(., crs=crs(land.use))\n\nincidents.proj &lt;- incidents.csv %&gt;% \n  filter(., !is.na(POO_LONGITUDE) | !is.na(POO_LATITUDE) ) %&gt;% \n  st_as_sf(., coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4269) %&gt;% \n  st_transform(., crs=crs(land.use))\nincidents.pnw &lt;- st_crop(incidents.proj, st_bbox(cejst.proj))\n\nhazard.smooth &lt;- focal(fire.haz.proj, w=5, fun=\"mean\")\nland.use.smooth &lt;- focal(land.use, w=5, fun=\"modal\")\nlevels(land.use.smooth) &lt;- levels(land.use)\n\ncejst.select &lt;- cejst.proj %&gt;% \n  select(., c(TPF, HBF_PFS, P200_I_PFS))\n\nincident.cejst &lt;- incidents.pnw %&gt;% \n  st_join(., y=cejst.select, join=st_within) \n\nincident.landuse.ext &lt;- terra::extract(x=land.use.smooth, y = vect(incident.cejst), fun=\"modal\", na.rm=TRUE)\n\nincident.firehaz.ext &lt;- terra::extract(x= hazard.smooth, y = vect(incident.cejst), fun=\"mean\", na.rm=TRUE)\n\nincident.cejst.join &lt;- cbind(incident.cejst,incident.landuse.ext$category, incident.firehaz.ext$focal_mean) %&gt;% \n  rename(category = \"incident.landuse.ext.category\", hazard = \"incident.firehaz.ext.focal_mean\")\n\nincident.cejst.prep &lt;- incident.cejst.join %&gt;% \n  select(., PROJECTED_FINAL_IM_COST, TPF, HBF_PFS, P200_I_PFS, hazard, category,) %&gt;% \n  st_drop_geometry(.) %&gt;% \n  filter(., complete.cases(.))\n\nincident.cejst.model &lt;- incident.cejst.prep  %&gt;% \n  mutate(across(TPF:hazard, ~ as.numeric(scale(.x))),\n         category=droplevels(category),\n         cost = as.integer(floor(PROJECTED_FINAL_IM_COST))) %&gt;% \n  select(-PROJECTED_FINAL_IM_COST)\n\n2. Fit a Poisson regression using your covariates and the cost of the incident data (using glm with family=poisson())\n\nNow that we have our data, it’s time to set up some models. We take advantage of the caret package to split the data into a training and testing set using the category variable to make sure we have representation of all the cateogries. We then set up our trainControl options to use cross validation as a means of adjusting tuning parameters and tell R to only save the best model once the tuning is complete. Finally, we use the train function from caret to fit our first model. For a simple Poisson regression, we can rely on the glm method with the family set to poisson. Note that because this is not binary data, our ROC metric doesn’t work as a means of evaluating the performance of the different tuning parameters. Instead, we use something called the Root-Mean Squared Error (RMSE).The RMSE is a measure of the difference between the fitted value and the observed value (in this case, for the cross-validation data within the model training). Larger values indicate poorer fits.\n\n\nset.seed(998)\ninTraining &lt;- createDataPartition(incident.cejst.model$category, p = .8, list = FALSE)\ntraining &lt;- incident.cejst.model[ inTraining,]\ntesting  &lt;- incident.cejst.model[-inTraining,]\n\nfitControl &lt;- trainControl(\n   method = \"cv\",  # k-fold cross validation\n   number = 10,  # 10 folds\n   savePredictions = \"final\"       # save predictions for the optimal tuning parameter\n)\n\nPoisFit &lt;- train( cost ~ ., data = training, \n                 method = \"glm\", \n                 family = poisson,\n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n\n3. Fit a regression tree using your covariates and the cost of the incident data (using caret package method='rpart')\n\nWe use similar syntax to fit the regression tree to the data, but make a few changes. First, we set cost as.numeric() to ensure that this is a regression tree (because our data do not reflect categories). We then set the method to rpart. Because rpart has a complexity parameter, there is a bit of tuning to be done. We tell R that we’re willing to look at 20 different values of this complexity parameter. We can use plot and rpart.plot to inspect the results.\n\n\nRtFit &lt;- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nplot(RtFit)\n\n\n\n\n\n\n\nrpart.plot::rpart.plot(RtFit$finalModel, type=4)\n\n\n\n\n\n\n\n\n4. Fit a random forest model using your covariates and the cost of the incident data (using caret package method= 'rf')\n\nThe syntax is similar to the previous models, but with method='rf' to signal that we want to use the rf package to fit the Random Forest. Here, the tuning parameter is the number of variables to include in the tree. We’ve only got 7 variables so we’ll set the tuneLength to the maximum number of variables.\n\n\nRFFit &lt;- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rf\",\n                 trControl = fitControl, \n               tuneLength=7\n                 )\nplot(RFFit)\n\n\n\n\n\n\n\nplot(RFFit$finalModel)\n\n\n\n\n\n\n\nrandomForest::varImpPlot(RFFit$finalModel)\n\n\n\n\n\n\n\n\n5. Use cross-validation to identify the best performing model of the 3 that you fit\n\nNow that we have three different models, let’s see how well they do predicting the testing dataset. We first generate predictions using the predict function and supplying the model object and the newdata. In this case, our new data is the five covariate columns from the testing partition. Once we have the predictions, we can calculate the RMSE. Based on RMSE values the regression tree and Random Forest model seem to be the better performers.\n\n\npois.pred &lt;- predict(PoisFit, newdata = testing[,1:5])\nrmse.pois &lt;- sqrt(sum(pois.pred - testing$cost)^2/length(pois.pred))\nrt.pred &lt;- predict(RtFit, newdata = testing[,1:5])\nrmse.rt &lt;- sqrt(sum(rt.pred - testing$cost)^2/length(rt.pred))\nrf.pred &lt;- predict(RFFit, newdata = testing[,1:5])\nrmse.rf &lt;- sqrt(sum(rf.pred - testing$cost)^2/length(rf.pred))\n\nprint(c(RMSE.Pois = rmse.pois,\n        RMSE.Tree = rmse.rt,\n        RMSE.RandomForest = rmse.rf))\n\n        RMSE.Pois         RMSE.Tree RMSE.RandomForest \n          1713732           6329192           2831212 \n\n\n6. Convert all of your predictors into rasters of the same resolution and generate a spatial prediction based on your model\n\nNow that we’ve identified the models we want to use to generate our spatial surface, we need to prepare all of the input rasters. We use rasterize to create the cejst variables. These are on original scale of the data and so we need to rescale them to the same range of our modeled datasets. Here, we can’t use scale because the mean of the total dataset would differ from the mean that we used for the incidents-only data so we have to manually set up the scale. Lastly, we have to drop the levels from the land use raster that weren’t present in the incident dataset. We do that with the subst call from terra. Once we’ve got our rasters set up, we can just use predict.\n\n\nTPF.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"TPF\") - mean(incident.cejst.prep$TPF,na.rm=TRUE))/sd(incident.cejst.prep$TPF)\n\nHBF_PFS.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"HBF_PFS\")- mean(incident.cejst.prep$HBF_PFS,na.rm=TRUE))/sd(incident.cejst.prep$HBF_PFS)\n\nP200_I_PFS.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"P200_I_PFS\")- mean(incident.cejst.prep$P200_I_PFS,na.rm=TRUE))/sd(incident.cejst.prep$P200_I_PFS)\n\nland.use.smooth &lt;- subst(land.use.smooth, from=c(\"Non-Forest Wetland\",\"Non-Processing Area Mask\"), to=c(NA, NA))\n\nhazard.smooth.scl &lt;- (hazard.smooth - mean(incident.cejst.prep$hazard))/sd(incident.cejst.prep$hazard)\n\npred.rast &lt;- c(TPF.rast, HBF_PFS.rast, P200_I_PFS.rast, land.use.smooth, hazard.smooth.scl)\nnames(pred.rast)[5] &lt;- \"hazard\"\n\n\nrf.spatial &lt;- terra :: predict(pred.rast, RFFit, na.rm=TRUE) \n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\npois.spatial &lt;- terra :: predict(pred.rast, PoisFit, na.rm=TRUE)\n\n\nIf you looked at the RMSE values for our initial models, you’ll notice that they were quite high and the models weren’t particularly interesting. Because the cost data ranges over several orders of magnitude, we might try log-transforming them and fitting a linear model (because the data are no longer integers) along with the other two models. We do that here following the syntax above. When calculating the RMSE, we have to remember to log-transform the cost variable in the testing dataset to make sure that the predictions are comparable. Again, the regression tree and Random Forest are the better performers, but the RMSE suggests that we are doing considerably better (\\(10^{2}=100\\) as opposed to the 100,000s we were getting before).\n\n\ntraining.log &lt;- training %&gt;% \n  mutate(cost = log(cost, 10))\n\nLinFit &lt;- train( cost ~ ., data = training.log, \n                 method = \"lm\", \n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n\nRtFit.log &lt;- train(cost ~ ., data = training.log, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nRFFit.log &lt;- train(cost ~ ., data = training.log, \n                 method = \"rf\",\n                 trControl = fitControl\n                 )\n\nlin.pred &lt;- predict(LinFit, newdata = testing[,1:5])\nrmse.lin &lt;- sqrt(sum(lin.pred - log(testing$cost,10))^2/length(pois.pred))\nrt.pred &lt;- predict(RtFit.log, newdata = testing[,1:5])\nrmse.rt &lt;- sqrt(sum(rt.pred - log(testing$cost,10))^2/length(rt.pred))\nrf.pred &lt;- predict(RFFit.log, newdata = testing[,1:5])\nrmse.rf &lt;- sqrt(sum(rf.pred - log(testing$cost,10))^2/length(rf.pred))\n\nprint(c(RMSE.Lin.Log = rmse.lin,\n        RMSE.Tree.Log = rmse.rt,\n        RMSE.RandomForest.Log = rmse.rf))\n\n         RMSE.Lin.Log         RMSE.Tree.Log RMSE.RandomForest.Log \n            0.5682741             0.3206917             0.2976975 \n\n\n7. Plot your result &gt;We use the par argument to set up a 2x2 layout and print all 4 plots.\n\nrt.spatial.log &lt;- terra :: predict(pred.rast, RtFit.log, na.rm=TRUE) \n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\nrf.spatial.log &lt;- terra :: predict(pred.rast, RFFit.log, na.rm=TRUE) \n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# un-log transform for comparability\nrt.spatial.log2 &lt;- 10^rt.spatial.log\nrf.spatial.log2 &lt;- 10^rf.spatial.log\n\npar(mfrow=c(2,2))\nplot(pois.spatial, main=\"Poisson Classifier\")\nplot(rf.spatial, main=\"Random Forest Classifier\")\nplot(rt.spatial.log2, main=\"Regression Tree Classifier (log)\")\nplot(rf.spatial.log2, main=\"Random Forest Classifier(log)\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "example/session-08-example.html",
    "href": "example/session-08-example.html",
    "title": "Session 8 Code",
    "section": "",
    "text": "Playing with resolution:\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nr &lt;- rast(xmin=-4, xmax=9,\n          ncols=10)\nres(r)\n\n\n[1] 1.3 1.0\n\n\n\n\nCode\nr2 &lt;- rast(xmin=-4, xmax=9,\n           resolution = c(1.3, 1))\nncol(r2)\n\n\n[1] 10\n\n\n\n\nCode\nr3 &lt;- rast(xmin=-4, xmax=5,\n           ncols=10)\nres(r3)\n\n\n[1] 0.9 1.0\n\n\n\n\nCode\nempty_rast &lt;- rast()\n\n\n\n\nPractice questions\n\n\nCode\n# Question 1\nrr &lt;- rast(xmin=-5, xmax=5, ymin=-5, ymax=5, res=2)\n\n# By looking at the help file, we realized that res overrides nrows/ncols\n# This code uses the default global extent\nrr2 &lt;- rast(nrows=5, ncols=5, res=2)\n\n\n\n\nCode\n# Question 2\nvalues(rr) &lt;- runif(25)\n\n# The \"values were recycled\" error clues us in that we don't have 25 cells\nvalues(rr2) &lt;- runif(25)\n\n\nWarning: [setValues] values were recycled\n\n\n\n\nCode\n# Question 3\norigin(rr)\n\n\n[1] 1 1\n\n\n\n\nCode\n# Question 4\nvalues(rr)[adjacent(rr, cells=12)] &lt;- NA\nplot(rr)\n\n\n\n\n\n\n\n\n\nCode\n# You can do this in two steps \n# by making an object to hold the adjacent cell numbers\nadj &lt;- adjacent(rr, cells=12)\nvalues(rr)[adj] &lt;- NA\n\n\nFor more information on subsetting, see the Software Carpentries R for Reproducible Data Analysis: Subsetting Data lesson.\n\n\nCode\n# Question 5\nplot(distance(rr))\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\n\n\n\n\n\n\n\n\n\nExtra practice\nTo run examples of functions, go to the help file and scroll down to the “Examples” section. You can either copy-paste the code into your file or click the link that says “run examples.”\n\n\nCode\n# Question 2\n\nfire_rast &lt;- rast(\"/opt/data/data/assignment03/wildfire_hazard_agg.tif\")\nplot(distance(fire_rast))",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Predicates and Measures"
    ]
  },
  {
    "objectID": "example/session-27-example.html",
    "href": "example/session-27-example.html",
    "title": "Session 27 code",
    "section": "",
    "text": "Load libraries:\n\n\nCode\nlibrary(mapview)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\n\nLoad data:\n\n\nCode\nlandmarks &lt;- read_csv(\"/opt/data/data/assignment04/landmarks_ID.csv\") %&gt;%\n  st_as_sf(., coords=c(\"longitude\", \"lattitude\"), crs=4326)\n\nfire.haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\nInteractivity with mapview:\n\n\nCode\nmapview(landmarks)\n\n\n\n\nCode\nmapview(raster::raster(fire.haz))\n\n\nAPI with tidycensus:\n\n\nCode\nlibrary(tidycensus)\n\nv20 &lt;- load_variables(2020, \"acs5\")\n\ncounty_pop_white &lt;- get_acs(geography = \"county\",\n                            year = 2020,\n                            variables = c(\"B02001_001\", \"B02001_002\"),\n                            state = \"ID\",\n                            geometry = TRUE)\n\ncounty_perc_white &lt;- county_pop_white %&gt;%\n  st_make_valid(.) %&gt;%\n  filter(!st_is_empty(.)) %&gt;%\n  st_transform(., crs=st_crs(landmarks)) %&gt;%\n  dplyr::select(-moe) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  mutate(perc_white = B02001_002/B02001_001*100)\n\n\nInteractivity with tmap:\nStatic map:\n\n\nCode\nlibrary(tmap)\n\ntm_shape(county_perc_white) +\n  tm_polygons(col=\"perc_white\")\n\n\nZoom map:\n\n\nCode\ntmap_mode(\"view\")\n\ntm_shape(county_perc_white) +\n  tm_polygons(col=\"perc_white\")\n\n\nChange pop-up value:\n\n\nCode\ntm_shape(county_perc_white) +\n  tm_polygons(col=\"perc_white\", id = \"perc_white\")\n\n\nAdd layer:\n\n\nCode\ntm_shape(county_perc_white) +\n  tm_polygons(col=\"perc_white\") +\n  tm_shape(landmarks) +\n  tm_dots()\n\n\nChange interactivity of different layers:\n\n\nCode\ntm_shape(county_perc_white) +\n  tm_polygons(col=\"perc_white\", interactive=FALSE) +\n  tm_shape(landmarks) +\n  tm_dots(id=\"FULLNAME\")",
    "crumbs": [
      "Examples",
      "Visualizing Spatial Data",
      "Interactivity"
    ]
  },
  {
    "objectID": "slides/01-slides.html#todays-plan",
    "href": "slides/01-slides.html#todays-plan",
    "title": "Getting Started",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nIntroductions\nWhy (not) R?\nCourse logistics and resources\nTesting out RStudio, git, and GitHub Classroom"
  },
  {
    "objectID": "slides/01-slides.html#about-me",
    "href": "slides/01-slides.html#about-me",
    "title": "Getting Started",
    "section": "About Me",
    "text": "About Me\n\n\n\n\nWhat I do\nMy path to this point\nWhy I teach this course"
  },
  {
    "objectID": "slides/01-slides.html#what-about-you",
    "href": "slides/01-slides.html#what-about-you",
    "title": "Getting Started",
    "section": "What about you?",
    "text": "What about you?\n\n\n\nYour preferred pronouns\nWhere are you from?\nWhat do you like most about Boise?\nWhat do you miss most about “home”?\nWhat is your research?"
  },
  {
    "objectID": "slides/01-slides.html#why-r",
    "href": "slides/01-slides.html#why-r",
    "title": "Getting Started",
    "section": "Why R?",
    "text": "Why R?\n\n\n\n\nOpen Source\nHuge user community\nIntegrated analysis pipelines\nReproducible workflows\n\n\n\n\nCodePlot\n\n\n\nlibrary(maps)\nlibrary(socviz)\nlibrary(tidyverse)\nparty_colors &lt;- c(\"#2E74C0\", \"#CB454A\") \nus_states &lt;- map_data(\"state\")\nelection$region &lt;- tolower(election$state)\nus_states_elec &lt;- left_join(us_states, election)\np0 &lt;- ggplot(data = us_states_elec,\n             mapping = aes(x = long, y = lat,\n                           group = group, \n                           fill = party))\np1 &lt;- p0 + geom_polygon(color = \"gray90\", \n                        size = 0.1) +\n    coord_map(projection = \"albers\", \n              lat0 = 39, lat1 = 45) \np2 &lt;- p1 + scale_fill_manual(values = party_colors) +\n    labs(title = \"Election Results 2016\", \n         fill = NULL)"
  },
  {
    "objectID": "slides/01-slides.html#why-not-r-1",
    "href": "slides/01-slides.html#why-not-r-1",
    "title": "Getting Started",
    "section": "Why not R?",
    "text": "Why not R?\n\n## ---\n## Error: could not find function \"performance\"\n## ---\n##  [1] \"Error in if (str_count(string = f[[j]], \n##  pattern = \\\"\\\\\\\\S+\\\") == 1) \n##  { : \\n  argument is of length zero\"   \n## ---\n## Error in eval(expr, envir, enclos) : object 'x' not found\n## ---\n## Error in file(file, \"rt\") : cannot open the connection\n## ---\n\n\n\n\nCoding can be hard…\nMemory challenges\n\n\n\nSpeed\nDecision fatigue"
  },
  {
    "objectID": "slides/01-slides.html#getting-help",
    "href": "slides/01-slides.html#getting-help",
    "title": "Getting Started",
    "section": "Getting Help",
    "text": "Getting Help\n\n\n\nGoogle it!!\n\nUse the exact error message\nInclude the package name\ninclude “R” in the search\n\n\n\n\nStack Overflow\n\nReproducible examples\n\nPackage “issue” pages\nr_spatial slack channel\nCommon errors\n\n\n\nAsk Me"
  },
  {
    "objectID": "slides/01-slides.html#logistics",
    "href": "slides/01-slides.html#logistics",
    "title": "Getting Started",
    "section": "Logistics",
    "text": "Logistics\n\n\nMeet on Mondays and Wednesdays\n~55 min lecture, 20 min practice\n4 major sections\nReadings"
  },
  {
    "objectID": "slides/01-slides.html#course-webpage",
    "href": "slides/01-slides.html#course-webpage",
    "title": "Getting Started",
    "section": "Course Webpage",
    "text": "Course Webpage\nhttps://isdrfall24.classes.spaseslab.com/\n\n\nSyllabus\nSchedule\nLectures\nAssignments\nResources"
  },
  {
    "objectID": "slides/01-slides.html#assignments",
    "href": "slides/01-slides.html#assignments",
    "title": "Getting Started",
    "section": "Assignments",
    "text": "Assignments\n\nCheck out the syllabus for more on grading!\n\n\n\n\n\n\nSelf-reflections (2x)\n\nYour goals for the course\nEvaluation criteria\n\nCoding exercises (10x)\n\nProblem solving\nReproducible workflows\nMuscle memory\n\n\n\n\nCode Revisions (3x)\n\nDigging deeper\nCommon issues\nMore extensive feedback\n\nFinal project (1st draft, final draft)\n\nPractice a full analysis workflow\nIntegrate analysis & visuals to tell a story"
  },
  {
    "objectID": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "href": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "title": "Getting Started",
    "section": "Orientation to RStudio and our RStudio server",
    "text": "Orientation to RStudio and our RStudio server"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git",
    "href": "slides/01-slides.html#introduce-yourself-to-git",
    "title": "Getting Started",
    "section": "Introduce yourself to Git",
    "text": "Introduce yourself to Git\n\nLots of ways, but one easy way is:\n\n\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\") #your info here\n\n\nGenerate a PAT token if you don’t have one (make sure you save it somewhere)\n\n\nusethis::create_github_token()"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "href": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "title": "Getting Started",
    "section": "Introduce yourself to Git (cont’d)",
    "text": "Introduce yourself to Git (cont’d)\n\nStore your credentials for use (times out after 1 hr)\n\n\ngitcreds::gitcreds_set()\n\n\nVerify\n\n\ngitcreds::gitcreds_get()"
  },
  {
    "objectID": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "href": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "title": "Getting Started",
    "section": "Joining the assignment and cloning the repo",
    "text": "Joining the assignment and cloning the repo\n\nClick this link\nBring the project into RStudio\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\nPaste the link from the “Clone Repository” button into the “Repository URL” space"
  },
  {
    "objectID": "slides/01-slides.html#the-git-workflow",
    "href": "slides/01-slides.html#the-git-workflow",
    "title": "Getting Started",
    "section": "The git workflow",
    "text": "The git workflow\n\nMake sure to pull every time you start working on a project\nMake some changes to code\nSave those changes\nCommit your changes\nPush your work to the remote!"
  },
  {
    "objectID": "slides/01-slides.html#checking-in",
    "href": "slides/01-slides.html#checking-in",
    "title": "Getting Started",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis?\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "assignment/08-combinationssolutions.html",
    "href": "assignment/08-combinationssolutions.html",
    "title": "Assignment 8 Solutions: Autocorrelation and Interpolation",
    "section": "",
    "text": "Read in the disasters dataset, convert it to points, filter it to those disasters in Idaho, and select any relevant columns. You will also need to use tigris::county() to download a county shapefile for the region. Make sure your data are projected correctly\n\nI start by loading the packages necessary for the entire analysis. Then, I use the tigris package to get my county files. We need this to subset the disaster data into our region of interest. Note that I used the entire region because it’s possible that there is information to be learned from the data on the borders of Idaho that don’t conform to the state boundaries. I then load the disaster dataset, select a handful of columns that I’m interest in, drop any records that are missing their coordinates, convert the csv to a sf object, project it to the same CRS as the county dataset, and then keep only the distinct point locations. This last step is important because having multiple events in the exact same location creates issues for calculating our spatial autocorrelation estimates (because the distance is exactly zero making it difficult to determine which event is the “parent”).\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(sp)\nlibrary(terra)\nlibrary(tmap)\n\ncty &lt;- tigris::counties(state = c(\"ID\", \"WA\", \"OR\"), progress_bar=FALSE)\n\ndisast.sf &lt;- read_csv(\"/opt/data/data/assignment07/ics209-plus-wf_incidents_1999to2020.csv\") %&gt;% \n  filter(., START_YEAR &gt;= 2000 & START_YEAR &lt;= 2017) %&gt;% \n  select(INCIDENT_ID, , POO_STATE, POO_LATITUDE, POO_LONGITUDE, FATALITIES, PROJECTED_FINAL_IM_COST, STR_DESTROYED_TOTAL, PEAK_EVACUATIONS) %&gt;% \n  drop_na(c(POO_LATITUDE, POO_LONGITUDE)) %&gt;% \n  st_as_sf(coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4326) %&gt;% \n  st_transform(., st_crs(cty)) %&gt;% \n  distinct(., geometry, .keep_all=TRUE)\n  \ndisast.sf &lt;- disast.sf[cty,]\n\nGenerate the Ripley’s K curves for the disaster dataset. What do you think? Is there evidence that the data is spatially autocorrelated?\n\nWe use the same code from class here to estimate the Ripley’s K function. We first select the variable we’re interested in (STR_DESTROYED_TOTAL in my case), transform the CRS to a planar coordinate system, and convert it to a ppp object for spdep. We use the envelope function with Kest to calculate several theoretical values for Ripley’s K under complete spatial randomness. Comparing the K_{obs} to the envelope of theoretical values suggests that there is more aggregation in the data than would be predicted under CSR.\n\n\nkf.env &lt;- envelope(as.ppp(st_transform(select(disast.sf, STR_DESTROYED_TOTAL), crs=8826)), Kest, correction = \"translation\",  nsim= 1000, envelope = TRUE, verbose = FALSE)\n\nplot(kf.env)\n\n\n\n\n\n\n\n\nUse the nearest-neighbor approach that we used in class to estimate the lagged values for the disaster dataset and estimate the slope of the line describing Moran’s I statistic.\n\nWe begin by finding the nearest neighbor for each observation using the knearneigh function which finds the k closest neighbors for each point. Because we only want the nearest neighbor, we set k=1. We need to convert this to a neighbor list (class(geog.nearnb) = nb) and do this by wrapping the output of knearneigh inside of knn2nb which converts knn objects to nb objects. We then need to estimate the distance to each neighbor (using dnearneigh) and convert it to a spatial weights matrix (using nb2listw). Finally, we convert this weight’s matrix into a vector of the same number of rows as our disaster dataset using lag.listw. This function creates a new estimate of STR_DESTROYED_TOTAL for each row based on the spatially weighted value of the nearest neighbor. Finally, we fit a simple linear regression to the data and see that there is a slight positive slope to the line suggesting that there is some autocorrelation (remember, the slope of this simple linear model is the Moran’s I coefficient).\n\n\ngeog.nearnb &lt;- knn2nb(knearneigh(disast.sf, k = 1), row.names = disast.sf$INCIDENT_ID, sym=TRUE); #estimate distance to first neareset neighbor\nnb.nearest &lt;- dnearneigh(disast.sf, 0,  max( unlist(nbdists(geog.nearnb, disast.sf))));\nlw.nearest &lt;- nb2listw(nb.nearest, style=\"W\", zero.policy = TRUE)\nbldg.lag &lt;- lag.listw(lw.nearest, disast.sf$STR_DESTROYED_TOTAL)\nM &lt;- lm(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL)\nsummary(M)\n\n\nCall:\nlm(formula = bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1969 -0.9554 -0.7483 -0.0924 14.3268 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    1.04165    0.03505  29.721  &lt; 2e-16 ***\ndisast.sf$STR_DESTROYED_TOTAL  0.01487    0.00309   4.812 1.56e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.047 on 3438 degrees of freedom\nMultiple R-squared:  0.00669,   Adjusted R-squared:  0.006402 \nF-statistic: 23.16 on 1 and 3438 DF,  p-value: 1.557e-06\n\nplot(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL, xlim=c(0,20))\nabline(M, col=\"red\")\n\n\n\n\n\n\n\n\nNow use the permutation approach to compare your measured value to one generated from multiple simulations. Generate the plot of the data. Do you see more evidence of spatial autocorrelation?\n\nWe can verify this by using a Monte Carlo permutation approach. We use a for loop to “shuffle” the data (using sample), but keep the same neighbor structure (using our same lw.nearest spatial weights matrix). We then fit a linear model to the reshuffled data and estimate the slope to see what values are plausible under complete spatial randomness (which we achieve by shuffling the data independent of their location). Run this loop 1000 times (set by n &lt;- 1000L) and you’ll generate a distribution of plausible values. Based on the distribution and our actual value (in red), we can see that this value for Moran’s I is generally larger than we’d expect under CSR, but not terribly so.\n\n\nn &lt;- 1000L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle income values\n  x &lt;- sample(disast.sf$STR_DESTROYED_TOTAL, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag &lt;- lag.listw(lw.nearest, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}\n\nhist(I.r, main=NULL, xlab=\"Moran's I\", las=1)\nabline(v=coef(M)[2], col=\"red\")\n\n\n\n\n\n\n\n\nGenerate the 0th, 1st, and 2nd order spatial trend surfaces for the data. Is there evidence for a second order trend? How can you tell?\n\nIn order to generate a spatial trend surface, we need to predict values across a uniform grid that covers the study region. We initialize that grid by using our county dataset and drawing 15000 random sample points across the region. We then create a series of formula object depicting the 0th, 1st (linear), and 2nd (quadratic) models to the data where the predictors are just the X and Y coordinates. We fit each of the models using lm, convert them into a SpatialGridDataFrame from the sp package, and then convert them to a raster to make plotting easier. Based on the curvature we see in the 2nd order trend surface, there is an indication of a 2nd order trend, though it is not super strong. We’ll use that model for kriging in the subsequent steps.\n\n\n#set up interpolation grid\n# Create an empty grid where n is the number of grid cells in X and Y direction\ngrd &lt;- st_make_grid(cty, n=123, \n                    what = \"centers\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2])\n\n# Define the polynomial equation\nf.0  &lt;- as.formula(PROJECTED_FINAL_IM_COST ~ 1)\n\n# Run the regression model\nlm.0 &lt;- lm( f.0 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ngrd$var0.pred &lt;- predict(lm.0, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.0th &lt;- grd %&gt;%\n  select(X, Y, var0.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.0th, crs = crs(grd))\nr.m0 &lt;- mask(r, cty)\n\nf.1  &lt;- as.formula(STR_DESTROYED_TOTAL ~ X + Y)\n\ndisast.sf$X &lt;- st_coordinates(disast.sf)[,1]\ndisast.sf$Y &lt;- st_coordinates(disast.sf)[,2]\n\n# Run the regression model\nlm.1 &lt;- lm( f.1 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ngrd$var1.pred &lt;- predict(lm.1, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.1st &lt;- grd %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.1st, crs = crs(grd))\nr.m1 &lt;- mask(r, cty)\n\nf.2 &lt;- as.formula(STR_DESTROYED_TOTAL ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n\n# Run the regression model\nlm.2 &lt;- lm( f.2, data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ngrd$var2.pred &lt;- predict(lm.2, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.2nd &lt;- grd %&gt;%\n  select(X, Y, var2.pred) %&gt;%\n  st_drop_geometry()\n\nr   &lt;- rast(dat.2nd, crs = crs(grd))\nr.m2 &lt;- mask(r, cty)\n\n# stack to plot\nrst.stk &lt;- c(r.m0, r.m1, r.m2)\nnames(rst.stk) &lt;- c(\"zeroOrder\", \"firstOrder\", \"secondOrder\")\nplot(rst.stk)\n\n\n\n\n\n\n\n\nNow use the spatial trend surface to perform some ordinary kriging. You’ll want to have a grid of 15,000 points, fit 3 different experimental variogram functions (see the vgm function helpfile to learn more about the shapes available to you). Plot your variogram fits. Which one would you choose? Why?\n\nA variogram simply plots the relationship between distance and the residuals of a model. We first assign those residuals to our disaster dataset. We then generate a cloud-style variogram for data without eliminating the spatial trend. As you can see, there are some strange bands that show up in the data likely due to the second order effects we saw in the model previously.\n\n\ndisast.sf$res &lt;- lm.2$residuals\n\nvar.cld  &lt;- gstat::variogram(res ~ 1, disast.sf, cloud = TRUE)\nvar.df  &lt;- as.data.frame(var.cld)\n\n\nOP &lt;- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\n\n\n\n\n\n\n\npar(OP)\n\n\nWe then fit a variogram to the detrended data (this is the sample variogram) by passing our f.2 formula to the variogram function. We take the mean values of the pairwise differences and plot them in bins on top of the original data. As you can see, this reduces a considerable amount of noise and the shape of the variogram begins to materialize.\n\n\nvar.smpl &lt;- gstat::variogram(f.2, disast.sf, cloud = FALSE)\n\nbins.ct &lt;- c(0, var.smpl$dist , max(var.cld$dist) )\nbins &lt;- vector()\nfor (i in 1: (length(bins.ct) - 1) ){\n  bins[i] &lt;- mean(bins.ct[ seq(i,i+1, length.out=2)] ) \n}\nbins[length(bins)] &lt;- max(var.cld$dist)\nvar.bins &lt;- findInterval(var.cld$dist, bins)\nvar.cld2 &lt;- var.cld[var.cld$gamma &lt; 500,]\nOP &lt;- par( mar = c(5,6,1,1))\nplot(var.cld2$gamma ~ eval(var.cld2$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,\n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( gamma ) )\npoints( var.smpl$dist/1000, var.smpl$gamma, pch=21, col=\"black\", bg=\"red\", cex=1.3)\nabline(v=bins/1000, col=\"red\", lty=2)\n\n\n\n\n\n\n\npar(OP)\n\n\nYou can use vgm() to see potential shapes of the different variograms that we can fit using gstat. I chose linear, Gaussian, and spherical as they are common choices and because the initial rise to the sill seemed consistent with those shapes. Note that the semivariance begins to increase again you get further out. This might suggest that we need a more complicated de-trending model, but we won’t worry about that now. The 3 forms seem to fit the data in similar ways, but the spherical form has a slightly more gradual rise to the sill. I choose that one as that may help smooth a bit more than the other 2.\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit.lin  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill= 50, model=\"Lin\"))\ndat.fit.gau  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Gau\"))\ndat.fit.sph  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Sph\"))\n\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit.lin, main = \"Linear variogram\")\n\n\n\n\n\n\n\nplot(var.smpl, dat.fit.gau, main = \"Gaussian variogram\")\n\n\n\n\n\n\n\nplot(var.smpl, dat.fit.sph, main = \"Spherical variogram\")\n\n\n\n\n\n\n\n\nUsing your spatial trend model and your fitted variogram, krige the data and generate a map of the interpolated value and a map of the error.\n\nNow that we’ve got our Spherical variogram estimated on the detrended data, we can use the krige function to generate spatial predictions across the grid. We can also access the variance resulting from that model. We do that, convert them to rasters and plot the two outcomes. We can see that we’ve eliminated the bulk of spatial patterns in the residuals (as evidenced by light colors on the predicted residual map); however, the predictions for the western cost are much less stable (as evidenced by the variance map).\n\n\ndat.krg &lt;- gstat::krige( formula = f.2, \n                         locations = disast.sf, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit.sph)\n\n[using universal kriging]\n\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m.pred &lt;- mask(r, cty)\n\ntm_shape(r.m.pred) + \n  tm_raster(n=10, palette=\"RdBu\", title=\"Predicted \\nstructures destroyed\")  +\n  tm_legend(legend.outside=TRUE)\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\ndat.krg.res &lt;- gstat::krige( formula = res ~ 1, \n                         locations = disast.sf, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit.sph)\n\n[using ordinary kriging]\n\ndat.krg.res &lt;-  dat.krg.res %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.res, crs = crs(grd))\nr.m.res &lt;- mask(r, cty)\n\ntm_shape(r.m.res) + \n  tm_raster(n=10, palette=\"RdBu\", title=\"Predicted residual\\nstructures destroyed\")  +\n  tm_legend(legend.outside=TRUE)\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\ndat.krg.var &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.var) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.var, crs = crs(grd))\nr.m.var &lt;- mask(r, cty)\ntm_shape(r.m.var) + \n  tm_raster(n=7, palette =\"Reds\", ,title=\"Variance map \") +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "example/session-28-example.html",
    "href": "example/session-28-example.html",
    "title": "Climate, social, and environmental justice markers for the Pacific Northwest",
    "section": "",
    "text": "The YAML header for the app is:\n\n\nCode\n---\ntitle: \"Climate, social, and environmental justice markers for the Pacific Northwest\"\noutput: flexdashboard::flex_dashboard\nruntime: shiny\n---\n\n\nglobal code chunk:\n\n\nCode\n```{r global}\n#| eval: false\n# replace 'eval:false' with 'include: false'\n\nlibrary(shiny)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\ntmap_mode(\"view\")\n\ncejst &lt;- st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n# get column codes and meanings\ncol_choices &lt;- read_csv(\"/opt/data/data/assignment04/columns.csv\") %&gt;%\n  # make nicer column names for a display table\n  rename(\"Code\" = \"shapefile_column\", \"Description\" = \"column_name\") %&gt;%\n  # keep only \"percentile\" type columns\n  filter(str_detect(Code, \"PFS\"))\n```\n\n\nCreate a sidebar column:\n\n\nCode\nColumn {.sidebar}\n-----------------------------------------------------------------------\n\nSelect the cejst marker:\n\n\nAdd code chunk with inputs under the sidebar column:\n\n\nCode\n# Box with choices: which cejst column to map\nselectInput(\"column_select\", \n            label = \"Justice Marker:\",\n            choices = col_choices$Code, \n            selected = \"DF_PFS\")\n\n# Two sliders to select the maximum and minimum values to map\nsliderInput(\"min_threshold_adjust\", \n            label = \"Minimum value:\",\n            min = 0, \n            max = 1, \n            value = 0.5, \n            step = 0.05)\nsliderInput(\"max_threshold_adjust\", label = \"Maximum value:\",\n            min = 0, max = 1, value = 1, step = 0.05)\n\n\nAdd table in sidebar:\n\n\nCode\nknitr::kable(col_choices[,1:2])\n\n\nAdd another column for map display:\n\n\nCode\nColumn\n-----------------------------------------------------------------------\n\n### Climate, Social, and Environmental Justice\n\n\nRender map with filters based on user inputs:\n\n\nCode\n# renderTmap is a tmap special case of renderPlot\nrenderTmap({\n  # put reactively filtered data in tm_shape\n  tm_shape(subset(cejst[, input$column_select], # subset data to user's column\n                  # use the subset in the filtering steps, selecting the column of data with [[1]]\n                  cejst[, input$column_select][[1]] &lt;= input$max_threshold_adjust & # data column should be less than or equal to the user's max threshold\n                    cejst[, input$column_select][[1]] &gt;= input$min_threshold_adjust)) + #more than or equal to the min threshold\n    # add the polygons filled by the user's selected column\n    tm_polygons(col = input$column_select)\n})\n\n\nYou can test your code by creating the “base” plot first, then adding the input$ reactive elements later.\n\n\nCode\n# how to test your code\ntm_shape(subset(cejst[, \"DF_PFS\"], \n                  cejst[, \"DF_PFS\"][[1]] &lt;= 1 &\n                    cejst[, \"DF_PFS\"][[1]] &gt;= 0.5)) +\n    tm_polygons(col = \"DF_PFS\")",
    "crumbs": [
      "Examples",
      "Visualizing Spatial Data",
      "Interactive Dashboard"
    ]
  },
  {
    "objectID": "example/session-09-example.html",
    "href": "example/session-09-example.html",
    "title": "Intro to Mapping",
    "section": "",
    "text": "Base plot methods:\nLoad library and vector data:\n\n\nCode\nlibrary(sf)\n\ncejst &lt;- st_read(\"/opt/data/data/assignment04/cejst_nw.shp\")\n\n\nPlot vector data:\n\n\nCode\nplot(st_geometry(cejst))\n\nplot(cejst$geometry)\n\n\n\n\n\n\n\n\n\nCode\nplot(cejst[\"EALR_PFS\"])\n\n\n\n\n\n\n\n\n\nSee column name meanings:\n\n\nCode\nView(read.csv(\"/opt/data/data/assignment04/columns.csv\"))\n\n\nRead in library and raster data:\n\n\nCode\nlibrary(terra)\n\nrast.data &lt;- rast(\"/opt/data/data/assignment03/wildfire_hazard_agg.tif\")\n\n\nPlot raster:\n\n\nCode\nplot(rast.data)\n\n\n\n\n\n\n\n\n\nCode\nplot(rast.data, col=heat.colors(24, rev=TRUE))\n\n\n\n\n\n\n\n\n\nCombine raster and vector data:\n\n\nCode\nplot(rast.data, col=heat.colors(24, rev=TRUE))\nplot(st_geometry(st_transform(cejst, crs=crs(rast.data))), add=TRUE)\n\n\n\n\n\n\n\n\n\nCombining two vectors:\nIn class, we could not get the bounding box to appear. The fix is to plot the bounding box before the census tracts. Why would this be? plot will only plot a geometry if the entire shape fits in the current plot window. Because of rounding error introduced in st_as_sfc and st_transform, the bounding_box polygon is slightly larger than the plot window. Because plot couldn’t fit all its vertices, the bounding box did not appear.\n\n\nCode\nbounding_box &lt;- st_as_sfc(st_bbox(cejst))\n\nplot(st_geometry(st_transform(bounding_box, crs=st_crs(cejst))), col=\"red\")\nplot(cejst[\"EALR_PFS\"], add=TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# note how xmax of the bounding_box object is slightly higher than the true xmax\nst_bbox(cejst)\n\n\n      xmin       ymin       xmax       ymax \n-124.76255   41.98801 -111.04349   49.00249 \n\n\nCode\nst_coordinates(st_geometry(st_transform(bounding_box, crs=st_crs(cejst))))\n\n\n             X        Y L1 L2\n[1,] -124.7625 41.98801  1  1\n[2,] -111.0435 41.98801  1  1\n[3,] -111.0435 49.00249  1  1\n[4,] -124.7625 49.00249  1  1\n[5,] -124.7625 41.98801  1  1\n\n\n\n\ntmap methods:\n\n\nCode\nlibrary(tmap)\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(viridis)\n\n\nLoading required package: viridisLite\n\n\nCode\ncejst_filt &lt;- cejst %&gt;%\n  filter(!st_is_empty(.))\n\npt &lt;- tm_shape(cejst_filt) +\n  tm_polygons(col = \"EALR_PFS\", n=10, palette=viridis(10),\n              border.col = \"white\") +\n  tm_legend(outside = TRUE)\n\n\nLayering in tmap:\n\n\nCode\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS %in% c(\"ID\", \"WA\", \"OR\")) %&gt;% \n  st_transform(., crs = st_crs(cejst))\n\n\nRetrieving data for the year 2021\n\n\nCode\npt2 &lt;- tm_shape(cejst_filt) +\n  tm_polygons(col = \"EALR_PFS\", n=10, palette=viridis(10),\n              border.col=\"white\") +\n  tm_shape(st) +\n  tm_borders(col=\"red\") +\n  tm_legend(outside = TRUE)\n\n\nLayering a raster in tmap:\n\n\nCode\ncejst.proj &lt;- st_transform(cejst, crs=crs(rast.data)) %&gt;% filter(!st_is_empty(.))\nstates.proj &lt;- st_transform(st, crs=crs(rast.data))\n\npal8 &lt;- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\n\npt3 &lt;- tm_shape(rast.data) +\n  tm_raster() +\n  # tm_shape(cejst.proj) + \n  # tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n  #             border.col = \"white\") + \n  tm_shape(states.proj) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)\n\n\nYou can use tmap_mode(\"view\") to enable zoom on your maps.",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Intro to Mapping"
    ]
  },
  {
    "objectID": "example/session-19-example.html",
    "href": "example/session-19-example.html",
    "title": "Session 19 code",
    "section": "",
    "text": "Load libraries:\nCode\nlibrary(terra)\nlibrary(tmap)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(spatstat)  # Used for the dirichlet tesselation function\nlibrary(sp)\nGet data:\nCode\naq &lt;- read_csv(\"/opt/data/data/classexamples/ad_viz_plotval_data_PM25_2024_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"Site Longitude\", \"Site Latitude\"), crs = \"EPSG:4326\") %&gt;% \n  st_transform(., crs = \"EPSG:8826\") %&gt;% \n  mutate(date = as_date(parse_datetime(Date, \"%m/%d/%Y\"))) %&gt;% \n  filter(., date &gt;= 2024-07-01) %&gt;% \n  filter(., date &gt; \"2024-07-01\" & date &lt; \"2024-07-31\")\naq.sum &lt;- aq %&gt;% \n  group_by(., `Site ID`) %&gt;% \n  summarise(., meanpm25 = mean(`Daily AQI Value`))\n\nid.cty &lt;- tigris::counties(state = \"ID\") %&gt;%\n  st_transform(., crs = st_crs(aq.sum))",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation II"
    ]
  },
  {
    "objectID": "example/session-19-example.html#trend-surfaces",
    "href": "example/session-19-example.html#trend-surfaces",
    "title": "Session 19 code",
    "section": "Trend Surfaces",
    "text": "Trend Surfaces\n\n0th Order Trend\n\n\nCode\n#set up interpolation grid\n# Create an empty grid where n is the total number of cells\ngrd &lt;- st_make_grid(id.cty, n=150, \n                    what = \"centers\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2])\n\n# Define the polynomial equation\nf.0  &lt;- as.formula(meanpm25 ~ 1)\n\n# Run the regression model\nlm.0 &lt;- lm( f.0 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var0.pred &lt;- predict(lm.0, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.0th &lt;- grd %&gt;%\n  select(X, Y, var0.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.0th, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + \n  tm_raster( title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nFirst Order Trend\n\n\nCode\n# Define the polynomial equation\nf.1  &lt;- as.formula(meanpm25 ~ X + Y)\n\naq.sum$X &lt;- st_coordinates(aq.sum)[,1]\naq.sum$Y &lt;- st_coordinates(aq.sum)[,2]\n\n# Run the regression model\nlm.1 &lt;- lm( f.1 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var1.pred &lt;- predict(lm.1, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.1st &lt;- grd %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.1st, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + \n  tm_raster( title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nSecond Order Trend\n\n\nCode\n# Define the 1st order polynomial equation\nf.2 &lt;- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n \n# Run the regression model\nlm.2 &lt;- lm( f.2, data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ngrd$var2.pred &lt;- predict(lm.2, newdata = grd)\n# Use data.frame without geometry to convert to raster\ndat.2nd &lt;- grd %&gt;%\n  select(X, Y, var2.pred) %&gt;%\n  st_drop_geometry()\n\nr   &lt;- rast(dat.2nd, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +\n  tm_shape(aq.sum) + \n  tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation II"
    ]
  },
  {
    "objectID": "example/session-19-example.html#kriging",
    "href": "example/session-19-example.html#kriging",
    "title": "Session 19 code",
    "section": "Kriging",
    "text": "Kriging\n\n\nCode\naq.sum$res &lt;- lm.2$residuals\n\n\n\n\nCode\nvar.cld  &lt;- gstat::variogram(res ~ 1, aq.sum, cloud = TRUE)\nvar.df  &lt;- as.data.frame(var.cld)\nindex1  &lt;- which(with(var.df, left==21 & right==2))\n\nOP &lt;- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute the sample variogram, note the f.2 trend model is one of the parameters\n# passed to variogram(). This tells the function to create the variogram on\n# the de-trended data\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range= 60000, model=\"Gau\", cutoff=1000000))\n\n\nWarning in gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range =\n60000, : No convergence after 200 iterations: try different initial values?\n\n\nCode\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat.krg &lt;- gstat::krige( formula = f.2, \n                         locations = aq.sum, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit)\n\n\n[using universal kriging]\n\n\nCode\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\ndat.krg.var &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.var) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\nr.var &lt;- rast(dat.krg.var, crs = crs(grd))\nr.m.var &lt;- mask(r.var, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation II"
    ]
  },
  {
    "objectID": "example/session-19-example.html#playing-with-semivariograms",
    "href": "example/session-19-example.html#playing-with-semivariograms",
    "title": "Session 19 code",
    "section": "Playing with semivariograms",
    "text": "Playing with semivariograms\nWe tried changing the model, nugget, range, and psill arguments in gstat::vgm.\n\n\nCode\n# Compute the sample variogram, note the f.2 trend model is one of the parameters\n# passed to variogram(). This tells the function to create the variogram on\n# the de-trended data\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(model=\"Sph\", \n                                                      nugget = 10,\n                                                      range = 60000))\n\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat.krg &lt;- gstat::krige( formula = f.2, \n                         locations = aq.sum, \n                         newdata = grd[, c(\"X\", \"Y\", \"var2.pred\")], \n                         model = dat.fit)\n\n\n[using universal kriging]\n\n\nCode\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\ndat.krg.var &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.var) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\nr.var &lt;- rast(dat.krg.var, crs = crs(grd))\nr.m.var &lt;- mask(r.var, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(r.m.var) + tm_raster(n=20, title=\"Variance\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation II"
    ]
  },
  {
    "objectID": "example/session-19-example.html#kriging-with-the-first-order-trend",
    "href": "example/session-19-example.html#kriging-with-the-first-order-trend",
    "title": "Session 19 code",
    "section": "Kriging with the First Order Trend",
    "text": "Kriging with the First Order Trend\n\n\nCode\n# Compute the sample variogram, note the f.2 trend model is one of the parameters\n# passed to variogram(). This tells the function to create the variogram on\n# the de-trended data\nvar.smpl &lt;- gstat::variogram(f.1, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(model=\"Exp\", \n                                                      nugget = 20))\n\n\nWarning in gstat::fit.variogram(var.smpl, gstat::vgm(model = \"Exp\", nugget =\n20)): No convergence after 200 iterations: try different initial values?\n\n\nCode\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat.krg &lt;- gstat::krige( formula = f.1, \n                         locations = aq.sum, \n                         newdata = grd[, c(\"X\", \"Y\", \"var1.pred\")], \n                         model = dat.fit)\n\n\n[using universal kriging]\n\n\nCode\ndat.krg.preds &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.pred) %&gt;%\n  st_drop_geometry()\n\ndat.krg.var &lt;-  dat.krg %&gt;%\n  mutate(X = st_coordinates(.)[, 1], \n         Y = st_coordinates(.)[, 2]) %&gt;%\n  select(X, Y, var1.var) %&gt;%\n  st_drop_geometry()\n\nr &lt;- rast(dat.krg.preds, crs = crs(grd))\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\nr.var &lt;- rast(dat.krg.var, crs = crs(grd))\nr.m.var &lt;- mask(r.var, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + tm_raster(n=10, title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(r.m.var) + tm_raster(n=20, title=\"Variance\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)",
    "crumbs": [
      "Examples",
      "Statistical Workflows",
      "Interpolation II"
    ]
  },
  {
    "objectID": "slides/03-slides.html#todays-plan",
    "href": "slides/03-slides.html#todays-plan",
    "title": "Introduction to Spatial Data",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWays to view the world\nWhat makes data (geo)spatial?\nCoordinate Reference Systems\nGeometries, support, and spatial messiness"
  },
  {
    "objectID": "slides/03-slides.html#as-a-series-of-objects",
    "href": "slides/03-slides.html#as-a-series-of-objects",
    "title": "Introduction to Spatial Data",
    "section": "…As a Series of Objects?",
    "text": "…As a Series of Objects?\n\n\n\n\nThe world is a series of entities located in space.\nUsually distinguishable, discrete, and bounded\nSome spaces can hold multiple entities, others are empty\nObjects are digital representations of entities"
  },
  {
    "objectID": "slides/03-slides.html#as-a-continuous-field",
    "href": "slides/03-slides.html#as-a-continuous-field",
    "title": "Introduction to Spatial Data",
    "section": "…As a Continuous Field",
    "text": "…As a Continuous Field\n\n\n\n\nThe earth is a single entity with properties that vary continuosly through space\nSpatial continuity: Every cell has a value (including “no data” or “not here”)\nSelf-definition: the values define the field\nSpace is tessellated: cells are mutually exclusive"
  },
  {
    "objectID": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "href": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "title": "Introduction to Spatial Data",
    "section": "Spatial data as a stochastic process",
    "text": "Spatial data as a stochastic process\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\nThere is some attribute (\\(Z(\\mathbf{s})\\)) that we observe at a location (\\(s\\)). That location (\\(s\\)) is an element of a domain of data (\\(D\\)), which is a subset of real coordinate numbers (\\(\\mathbb{R}^d\\), \\(d\\) = 2).\nThree types of spatial data are defined by the differences in domain (\\(D\\))."
  },
  {
    "objectID": "slides/03-slides.html#areal-data",
    "href": "slides/03-slides.html#areal-data",
    "title": "Introduction to Spatial Data",
    "section": "Areal Data",
    "text": "Areal Data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\\(D\\) is fixed domain of countable units\nTypically involve some aggregation"
  },
  {
    "objectID": "slides/03-slides.html#geostatistical-data",
    "href": "slides/03-slides.html#geostatistical-data",
    "title": "Introduction to Spatial Data",
    "section": "Geostatistical data",
    "text": "Geostatistical data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\n\nMitzi Morris\n\n\n\n\n\\(D\\) is a fixed subset of \\(\\mathbb{R}^d\\)\n\\(Z(\\mathbf{s})\\) could be observed at any location within \\(D\\).\nModels predict unobserved locations"
  },
  {
    "objectID": "slides/03-slides.html#point-patterns",
    "href": "slides/03-slides.html#point-patterns",
    "title": "Introduction to Spatial Data",
    "section": "Point patterns",
    "text": "Point patterns\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\\(D\\) is random; where \\(\\mathbf{s}\\) depicts the location of events\n\n\n\n\nBen-Said, M. Ecol Process 10, 56 (2021)."
  },
  {
    "objectID": "slides/03-slides.html#what-is-a-data-model",
    "href": "slides/03-slides.html#what-is-a-data-model",
    "title": "Introduction to Spatial Data",
    "section": "What is a data model?",
    "text": "What is a data model?\n\n\nData: a collection of discrete values that describe phenomena\nYour brain stores millions of pieces of data\nComputers are not your brain\n\nNeed to organize data systematically\nBe able to display and access efficiently\nNeed to be able to store and access repeatedly\n\nData models solve this problem"
  },
  {
    "objectID": "slides/03-slides.html#types-of-spatial-data-models",
    "href": "slides/03-slides.html#types-of-spatial-data-models",
    "title": "Introduction to Spatial Data",
    "section": "2 Types of Spatial Data Models",
    "text": "2 Types of Spatial Data Models\n\nRaster: grid-cell tessellation of an area. Each raster describes the value of a single phenomenon. More next week…\nVector: (many) attributes associated with locations defined by coordinates"
  },
  {
    "objectID": "slides/03-slides.html#the-vector-data-model",
    "href": "slides/03-slides.html#the-vector-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nVertices (i.e., discrete x-y locations) define the shape of the vector\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons\n\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#vectors-in-action",
    "href": "slides/03-slides.html#vectors-in-action",
    "title": "Introduction to Spatial Data",
    "section": "Vectors in Action",
    "text": "Vectors in Action\n\n\nUseful for locations with discrete, well-defined boundaries\nVery precise (not necessarily accurate)\n\n\n\nImage Source: QGIS User’s manual"
  },
  {
    "objectID": "slides/03-slides.html#vector-challenge",
    "href": "slides/03-slides.html#vector-challenge",
    "title": "Introduction to Spatial Data",
    "section": "Vector Challenge!",
    "text": "Vector Challenge!\nThe plot below includes examples of two of the three types of vector objects. Which ones are they?\n\nData Carpentry: Geospatial Concepts"
  },
  {
    "objectID": "slides/03-slides.html#the-raster-data-model",
    "href": "slides/03-slides.html#the-raster-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Raster Data Model",
    "text": "The Raster Data Model\n\n\n\n\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data",
    "href": "slides/03-slides.html#types-of-raster-data",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular: constant cell size; axes aligned with Easting and Northing\nRotated: constant cell size; axes not aligned with Easting and Northing\nSheared: constant cell size; axes not perpendicular\nRectilinear: cell size varies along a dimension\nCurvilinear: cell size and orientation dependent on the other dimension"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data-1",
    "href": "slides/03-slides.html#types-of-raster-data-1",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\nContinuous: numeric data representing a measurement (e.g., elevation, precipitation)\nCategorical: integer data representing factors (e.g., land use, land cover)"
  },
  {
    "objectID": "slides/03-slides.html#location-vs.-place",
    "href": "slides/03-slides.html#location-vs.-place",
    "title": "Introduction to Spatial Data",
    "section": "Location vs. Place",
    "text": "Location vs. Place\n\n\n\n\n\nPlace: an area having unique physical and human characteristics interconnected with other places\nLocation: the actual position on the earth’s surface\nSense of Place: the emotions someone attaches to an area based on experiences\nPlace is location plus meaning\n\n\n\n\n\n\nnominal: (potentially contested) place names\nabsolute: the physical location on the earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#describing-absolute-locations",
    "href": "slides/03-slides.html#describing-absolute-locations",
    "title": "Introduction to Spatial Data",
    "section": "Describing Absolute Locations",
    "text": "Describing Absolute Locations\n\nCoordinates: 2 or more measurements that specify location relative to a reference system\n\n\n\n\n\nCartesian coordinate system\norigin (O) = the point at which both measurement systems intersect\nAdaptable to multiple dimensions (e.g. z for altitude)\n\n\n\n\n\n\nCartesian Coordinate System"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe",
    "href": "slides/03-slides.html#locations-on-a-globe",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\n\n\nLatitude and Longitude"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe-1",
    "href": "slides/03-slides.html#locations-on-a-globe-1",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\nGlobal Reference Systems (GRS)\nGraticule: the grid formed by the intersection of longitude and latitude\nThe graticule is based on an ellipsoid model of earth’s surface and contained in the datum"
  },
  {
    "objectID": "slides/03-slides.html#global-reference-systems",
    "href": "slides/03-slides.html#global-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems\n\nThe datum describes which ellipsoid to use and the precise relations between locations on earth’s surface and Cartesian coordinates\n\n\nGeodetic datums (e.g., WGS84): distance from earth’s center of gravity\nLocal data (e.g., NAD83): better models for local variation in earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#global-reference-systems-1",
    "href": "slides/03-slides.html#global-reference-systems-1",
    "title": "Introduction to Spatial Data",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-extent",
    "href": "slides/03-slides.html#describing-location-extent",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: extent",
    "text": "Describing location: extent\n\n\nHow much of the world does the data cover?\nFor rasters, these are the corners of the lattice\nFor vectors, we call this the bounding box"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-resolution",
    "href": "slides/03-slides.html#describing-location-resolution",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: resolution",
    "text": "Describing location: resolution\n\n\n\n\nResolution: the accuracy that the location and shape of a map’s features can be depicted\nMinimum Mapping Unit: The minimum size and dimensions that can be reliably represented at a given map scale.\nMap scale vs. scale of analysis"
  },
  {
    "objectID": "slides/03-slides.html#projections",
    "href": "slides/03-slides.html#projections",
    "title": "Introduction to Spatial Data",
    "section": "Projections",
    "text": "Projections\n\n\n\n\nBut maps, screens, and publications are…\nProjections describe how the data should be translated to a flat surface\nRely on ‘developable surfaces’\nDescribed by the Coordinate Reference System (CRS)\n\n\n\n\n\n\nDevelopable Surfaces\n\n\n\n\nProjection necessarily induces some form of distortion (tearing, compression, or shearing)"
  },
  {
    "objectID": "slides/03-slides.html#coordinate-reference-systems",
    "href": "slides/03-slides.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nSome projections minimize distortion of angle, area, or distance\nOthers attempt to avoid extreme distortion of any kind\nIncludes: Datum, ellipsoid, units, and other information (e.g., False Easting, Central Meridian) to further map the projection to the GCS\nNot all projections have/require all of the parameters"
  },
  {
    "objectID": "slides/03-slides.html#the-orange-peel-analogy",
    "href": "slides/03-slides.html#the-orange-peel-analogy",
    "title": "Introduction to Spatial Data",
    "section": "The Orange Peel Analogy",
    "text": "The Orange Peel Analogy\n\n\nA datum is the choice of fruit to use. Is the earth an orange, a lemon, a lime, a grapefruit?\n\n\nA projection is how you peel your orange and then flatten the peel.\n\n\n\nSource: Data Carpentry: Geospatial Concepts"
  },
  {
    "objectID": "slides/03-slides.html#choosing-projections",
    "href": "slides/03-slides.html#choosing-projections",
    "title": "Introduction to Spatial Data",
    "section": "Choosing Projections",
    "text": "Choosing Projections\n\n\n\n\n\nEqual-area for thematic maps\nConformal for presentations\nMercator or equidistant for navigation and distance"
  },
  {
    "objectID": "slides/03-slides.html#geometries",
    "href": "slides/03-slides.html#geometries",
    "title": "Introduction to Spatial Data",
    "section": "Geometries",
    "text": "Geometries\n\n\n\nVectors store and aggregate the locations of a feature into a geometry\nMost vector operations require simple, valid geometries\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#valid-geometries",
    "href": "slides/03-slides.html#valid-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Valid Geometries",
    "text": "Valid Geometries\n\nA linestring is simple if it does not intersect\nValid polygons:\n\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line) - For multipolygons, adjacent polygons touch only at points\nDo not repeat their own path"
  },
  {
    "objectID": "slides/03-slides.html#empty-geometries",
    "href": "slides/03-slides.html#empty-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/03-slides.html#support",
    "href": "slides/03-slides.html#support",
    "title": "Introduction to Spatial Data",
    "section": "Support",
    "text": "Support\n\nSupport is the area to which an attribute applies.\n\n\n\nFor vectors, the attribute-geometry relationship can be:\nconstant = applies to every point in the geometry (lines and polygons are just lots of points)\nidentity = a value unique to a geometry\naggregate = a single value that integrates data across the geometry\nRasters can have point (attribute refers to the cell center) or cell (attribute refers to an area similar to the pixel) support"
  },
  {
    "objectID": "slides/03-slides.html#types-of-support-for-vectors",
    "href": "slides/03-slides.html#types-of-support-for-vectors",
    "title": "Introduction to Spatial Data",
    "section": "Types of support for vectors",
    "text": "Types of support for vectors\n\n\nGive an example of:\n\nconstant support\nidentity support\naggregate support"
  },
  {
    "objectID": "slides/03-slides.html#spatial-messiness",
    "href": "slides/03-slides.html#spatial-messiness",
    "title": "Introduction to Spatial Data",
    "section": "Spatial Messiness",
    "text": "Spatial Messiness\n\nQuantitative geography requires that our data are aligned\nAchieving alignment is part of reproducible workflows\nMaking principled decisions about projections, resolution, extent, etc"
  },
  {
    "objectID": "assignment/06-vectoropssolutions.html",
    "href": "assignment/06-vectoropssolutions.html",
    "title": "Assignment 6 Solutions: Vector Operations",
    "section": "",
    "text": "We want to begin to assess the role of distance from schools in determining the education outcomes for Idahoans. We’ll use the landmarks_pnw.csv and cejst_pnw.shp datasets as the basis for this assignment. You’ll need to load the csv and convert it to an sf object. We want to compare the percentage of individuals age 25 or over with less than a high school degree (HSEF in the cejst dataset) for of counties within 50km of a school (MTFCC == K2543) to those that are more than 50km. \nYou’ll need to follow many of the same operations in the video example from class. Your assignment is:\n1. Write out the pseudocode for your analysis\n\nWe’ll need to do a few things here including load the data, find the tracts within 50km of a school, and then compare the cejst results. Breaking that into pseudocode would look like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Find tracts within 50km\n6. Make Maps\n\n\nNote that my fifth step (find tracts within 50km) is a little vague. There are lots of ways I could do this. It might be more helpful to add some specificity here like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Buffer schools by 50km\n6. Select tracts within the buffer and attribute\n7. Make Maps\n\n\nThere are other ways to do this too (like calculating the distance), but those are likely to be more computationally intensive so I’ll leave it at this.\n\n2. Translate the pseudocode into code chunks and create the necessary code (You’ll need to use things like st_distance, st_buffer, st_sym_difference)\n\nLoading the data should be pretty straightforward for you by now. We use read_sf for the shapefile and read_csv for the landmarks.csv. We then filter the data here so that we aren’t working with the entire landmarks dataset.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\n\ncejst.pnw &lt;- read_sf(\"/opt/data/data/assignment06/cejst_pnw.shp\")\nlandmarks.pnw &lt;- read_csv(\"/opt/data/data/assignment06/landmarks_pnw.csv\") %&gt;% \n  filter(., MTFCC == \"K2543\")\n\n\nWe know that one of the datasets are still in long/lat form so we’ll need to make it a sf object before checking the geometry makes any sense. We’ll also assign the crs here by adding it to the st_as_sf call. We also need to make sure that there aren’t any empty geometries as that will cause problems for mapping later.\n\n\nlandmarks.sf &lt;- landmarks.pnw %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"), crs=4269)\nall(st_is_valid(cejst.pnw))\n\n[1] TRUE\n\nall(st_is_valid(landmarks.sf))\n\n[1] TRUE\n\nany(st_is_empty(cejst.pnw))\n\n[1] TRUE\n\nany(st_is_empty(landmarks.sf))\n\n[1] FALSE\n\n\n\nLooks like all the geometries are valid, but there are some empty geometries in the cejst dataset. We will drop those by using filter combined with the negation operator (!) and st_is_empty to return the rows where st_is_empty is not equal to TRUE. Then we can move forward with making sure the two datasets are aligned by using st_transform to change the CRS. We can verify the alignment using a simple call to the plot function.\n\n\ncejst.pnw &lt;- cejst.pnw %&gt;% \n  filter(., !st_is_empty(.))\nlandmarks.proj &lt;- landmarks.sf %&gt;% \n  st_transform(., crs=st_crs(cejst.pnw))\nplot(st_geometry(cejst.pnw))\nplot(st_geometry(landmarks.proj), add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n\nNow it’s time to find the tracts that are within 50km of a school. We can do this a few ways. First, we’ll use the st_buffer approach. We can also calculate the distance matrix between the schools and the tracts using st_distance. This adds a little more complexity as we have to then find the values that are greater than 50km. You’ll notice that st_distance returns a matrix with a row for each school and a column for each tract. This is a little clumsier to deal with, but more precise than a simple buffer.\n\n\nschool.buf &lt;- landmarks.proj %&gt;% \n  st_buffer(., dist=50000) \n\nschool.dist &lt;- st_distance(landmarks.proj, cejst.pnw)\ndim(school.dist)\n\n[1]  220 2571\n\n\n\nOnce we have the buffered “footprint” of the school we can use st_filter (which filters using topological relations) combined with the st_covered_by predicate to find all of the cejst.pnw tracts that are covered by the buffer. Notice that if we use the typical [] subset we get over 200 more records. This is because the latter takes all tracts with an intersection (rather than using our covered by criteria.). We can alter this to achieve the same result as the st_filter by adding the op= argument. Using the distance to the points themselves can be a more conservative way of calculating this, but takes a little more work. First we have to get a list of all of the tracts that fall within 50km (using st_is_within_distance), then identify which of those list elements are empty (i.e., no schools are within 50km), then set that as an index to subset our cejst data.\n\n\nschool.tracts.stf &lt;- cejst.pnw %&gt;% \n  st_filter(x =., y = school.buf, .predicate = st_covered_by)\nschool.tracts.sbst &lt;- cejst.pnw[school.buf,]\nschool.tracts.sbst2 &lt;- cejst.pnw[school.buf,, op=st_covered_by]\n\nnrow(school.tracts.stf)\n\n[1] 2285\n\nnrow(school.tracts.sbst)\n\n[1] 2512\n\nnrow(school.tracts.sbst2)\n\n[1] 2285\n\nidentical(school.tracts.stf, school.tracts.sbst2)\n\n[1] TRUE\n\nwithin50 &lt;- st_is_within_distance(cejst.pnw, landmarks.proj, dist=50000, sparse = TRUE)\nwithin50.idx &lt;- lengths(within50) &gt; 0\nschool.tracts.sbst3 &lt;- cejst.pnw[within50.idx,]\nnrow(school.tracts.sbst3)\n\n[1] 2511\n\n\n3. Make a map for both the percentage of individuals with less than a high school degree in counties within 50km and beyond 50km (i.e. make 2 maps)\n\nWe now have the full cejst dataset and a dataset that is subsetted to the tracts within 50km of a school. Plotting the HSEF values for the tracts within 50km of a school is easy enough. Just map a layer that contains all of the tracts and set it’s color to gray. Then layer the subsetted features on top. Plotting the values of HSEF for the tracts beyond 50km is a little trickier (because we haven’t created that dataset yet). We can use the index we created in the previous step to do that here. We can also use the mutate function to create an indicator variable using our index and then create a “small multiples” style map that plots the two side by side. We’ll learn more about “prettying” up these maps in the later parts of the course.\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.tracts.sbst3) +\n  tm_fill(col=\"HSEF\")\n\n\n\n\n\n\n\nnoschool.tracts &lt;- cejst.pnw[!within50.idx,]\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(noschool.tracts) +\n  tm_fill(col=\"HSEF\")\n\n\n\n\n\n\n\nschool.combined &lt;- cejst.pnw %&gt;% \n  mutate(., indist = if_else(lengths(within50) &gt; 0, \"within50km\", \"notWithin50km\"))\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.combined) +\n  tm_fill(col=\"HSEF\") +\n  tm_facets(by = c(\"indist\"), nrow = 1)"
  },
  {
    "objectID": "example/session-06-example.html",
    "href": "example/session-06-example.html",
    "title": "Session 6 Live Code",
    "section": "",
    "text": "This code and a few more examples can also be found on the session 6 slides.\n\nAccess vector data:\n\n\nCode\nvector.data &lt;- sf::st_read(\"/opt/data/data/assignment01/cejst_nw.shp\")\n\n\n\n\nCheck the CRS:\n\ninput\nproj4string\nwkt\n\n\n\nCode\nsf::st_crs(vector.data)$input\n\n\n[1] \"WGS 84\"\n\n\nCode\nsf::st_crs(vector.data)$proj4string\n\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nCode\nsf::st_crs(vector.data)$wkt\n\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\n\n\nRead in raster data:\n\n\nCode\nraster.data &lt;- terra::rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\n\n\n\nCheck raster CRS:\n\n\nCode\nterra::crs(raster.data, describe=TRUE, proj=TRUE)\n\n\n     name authority code area         extent\n1 unnamed      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n                                                                                                 proj\n1 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\n\n\n\n\nGuess the CRS:\n\n\nCode\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\n#library(mapview)\nlocations &lt;- data.frame(\n  X = c(1200822.97857801, 1205015.51644983, 1202297.44383987, 1205877.68696743, \n        1194763.21511923, 1195463.42403192, 1199836.01037452, 1207081.96500368, \n        1201924.15986897),\n  Y = c(1246476.31475063, 1248612.72571423, 1241479.45996392, 1243898.58428024, \n        1246033.7550009, 1241827.7730307, 1234691.50899912, 1251125.67808482, \n        1252188.4333016),\n  id = 1:9\n)\n\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"X\", \"Y\"))\n\n\nWe stopped this example short because of confusion with the mapview library. Feel free to try the code from the slides on your own time!\n\n\nPlot vector data:\n\n\nCode\nplot(st_geometry(vector.data))\n\n\n\n\n\n\n\n\n\n\n\nRe-project a vector:\n\n\nCode\nvector.data.proj &lt;- vector.data %&gt;%\n  st_transform(., crs = 3083)\n\nst_crs(vector.data.proj)$input\n\n\n[1] \"EPSG:3083\"\n\n\nCode\nplot(st_geometry(vector.data.proj))\n\n\n\n\n\n\n\n\n\n\n\nPlot a raster:\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nplot(raster.data)\n\n\n\n\n\n\n\n\n\n\n\nChanging the CRS of a raster:\n\n\nCode\nraster.data.proj &lt;- project(raster.data, \"epsg:3083\")\n\ncrs(raster.data.proj, describe=TRUE)\n\n\n                                     name authority code\n1 NAD83 / Texas Centric Albers Equal Area      EPSG 3083\n                         area                        extent\n1 United States (USA) - Texas -106.66, -93.50, 25.83, 36.50\n\n\nCode\nplot(raster.data.proj)\n\n\n\n\n\n\n\n\n\n\n\nProject based on another dataset:\n\n\nCode\nvector.data.proj.raster &lt;- vector.data %&gt;%\n  st_transform(., crs = crs(raster.data))\n\nst_crs(vector.data.proj.raster)$input\n\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\n\n\nManually create polygon:\n\n\nCode\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\ncoords = list(outer, hole1, hole2)\npl1 = st_polygon(coords)\n\n\n\n\nCheck polygon validity\n\n\nCode\nst_is_valid(vector.data)\n\n\n   [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [57] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [71] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [85] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [99] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [113] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [127] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [141] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [155] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [169] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [183] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [197] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [225] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [239] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [253] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [267] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [281] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [295] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [309] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [323] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [337] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [351] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [365] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [379] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [393] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [407] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [435] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [449] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [463] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [477] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [491] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [505] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [519] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [533] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [547] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [561] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [575] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [589] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [603] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [617] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [631] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [645] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [659] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [673] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [687] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [701] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [715] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [729] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [743] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [757] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [771] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [785] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [799] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [813] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [827] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [841] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [855] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [869] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [883] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [897] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [911] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [925] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [939] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [953] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [967] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [981] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [995] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1009] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1023] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1037] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1051] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1065] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1079] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1093] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1107] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1135] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1149] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1163] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1177] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1191] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1205] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1219] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1233] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1247] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1261] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1275] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1289] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1303] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1317] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1345] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1359] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1373] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1387] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1401] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1415] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1429] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1443] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1457] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1471] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1485] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1499] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1513] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1527] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1541] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1555] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1569] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1583] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1597] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1611] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1625] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1639] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1653] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1667] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1681] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1695] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1709] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1723] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1737] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1751] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1765] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1779] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1793] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1807] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1821] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1835] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1849] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1863] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1877] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1891] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1905] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1919] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1933] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1947] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1961] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1975] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1989] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2003] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2017] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2031] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2045] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2059] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2073] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2087] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2101] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2115] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2129] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2143] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2157] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2171] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2185] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2199] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2213] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2227] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2255] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2269] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2283] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2297] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2311] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2325] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2339] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2353] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2367] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2381] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2395] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2409] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2423] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2437] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2465] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2479] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2493] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2507] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2521] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2535] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2549] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2563] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[2577] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nCode\ntable(st_is_valid(vector.data))\n\n\n\nTRUE \n2590",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "CRS and Geometries"
    ]
  },
  {
    "objectID": "lesson/quarto.html",
    "href": "lesson/quarto.html",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "lesson/quarto.html#quarto",
    "href": "lesson/quarto.html#quarto",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "lesson/quarto.html#the-example",
    "href": "lesson/quarto.html#the-example",
    "title": "Quarto and literate programming",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.",
    "crumbs": [
      "Lessons",
      "Getting started",
      "Quarto"
    ]
  },
  {
    "objectID": "slides/08-slides.html#revisiting-the-raster-data-model",
    "href": "slides/08-slides.html#revisiting-the-raster-data-model",
    "title": "Areal Data: Rasters",
    "section": "Revisiting the Raster Data Model",
    "text": "Revisiting the Raster Data Model\n\n\n\n\nVector data describe the “exact” locations of features on a landscape (including a Cartesian landscape)\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\n\nOperations mimic those for matrix objects in R\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/08-slides.html#rasters-with-terra",
    "href": "slides/08-slides.html#rasters-with-terra",
    "title": "Areal Data: Rasters",
    "section": "Rasters with terra",
    "text": "Rasters with terra\n\nsyntax is different for terra compared to sf\nRepresentation in Environment is also different\nCan break pipes, Be Explicit"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-1",
    "href": "slides/08-slides.html#rasters-by-construction-1",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction",
    "text": "Rasters by Construction\n\n\n\nmtx &lt;- matrix(1:16, nrow=4)\nmtx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nrstr &lt;- terra::rast(mtx)\nrstr\n\nclass       : SpatRaster \ndimensions  : 4, 4, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 4, 0, 4  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     1 \nmax value   :    16 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: you must have terra loaded for plot() to work on Rast* objects; otherwise you get Error in as.double(y) : cannot coerce type 'S4' to vector of type 'double'"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-origin",
    "href": "slides/08-slides.html#rasters-by-construction-origin",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Origin",
    "text": "Rasters by Construction: Origin\n\nOrigin defines the location of the intersection of the x and y axes\n\n\n\n\nIdeally, the origin is the cell “corner” closest to c(0, 0)\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, ncols=10)\nr[] &lt;- runif(ncell(r))\norigin(r)\n\n[1] 0.05 0.00\n\nr2 &lt;- r\norigin(r2) &lt;- c(2,2)"
  },
  {
    "objectID": "slides/08-slides.html#rasters-by-construction-resolution",
    "href": "slides/08-slides.html#rasters-by-construction-resolution",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Resolution",
    "text": "Rasters by Construction: Resolution\n\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size\nResolution (res) defines the length and width of an individual pixel\n\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          ncols=10)\nres(r)\n\n[1] 1.35 1.00\n\nr2 &lt;- rast(xmin=-4, xmax = 5, \n           ncols=10)\nres(r2)\n\n[1] 0.9 1.0\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          res=c(0.5,0.5))\nncol(r)\n\n[1] 27\n\nr2 &lt;- rast(xmin=-4, xmax = 9.5, \n           res=c(5,5))\nncol(r2)\n\n[1] 3"
  },
  {
    "objectID": "slides/08-slides.html#extending-predicates",
    "href": "slides/08-slides.html#extending-predicates",
    "title": "Areal Data: Rasters",
    "section": "Extending predicates",
    "text": "Extending predicates\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nterra does not follow the same hierarchy as sf so a little trickier"
  },
  {
    "objectID": "slides/08-slides.html#unary-predicates-in-terra",
    "href": "slides/08-slides.html#unary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\nCan tell us qualities of a raster dataset\nMany similar operations for SpatVector class (note use of .)\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis.lonlat\nDoes the object have a longitude/latitude CRS?\n\n\ninMemory\nis the object stored in memory?\n\n\nis.factor\nAre there categorical layers?\n\n\nhasValues\nDo the cells have values?"
  },
  {
    "objectID": "slides/08-slides.html#unary-predicates-in-terra-1",
    "href": "slides/08-slides.html#unary-predicates-in-terra-1",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\n\n\n\nglobal: tests if the raster covers all longitudes (from -180 to 180 degrees) such that the extreme columns are in fact adjacent\n\n\nr &lt;- rast()\nis.lonlat(r)\n\n[1] TRUE\n\nis.lonlat(r, global=TRUE)\n\n[1] TRUE\n\n\n\n\n\n\nperhaps: If TRUE and the crs is unknown, the method returns TRUE if the coordinates are plausible for longitude/latitude\n\n\ncrs(r) &lt;- \"\"\nis.lonlat(r)\n\n[1] NA\n\nis.lonlat(r, perhaps=TRUE, warn=FALSE)\n\n[1] TRUE\n\n\n\ncrs(r) &lt;- \"+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84\"\nis.lonlat(r)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/08-slides.html#binary-predicates-in-terra",
    "href": "slides/08-slides.html#binary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary predicates in terra",
    "text": "Binary predicates in terra\n\nTake exactly 2 inputs, return 1 matrix of cell locs where value is TRUE\nadjacent: identifies cells adajcent to a set of raster cells"
  },
  {
    "objectID": "slides/08-slides.html#unary-measures-in-terra",
    "href": "slides/08-slides.html#unary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary measures in terra",
    "text": "Unary measures in terra\n\nSlightly more flexible than sf\nOne result for each layer in a stack\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ncellSize\narea of individual cells\n\n\nexpanse\nsummed area of all cells\n\n\nvalues\nreturns all cell values\n\n\nncol\nnumber of columns\n\n\nnrow\nnumber of rows\n\n\nncell\nnumber of cells\n\n\nres\nresolution\n\n\next\nminimum and maximum of x and y coords\n\n\norigin\nthe orgin of a SpatRaster\n\n\ncrs\nthe coordinate reference system\n\n\ncats\ncategories of a categorical raster"
  },
  {
    "objectID": "slides/08-slides.html#binary-measures-in-terra",
    "href": "slides/08-slides.html#binary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary measures in terra",
    "text": "Binary measures in terra\n\nReturns a matrix or SpatRaster describing the measure\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndistance\nshortest distance to non-NA or vector object\n\n\ngridDistance\nshortest distance through adjacent grid cells\n\n\ncostDist\nShortest distance considering cell-varying friction\n\n\ndirection\nazimuth to cells that are not NA"
  },
  {
    "objectID": "slides/08-slides.html#extra-practice",
    "href": "slides/08-slides.html#extra-practice",
    "title": "Areal Data: Rasters",
    "section": "Extra Practice",
    "text": "Extra Practice\n\nRun the examples for costDist and gridDistance to see how those functions can be used.\nLoad the wildfire_hazard_agg.tif data from the assignment03 folder. Use the data as the input for the distance function and plot the result. How might this be useful in your research?"
  },
  {
    "objectID": "example/session-13-example.html",
    "href": "example/session-13-example.html",
    "title": "Session 13 Code",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(terra)\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)\n\n\n\n\nCode\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")\n\n\n\n\nCode\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats\n\n\nWarning: [set.cats] setting categories like this is deprecated; use a\ntwo-column data.frame instead\n\n\nReassigning the levels like this shows a warning that this method is deprecated. This is the two column data.frame method it prefers:\n\n\nCode\ntempcats2 &lt;- data.frame(value = c(0, 1, 2),\n                        category = c(\"cold\", \"mild\", \"warm\"))\n\nlevels(temp_reclass) &lt;- tempcats2",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#code-from-slides",
    "href": "example/session-13-example.html#code-from-slides",
    "title": "Session 13 Code",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(terra)\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)\n\n\n\n\nCode\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")\n\n\n\n\nCode\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats\n\n\nWarning: [set.cats] setting categories like this is deprecated; use a\ntwo-column data.frame instead\n\n\nReassigning the levels like this shows a warning that this method is deprecated. This is the two column data.frame method it prefers:\n\n\nCode\ntempcats2 &lt;- data.frame(value = c(0, 1, 2),\n                        category = c(\"cold\", \"mild\", \"warm\"))\n\nlevels(temp_reclass) &lt;- tempcats2",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#hillshade",
    "href": "example/session-13-example.html#hillshade",
    "title": "Session 13 Code",
    "section": "Hillshade",
    "text": "Hillshade\n\n\nCode\nsrtm.slope &lt;- terrain(srtm, \"slope\", unit=\"radians\")\nsrtm.aspect &lt;- terrain(srtm, \"aspect\", unit=\"radians\")\n\nsrtm.shade &lt;- shade(srtm.slope, srtm.aspect)\n\n\n\n\nCode\nplot(srtm.shade, col=grey(0:100/100), legend=FALSE)\nplot(srtm, col=rainbow(25, alpha=0.35), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "example/session-13-example.html#practice",
    "href": "example/session-13-example.html#practice",
    "title": "Session 13 Code",
    "section": "Practice:",
    "text": "Practice:\n\nLoad wildfire risk data\n\n\nCode\nwildfire.risk &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\n\nplot(wildfire.risk)\n\n\n\n\nGet boundary for county\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nid_counties &lt;- tigris::counties(\"ID\", progress_bar = FALSE)\n\nada.cty &lt;- filter(id_counties, NAME == \"Ada\")\n\nplot(st_geometry(ada.cty))\n\n\n\n\n\n\n\n\n\n\n\nReclassify wildfire data\n\n\nCode\n# Method 1\nrcl &lt;- data.frame(from = c(0,10,30,50,80),\n                  to = c(10,30,50,80,100),\n                  becomes = c(0:4))\n\n# Method 2\nrcl.m &lt;- matrix(c(\n  0, 10, 0,\n  10, 30, 1,\n  30, 50, 2,\n  50, 80, 3,\n  80, 100, 4\n), ncol=3, byrow=TRUE)\n\n# Both methods work for the second argument\nwr_reclass &lt;- classify(wildfire.risk, rcl.m)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\n# Add names\nwr_cats &lt;- data.frame(value = 0:4,\n                      category = c(\"No worries\", \n                                   \"A little bit risky\",\n                                   \"Moderate risk\",\n                                   \"Very risky\",\n                                   \"Don't move here\"))\nlevels(wr_reclass) &lt;- wr_cats\n\nplot(wr_reclass)\n\n\n\n\n\n\n\n\n\n\n\nSmooth categorical raster\n\n\nCode\nwr_sm &lt;- focal(wr_reclass, w=7, fun=\"modal\")\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nplot(wr_sm)\n\n\n\n\n\n\n\n\n\n\n\nMake county-level map\n\n\nCode\n# match CRS\nada_proj &lt;- st_transform(ada.cty, crs(wr_sm))\n\nwr_sm_ada &lt;- crop(wr_sm, ada_proj, mask=TRUE)\n\nplot(wr_sm_ada)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations II"
    ]
  },
  {
    "objectID": "slides/09-slides.html#objectives",
    "href": "slides/09-slides.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "slides/09-slides.html#packages-with-plot-methods",
    "href": "slides/09-slides.html#packages-with-plot-methods",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Packages with plot methods",
    "text": "Packages with plot methods\n\n\n\nOften the fastest way to view data\nUse ?plot to see which packages export a method for the plot function\nOr you can use ?plot.*** to see which classes of objects have plot functions defined"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-sf-objects",
    "href": "slides/09-slides.html#plot-for-sf-objects",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\nCan plot outlines using plot(st_geometry(your.shapefile)) or plot(your.shapefile$geometry)\nPlotting attributes requires “extracting” the attributes (using plot(your.shapefile[\"ATTRIBUTE\"]))\nControlling aesthetics can be challenging\nlayering requires add=TRUE"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-sf-objects-1",
    "href": "slides/09-slides.html#plot-for-sf-objects-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\n\n\nplot(st_geometry(cejst))\n\n\n\n\n\n\n\n\n\n\nplot(cejst[\"EALR_PFS\"])"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-spatrasters",
    "href": "slides/09-slides.html#plot-for-spatrasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data)"
  },
  {
    "objectID": "slides/09-slides.html#plot-for-spatrasters-1",
    "href": "slides/09-slides.html#plot-for-spatrasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))"
  },
  {
    "objectID": "slides/09-slides.html#combining-the-two-with-addtrue",
    "href": "slides/09-slides.html#combining-the-two-with-addtrue",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Combining the two with add=TRUE",
    "text": "Combining the two with add=TRUE\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))\nplot(st_geometry(st_transform(cejst, crs=crs(rast.data))), add=TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#grammar-of-graphics-wilkinson-2005",
    "href": "slides/09-slides.html#grammar-of-graphics-wilkinson-2005",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Grammar of Graphics (Wilkinson 2005)",
    "text": "Grammar of Graphics (Wilkinson 2005)\n\nGrammar: A set of structural rules that help establish the components of a language\nSystem and structure of language consist of syntax and semantics\nGrammar of Graphics: a framework that allows us to concisely describe the components of any graphic\nFollows a layered approach by using defined components to build a visualization\nggplot2 is a formal implementation in R"
  },
  {
    "objectID": "slides/09-slides.html#aesthetics-mapping-data-to-visual-elements",
    "href": "slides/09-slides.html#aesthetics-mapping-data-to-visual-elements",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Aesthetics: Mapping Data to Visual Elements",
    "text": "Aesthetics: Mapping Data to Visual Elements\n\n\n\n\nDefine the systematic conversion of data into elements of the visualization\nAre either categorical or continuous (exclusively)\nExamples include x, y, fill, color, and alpha\n\n\n\n\n\n\nFrom Wilke 2019"
  },
  {
    "objectID": "slides/09-slides.html#scales",
    "href": "slides/09-slides.html#scales",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Scales",
    "text": "Scales\n\nScales map data values to their aesthetics\nMust be a one-to-one relationship; each specific data value should map to only one aesthetic"
  },
  {
    "objectID": "slides/09-slides.html#using-tmap",
    "href": "slides/09-slides.html#using-tmap",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\",\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#using-tmap-1",
    "href": "slides/09-slides.html#using-tmap-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap"
  },
  {
    "objectID": "slides/09-slides.html#changing-aesthetics",
    "href": "slides/09-slides.html#changing-aesthetics",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics\n\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#changing-aesthetics-1",
    "href": "slides/09-slides.html#changing-aesthetics-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics"
  },
  {
    "objectID": "slides/09-slides.html#adding-layers",
    "href": "slides/09-slides.html#adding-layers",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers\nORDER MATTERS\n\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% filter(STUSPS %in% c(\"ID\", \"WA\", \"OR\")) %&gt;% st_transform(., crs = st_crs(cejst))\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(st) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#adding-layers-1",
    "href": "slides/09-slides.html#adding-layers-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers"
  },
  {
    "objectID": "slides/09-slides.html#integrating-rasters",
    "href": "slides/09-slides.html#integrating-rasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters\n\ncejst.proj &lt;- st_transform(cejst, crs=crs(rast.data)) %&gt;% filter(!st_is_empty(.))\nstates.proj &lt;- st_transform(st, crs=crs(rast.data))\npal8 &lt;- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\npt &lt;- tm_shape(rast.data[\"category\"]) +\n  tm_raster(palette = pal8) +\n  tm_shape(cejst.proj) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(states.proj) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/09-slides.html#integrating-rasters-1",
    "href": "slides/09-slides.html#integrating-rasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters"
  },
  {
    "objectID": "slides/09-slides.html#todays-objectives",
    "href": "slides/09-slides.html#todays-objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Today’s Objectives",
    "text": "Today’s Objectives\nYou should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "example/getting-setup.html",
    "href": "example/getting-setup.html",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#lets-git-started",
    "href": "example/getting-setup.html#lets-git-started",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#quarto",
    "href": "example/getting-setup.html#quarto",
    "title": "Getting Setup",
    "section": "Quarto",
    "text": "Quarto\nThis is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "example/getting-setup.html#the-example",
    "href": "example/getting-setup.html#the-example",
    "title": "Getting Setup",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.4.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "slides/22-slides.html#favorability-in-general",
    "href": "slides/22-slides.html#favorability-in-general",
    "title": "Statistical Modelling II",
    "section": "Favorability in General",
    "text": "Favorability in General\n\\[\n\\begin{equation}\nF(\\mathbf{s}) = f(w_1X_1(\\mathbf{s}), w_2X_2(\\mathbf{s}), w_3X_3(\\mathbf{s}), ..., w_mX_m(\\mathbf{s}))\n\\end{equation}\n\\]\n\nLogistic regression treats \\(f(x)\\) as a (generalized) linear function\nAllows for multiple qualitative classes\nEnsures that estimates of \\(F(\\mathbf{s})\\) are [0,1]"
  },
  {
    "objectID": "slides/22-slides.html#key-assumptions-of-logistic-regression",
    "href": "slides/22-slides.html#key-assumptions-of-logistic-regression",
    "title": "Statistical Modelling II",
    "section": "Key assumptions of logistic regression",
    "text": "Key assumptions of logistic regression\n\nDependent variable must be binary\nObservations must be independent (important for spatial analyses)\nPredictors should not be collinear\nPredictors should be linearly related to the log-odds\nSample Size\n\n\n0-1 case is data deficient, so trying to model spatial autocorrelation quickly becomes difficult"
  },
  {
    "objectID": "slides/22-slides.html#beyond-linearity",
    "href": "slides/22-slides.html#beyond-linearity",
    "title": "Statistical Modelling II",
    "section": "Beyond Linearity",
    "text": "Beyond Linearity\n\nLogistic (and other generalized linear models) are relatively interpretable\nProbability theory allows robust inference of effects\nPredictive power can be low\nRelaxing the linearity assumption can help"
  },
  {
    "objectID": "slides/22-slides.html#classification-trees",
    "href": "slides/22-slides.html#classification-trees",
    "title": "Statistical Modelling II",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nUse decision rules to segment the predictor space\nSeries of consecutive decision rules form a ‘tree’\nTerminal nodes (leaves) are the outcome; internal nodes (branches) the splits"
  },
  {
    "objectID": "slides/22-slides.html#classification-trees-1",
    "href": "slides/22-slides.html#classification-trees-1",
    "title": "Statistical Modelling II",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nDivide the predictor space (\\(R\\)) into \\(J\\) non-overlapping regions\nEvery observation in \\(R_j\\) gets the same prediction\nRecursive binary splitting\nPruning and over-fitting\n\n\nDraw a cube dimensional space"
  },
  {
    "objectID": "slides/22-slides.html#an-example",
    "href": "slides/22-slides.html#an-example",
    "title": "Statistical Modelling II",
    "section": "An Example",
    "text": "An Example\nPredictor inputs from the dismo package"
  },
  {
    "objectID": "slides/22-slides.html#an-example-1",
    "href": "slides/22-slides.html#an-example-1",
    "title": "Statistical Modelling II",
    "section": "An Example",
    "text": "An Example\nPredictor inputs from the dismo package\n\nbase.path &lt;- \"/opt/data/data/presabsexample/\" #sets the path to the root directory\n\npres.abs &lt;- st_read(paste0(base.path, \"presenceabsence.shp\"), quiet = TRUE) #read the points with presence absence data\npred.files &lt;- list.files(base.path,pattern='grd$', full.names=TRUE) #get the bioclim data\n\npred.stack &lt;- rast(pred.files) #read into a RasterStack\nnames(pred.stack) &lt;- c(\"MeanAnnTemp\", \"TotalPrecip\", \"PrecipWetQuarter\", \"PrecipDryQuarter\", \"MinTempCold\", \"TempRange\")\nplot(pred.stack)"
  },
  {
    "objectID": "slides/22-slides.html#an-example-2",
    "href": "slides/22-slides.html#an-example-2",
    "title": "Statistical Modelling II",
    "section": "An Example",
    "text": "An Example\nThe sample data\n\n\n\nhead(pres.abs)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -106.75 ymin: 31.25 xmax: -98.75 ymax: 37.75\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs\n  y              geometry\n1 0  POINT (-99.25 35.25)\n2 1  POINT (-98.75 36.25)\n3 1 POINT (-106.75 35.25)\n4 0 POINT (-100.75 31.25)\n5 1  POINT (-99.75 37.75)\n6 1 POINT (-104.25 36.75)"
  },
  {
    "objectID": "slides/22-slides.html#an-example-3",
    "href": "slides/22-slides.html#an-example-3",
    "title": "Statistical Modelling II",
    "section": "An Example",
    "text": "An Example\nBuilding our dataframe\n\npts.df &lt;- terra::extract(pred.stack, vect(pres.abs), df=TRUE)\nhead(pts.df)\n\n  ID MeanAnnTemp TotalPrecip PrecipWetQuarter PrecipDryQuarter MinTempCold\n1  1         155         667              253               71         350\n2  2         147         678              266               66         351\n3  3         123         261              117               40         329\n4  4         181         533              198               69         348\n5  5         127         589              257               48         338\n6  6          83         438              213               38         278\n  TempRange\n1       -45\n2       -58\n3       -64\n4        -5\n5       -81\n6      -107"
  },
  {
    "objectID": "slides/22-slides.html#an-example-4",
    "href": "slides/22-slides.html#an-example-4",
    "title": "Statistical Modelling II",
    "section": "An Example",
    "text": "An Example\nBuilding our dataframe\n\npts.df[,2:7] &lt;- scale(pts.df[,2:7])\nsummary(pts.df)\n\n       ID          MeanAnnTemp       TotalPrecip      PrecipWetQuarter \n Min.   :  1.00   Min.   :-3.3729   Min.   :-1.3377   Min.   :-1.6926  \n 1st Qu.: 25.75   1st Qu.:-0.4594   1st Qu.:-0.7980   1st Qu.:-0.6895  \n Median : 50.50   Median : 0.2282   Median :-0.2373   Median :-0.2224  \n Mean   : 50.50   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 75.25   3rd Qu.: 0.7118   3rd Qu.: 0.7140   3rd Qu.: 0.6508  \n Max.   :100.00   Max.   : 1.4285   Max.   : 2.4843   Max.   : 2.2713  \n PrecipDryQuarter   MinTempCold        TempRange      \n Min.   :-1.0828   Min.   :-3.9919   Min.   :-2.7924  \n 1st Qu.:-0.7013   1st Qu.:-0.0598   1st Qu.:-0.5216  \n Median :-0.3770   Median : 0.3582   Median : 0.2075  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.4290   3rd Qu.: 0.5495   3rd Qu.: 0.6450  \n Max.   : 3.1713   Max.   : 1.1092   Max.   : 2.0407  \n\n\n\nVery important for ML - the larger the range, the more likely it will be a splitting variable"
  },
  {
    "objectID": "slides/22-slides.html#an-example-5",
    "href": "slides/22-slides.html#an-example-5",
    "title": "Statistical Modelling II",
    "section": "An example",
    "text": "An example\n\nFitting the classification tree\n\n\nlibrary(tree)\npts.df &lt;- cbind(pts.df, pres.abs$y)\ncolnames(pts.df)[8] &lt;- \"y\"\npts.df$y &lt;- as.factor(ifelse(pts.df$y == 1, \"Yes\", \"No\"))\ntree.model &lt;- tree(y ~ . , pts.df)\nplot(tree.model)\ntext(tree.model, pretty=0)"
  },
  {
    "objectID": "slides/22-slides.html#an-example-6",
    "href": "slides/22-slides.html#an-example-6",
    "title": "Statistical Modelling II",
    "section": "An example",
    "text": "An example\n\nFitting the classification tree\n\n\nsummary(tree.model)\n\n\nClassification tree:\ntree(formula = y ~ ., data = pts.df)\nVariables actually used in tree construction:\n[1] \"TempRange\"        \"PrecipWetQuarter\" \"ID\"               \"MeanAnnTemp\"     \nNumber of terminal nodes:  8 \nResidual mean deviance:  0.3164 = 29.11 / 92 \nMisclassification error rate: 0.07 = 7 / 100 \n\n\n\nWhat would be the ideal number of terminal nodes in a presence/absence problem?\nResidual mean deviance - hard to interpret with pres/abs because of outcome range\nMisclassification rate: depends on distribution of your data (can the model always guess no and do pretty good?)"
  },
  {
    "objectID": "slides/22-slides.html#benefits-and-drawbacks",
    "href": "slides/22-slides.html#benefits-and-drawbacks",
    "title": "Statistical Modelling II",
    "section": "Benefits and drawbacks",
    "text": "Benefits and drawbacks\n\n\nBenefits\n\nEasy to explain\nLinks to human decision-making\nGraphical displays\nEasy handling of qualitative predictors\n\n\nCosts\n\nLower predictive accuracy than other methods\nNot necessarily robust\n\n\n\nsensitive to first decisions"
  },
  {
    "objectID": "slides/22-slides.html#random-forests",
    "href": "slides/22-slides.html#random-forests",
    "title": "Statistical Modelling II",
    "section": "Random Forests",
    "text": "Random Forests\n\n\n\n\nGrow 100(000s) of trees using bootstrapping\nRandom sample of predictors considered at each split\nAvoids correlation amongst multiple predictions\nAverage of trees improves overall outcome (usually)\nLots of extensions"
  },
  {
    "objectID": "slides/22-slides.html#an-example-7",
    "href": "slides/22-slides.html#an-example-7",
    "title": "Statistical Modelling II",
    "section": "An example",
    "text": "An example\n\nFitting the Random Forest\n\n\nlibrary(randomForest)\nclass.model &lt;- y ~ .\nrf2 &lt;- randomForest(class.model, data=pts.df)\nvarImpPlot(rf2)"
  },
  {
    "objectID": "slides/22-slides.html#the-sampling-situation",
    "href": "slides/22-slides.html#the-sampling-situation",
    "title": "Statistical Modelling II",
    "section": "The sampling situation",
    "text": "The sampling situation\n\n\n\n\n\nFrom Lentz et al. 2008\n\n\n\n\nOpportunistic collection of presences only\nHypothesized predictors of occurrence are measured (or extracted) at each presence\nBackground points (or pseudoabsences) generated for comparison"
  },
  {
    "objectID": "slides/22-slides.html#the-challenge-with-background-points",
    "href": "slides/22-slides.html#the-challenge-with-background-points",
    "title": "Statistical Modelling II",
    "section": "The Challenge with Background Points",
    "text": "The Challenge with Background Points\n\nWhat constitutes background?\nNot measuring probability, but relative likelihood of occurrence\nSampling bias affects estimation\nThe intercept\n\n\\[\n\\begin{equation}\ny_{i} \\sim \\text{Bern}(p_i)\\\\\n\\text{link}(p_i) = \\mathbf{x_i}'\\beta + \\alpha\n\\end{equation}\n\\]\n\nWhat predictors you leave out is just as important as predictors you leave in\nIntercept becomes ratio of background to presence - great debate"
  },
  {
    "objectID": "slides/22-slides.html#maximum-entropy-models",
    "href": "slides/22-slides.html#maximum-entropy-models",
    "title": "Statistical Modelling II",
    "section": "Maximum Entropy models",
    "text": "Maximum Entropy models\n\n\n\n\nMaxEnt (after the original software)\nNeed plausible background points across the remainder of the study area\nIterative fitting to maximize the distance between predictions generated by a spatially uniform model\nTuning parameters to account for differences in sampling effort, placement of background points, etc\nDevelopment of the model beyond the scope of this course, but see Elith et al. 2010\n\n\n\n\n\n\nFrom Elith et al. 2010"
  },
  {
    "objectID": "slides/22-slides.html#challenges-with-maxent",
    "href": "slides/22-slides.html#challenges-with-maxent",
    "title": "Statistical Modelling II",
    "section": "Challenges with MaxEnt",
    "text": "Challenges with MaxEnt\n\nNot measuring probability, but relative likelihood of occurrence\nSampling bias affects estimation (but can be mitigated using tuning parameters)\nTheoretical issues with background points and the intercept\nRecent developments relate MaxEnt (with cloglog links) to Inhomogenous Point Process models"
  },
  {
    "objectID": "slides/22-slides.html#extensions",
    "href": "slides/22-slides.html#extensions",
    "title": "Statistical Modelling II",
    "section": "Extensions",
    "text": "Extensions\n\nPolynomial, splines, piece-wise regression\nNeural nets, Support Vector Machines, many many more"
  },
  {
    "objectID": "slides/22-slides.html#thinking-about-the-data",
    "href": "slides/22-slides.html#thinking-about-the-data",
    "title": "Statistical Modelling II",
    "section": "Thinking about the data",
    "text": "Thinking about the data\n\nDatasets - Forest Service Boundaries, CFLRP Boundaries, Wildfire Risk Raster, CEJST shapefile\nDependent Variable - CFLRP (T or F)\nIndependent Variables - Wildfire hazard, income, education, housing burden"
  },
  {
    "objectID": "slides/22-slides.html#building-some-pseudocode",
    "href": "slides/22-slides.html#building-some-pseudocode",
    "title": "Statistical Modelling II",
    "section": "Building some Pseudocode",
    "text": "Building some Pseudocode\n\n1. Load libraries\n2. Load data\n3. Check validity and alignment\n4. Subset to relevant geographies\n5. Select relevant attributes\n6. Extract wildfire risk\n7. CFLRP T or F\n8. Compare risks"
  },
  {
    "objectID": "slides/22-slides.html#load-libraries",
    "href": "slides/22-slides.html#load-libraries",
    "title": "Statistical Modelling II",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)"
  },
  {
    "objectID": "slides/22-slides.html#load-the-data",
    "href": "slides/22-slides.html#load-the-data",
    "title": "Statistical Modelling II",
    "section": "Load the data",
    "text": "Load the data\n\nDownloading USFS data using the function in the code folder\n\n\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)"
  },
  {
    "objectID": "slides/14-slides.html#objectives",
    "href": "slides/14-slides.html#objectives",
    "title": "Building Spatial Databases with Attributes",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nBegin building a database for spatial analysis"
  },
  {
    "objectID": "slides/14-slides.html#what-is-spatial-analysis-1",
    "href": "slides/14-slides.html#what-is-spatial-analysis-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n“The process of examining the locations, attributes, and relationships of features in spatial data through overlay and other analytical techniques in order to address a question or gain useful knowledge. Spatial analysis extracts or creates new information from spatial data”.\n\n— ESRI Dictionary"
  },
  {
    "objectID": "slides/14-slides.html#what-is-spatial-analysis-2",
    "href": "slides/14-slides.html#what-is-spatial-analysis-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n\n\nThe process of turning maps into information\nAny- or everything we do with GIS\nThe use of computational and statistical algorithms to understand the relations between things that co-occur in space.\n\n\n\n\n\nJohn Snow’s cholera outbreak map"
  },
  {
    "objectID": "slides/14-slides.html#common-goals-for-spatial-analysis",
    "href": "slides/14-slides.html#common-goals-for-spatial-analysis",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common goals for spatial analysis",
    "text": "Common goals for spatial analysis\n\n\n\n\n\ncourtesy of NatureServe\n\n\n\n\nDescribe and visualize locations or events\nQuantify patterns\nCharacterize ‘suitability’\nDetermine (statistical) relations"
  },
  {
    "objectID": "slides/14-slides.html#common-pitfalls-of-spatial-analysis",
    "href": "slides/14-slides.html#common-pitfalls-of-spatial-analysis",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common pitfalls of spatial analysis",
    "text": "Common pitfalls of spatial analysis\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals\n\n\n\nSpatial analysis is an inherently complex endeavor and one that is advancing rapidly. So-called “best practices” for addressing many of these issues are still being developed and debated. This doesn’t mean you shouldn’t do spatial analysis, but you should keep these things in mind as you design, implement, and interpret your analyses"
  },
  {
    "objectID": "slides/14-slides.html#workflows-for-spatial-analysis-1",
    "href": "slides/14-slides.html#workflows-for-spatial-analysis-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Workflows for spatial analysis",
    "text": "Workflows for spatial analysis\n\n\n\nAcquisition (not really a focus, but see Resources)\nGeoprocessing\nAnalysis\nVisualization\n\n\n\n\n\ncourtesy of University of Illinois"
  },
  {
    "objectID": "slides/14-slides.html#geoprocessing",
    "href": "slides/14-slides.html#geoprocessing",
    "title": "Building Spatial Databases with Attributes",
    "section": "Geoprocessing",
    "text": "Geoprocessing\nManipulation of data for subsequent use\n\nAlignment\nData cleaning and transformation\nCombination of multiple datasets\nSelection and subsetting"
  },
  {
    "objectID": "slides/14-slides.html#databases-and-attributes-1",
    "href": "slides/14-slides.html#databases-and-attributes-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Databases and attributes",
    "text": "Databases and attributes\n\n\n\n\n\ncourtesy of Giscommons\n\n\n\n\n\nPrevious focus has been largely on location\nGeographic data often also includes non-spatial data\nAttributes: Non-spatial information that further describes a spatial feature\nTypically stored in tables where each row represents a spatial feature\n\nWide vs. long format"
  },
  {
    "objectID": "slides/14-slides.html#common-attribute-operations",
    "href": "slides/14-slides.html#common-attribute-operations",
    "title": "Building Spatial Databases with Attributes",
    "section": "Common attribute operations",
    "text": "Common attribute operations\n\nsf designed to work with tidyverse\nAllows use of dplyr data manipulation verbs (e.g. filter, select, slice)\nCan use scales package for units\nAlso allows %&gt;% to chain together multiple steps\ngeometries are “sticky”"
  },
  {
    "objectID": "slides/14-slides.html#subsetting-by-field",
    "href": "slides/14-slides.html#subsetting-by-field",
    "title": "Building Spatial Databases with Attributes",
    "section": "Subsetting by Field",
    "text": "Subsetting by Field\n\nFields contain individual attributes\nSelecting fields\n\n\n\n\ncolnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nhead(world)[,1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 6 × 3\n  iso_a2 name_long      continent    \n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;        \n1 FJ     Fiji           Oceania      \n2 TZ     Tanzania       Africa       \n3 EH     Western Sahara Africa       \n4 CA     Canada         North America\n5 US     United States  North America\n6 KZ     Kazakhstan     Asia         \n\n\n\n\nworld %&gt;%\n  dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.) \n\n# A tibble: 6 × 2\n  name_long      continent    \n  &lt;chr&gt;          &lt;chr&gt;        \n1 Fiji           Oceania      \n2 Tanzania       Africa       \n3 Western Sahara Africa       \n4 Canada         North America\n5 United States  North America\n6 Kazakhstan     Asia"
  },
  {
    "objectID": "slides/14-slides.html#subsetting-by-features",
    "href": "slides/14-slides.html#subsetting-by-features",
    "title": "Building Spatial Databases with Attributes",
    "section": "Subsetting by Features",
    "text": "Subsetting by Features\n\nFeatures refer to the individual observations in the dataset\nSelecting features\n\n\n\n\nhead(world)[1:3, 1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 3 × 3\n  iso_a2 name_long      continent\n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;    \n1 FJ     Fiji           Oceania  \n2 TZ     Tanzania       Africa   \n3 EH     Western Sahara Africa   \n\n\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 2\n  name_long   continent\n  &lt;chr&gt;       &lt;chr&gt;    \n1 Kazakhstan  Asia     \n2 Uzbekistan  Asia     \n3 Indonesia   Asia     \n4 Timor-Leste Asia     \n5 Israel      Asia     \n6 Lebanon     Asia"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse",
    "href": "slides/14-slides.html#revisiting-the-tidyverse",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent, pop, gdpPercap ,area_km2) %&gt;%\n  mutate(., dens = pop/area_km2,\n         totGDP = gdpPercap * pop) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 7\n  name_long   continent       pop gdpPercap area_km2   dens  totGDP\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Kazakhstan  Asia       17288285    23587. 2729811.   6.33 4.08e11\n2 Uzbekistan  Asia       30757700     5371.  461410.  66.7  1.65e11\n3 Indonesia   Asia      255131116    10003. 1819251. 140.   2.55e12\n4 Timor-Leste Asia        1212814     6263.   14715.  82.4  7.60e 9\n5 Israel      Asia        8215700    31702.   22991. 357.   2.60e11\n6 Lebanon     Asia        5603279    13831.   10099. 555.   7.75e10"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse-1",
    "href": "slides/14-slides.html#revisiting-the-tidyverse-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields"
  },
  {
    "objectID": "slides/14-slides.html#revisiting-the-tidyverse-2",
    "href": "slides/14-slides.html#revisiting-the-tidyverse-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\n\n\nAggregating data\n\n\nworld %&gt;%\n  st_drop_geometry(.) %&gt;% \n  group_by(continent) %&gt;%\n  summarize(pop = sum(pop, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  continent                      pop\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811"
  },
  {
    "objectID": "slides/14-slides.html#joining-aspatial-data-1",
    "href": "slides/14-slides.html#joining-aspatial-data-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Joining (a)spatial data",
    "text": "Joining (a)spatial data\n\n\n\nRequires a “key” field\nMultiple outcomes possible\nThink about your final data form"
  },
  {
    "objectID": "slides/14-slides.html#left-join",
    "href": "slides/14-slides.html#left-join",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join\n\nUseful for adding other attributes not in your spatial data\nReturns all of the records in x attributed with y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/14-slides.html#left-join-1",
    "href": "slides/14-slides.html#left-join-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/14-slides.html#left-join-2",
    "href": "slides/14-slides.html#left-join-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join\n\n\n\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\n\n\nworld_coffee = left_join(world, coffee_data)\nnrow(world_coffee)\n\n[1] 177"
  },
  {
    "objectID": "slides/14-slides.html#left-join-3",
    "href": "slides/14-slides.html#left-join-3",
    "title": "Building Spatial Databases with Attributes",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/14-slides.html#inner-join",
    "href": "slides/14-slides.html#inner-join",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join\n\nUseful for subsetting to “complete” records\nReturns all of the records in x with matching y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/14-slides.html#inner-join-1",
    "href": "slides/14-slides.html#inner-join-1",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/14-slides.html#inner-join-2",
    "href": "slides/14-slides.html#inner-join-2",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join\n\n\n\nworld_coffee_inner = inner_join(world, coffee_data)\nnrow(world_coffee_inner)\n\n[1] 45\n\n\n\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\""
  },
  {
    "objectID": "slides/14-slides.html#inner-join-3",
    "href": "slides/14-slides.html#inner-join-3",
    "title": "Building Spatial Databases with Attributes",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/11-slides.html#objectives",
    "href": "slides/11-slides.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data."
  },
  {
    "objectID": "slides/11-slides.html#example-questions",
    "href": "slides/11-slides.html#example-questions",
    "title": "Operations With Vector Data II",
    "section": "Example questions",
    "text": "Example questions\n\nWhat is the chronic heart disease risk of the 10 ID tracts that are furthest from hospitals?\nHow may \\(km^2\\) of ID are served by more than 1 hospital?\nWhat is the difference between the average risk of chronic heart disease in the tracts served by at least two hospitals compared to those that aren’t served by any?"
  },
  {
    "objectID": "slides/11-slides.html#key-assummptions",
    "href": "slides/11-slides.html#key-assummptions",
    "title": "Operations With Vector Data II",
    "section": "Key assummptions",
    "text": "Key assummptions\n\nAll hospital locations are contained in the landmarks dataset\nA hospital service area is defined as a 50km radius\nHospital service areas can cross state lines."
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know",
    "href": "slides/11-slides.html#what-do-we-need-to-know",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nHow far are the hospitals from ID tracts?\nWhich tracts are the furthest?\nWhat is the CHD risk?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode",
    "href": "slides/11-slides.html#pseudocode",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital and cdc datasets\n2. Align the data\n3. Filter cdc so it only has Idaho tracts\n4. Calculate distance from hospitals\n5. Find top 10 tracts based on distance\n6. Map chronic heart disease risk"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions",
    "href": "slides/11-slides.html#adding-functions",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital and cdc datasets\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nhospital.sf &lt;- read_csv(\"../../data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\nst_crs(hospital.sf)\n\nCoordinate Reference System: NA\n\ncdc.sf &lt;- read_sf(\"../../data/2023/vectorexample/cdc_nw.shp\")\nst_crs(cdc.sf)$epsg\n\n[1] 4269"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-1",
    "href": "slides/11-slides.html#adding-functions-1",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nAlign the data\n\n\nst_crs(hospital.sf) &lt;- 4326\n\nhospital.sf.proj &lt;- hospital.sf %&gt;% \n  st_transform(., crs=st_crs(cdc.sf))\n\nst_crs(hospital.sf.proj) == st_crs(cdc.sf)\n\n[1] TRUE\n\nidentical(st_crs(hospital.sf.proj), st_crs(cdc.sf))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-2",
    "href": "slides/11-slides.html#adding-functions-2",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFilter cdc so it only has Idaho tracts\n\n\n\n\ncdc.idaho &lt;- cdc.sf %&gt;% \n  filter(STATEFP == \"16\")\n\n\n\nplot(st_geometry(cdc.idaho))"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-3",
    "href": "slides/11-slides.html#adding-functions-3",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate distance from hospitals\n\n\nnearest.hosp &lt;- st_nearest_feature(cdc.idaho, hospital.sf.proj)\nstr(nearest.hosp)\n\n int [1:191] 6 45 45 45 3 3 3 3 6 3 ...\n\nnearest.hosp.sf &lt;- hospital.sf.proj[nearest.hosp,]\nhospital.dist &lt;- st_distance(cdc.idaho, nearest.hosp.sf, by_element = TRUE)\nstr(hospital.dist)\n\n Units: [m] num [1:191] 29414 46610 39432 32817 23548 ..."
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-4",
    "href": "slides/11-slides.html#adding-functions-4",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind top 10 counties based on distance\n\n\ncdc.idaho.hosp &lt;- cdc.idaho %&gt;% \n  mutate(., disthosp = hospital.dist)\n\ncdc.furthest &lt;- cdc.idaho.hosp %&gt;% \n  slice_max(., n=10, order_by= disthosp)\n\nhead(cdc.furthest$disthosp)\n\nUnits: [m]\n[1] 94622.55 83296.77 80916.73 70646.03 70292.69 69877.25"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-5",
    "href": "slides/11-slides.html#adding-functions-5",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nMap chronic heart disease risk\n\n\nlibrary(tmap)\n\ntm_shape(tigris::counties(\"ID\", progress_bar=FALSE)) +\n  tm_polygons() +\n  tm_shape(cdc.furthest) +\n  tm_polygons(\"disthosp\", title=\"Dist to Hospital (m2)\") +\n  tm_shape(hospital.sf.proj[cdc.idaho,]) +\n  tm_symbols(size=0.25)"
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know-1",
    "href": "slides/11-slides.html#what-do-we-need-to-know-1",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nWhat is the service area for each hospital?\nWhere do those service areas overlap?\nHow big is the overlap area?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode-1",
    "href": "slides/11-slides.html#pseudocode-1",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital dataset and add projection\n2. Buffer hospitals by service area\n3. Find intersection of service areas\n4. Calculate area of overlap"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-6",
    "href": "slides/11-slides.html#adding-functions-6",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital dataset and add projection\n\n\nhospital.sf &lt;- read_csv(\"../../data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\n\nst_crs(hospital.sf) &lt;- 4326"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-7",
    "href": "slides/11-slides.html#adding-functions-7",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\n\n\nBuffer hospitals by service area\n\n\nhospital.buf &lt;- hospital.sf %&gt;%\n  filter(STATEFP == \"16\") %&gt;% \n  st_buffer(., dist = units::set_units(50, \"kilometers\"))\n\n\n\nplot(st_geometry(hospital.buf))"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-8",
    "href": "slides/11-slides.html#adding-functions-8",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind intersection of service areas\n\n\nhospital.int &lt;- hospital.buf %&gt;% \n  st_intersection()\nall(st_is_valid(hospital.int))"
  },
  {
    "objectID": "slides/11-slides.html#troubleshooting-process",
    "href": "slides/11-slides.html#troubleshooting-process",
    "title": "Operations With Vector Data II",
    "section": "Troubleshooting Process",
    "text": "Troubleshooting Process\nGoogling error code with package name and R lead to this issue page: https://github.com/r-spatial/sf/issues/2143\n\nhospital.buf &lt;- hospital.buf %&gt;%\n  # project to planar CRS to get rid of warning\n  st_transform(., crs = 5070) %&gt;%\n  # remove +/- duplicate buffer\n  filter(!row_number() %in% c(7,8))\n  \n\nhospital.int &lt;- hospital.buf %&gt;% \n  st_intersection(.)\nall(st_is_valid(hospital.int))\n\n[1] TRUE\n\nhospital.int.overlaps &lt;- hospital.int %&gt;%\n  filter(n.overlaps &gt; 1)"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-9",
    "href": "slides/11-slides.html#adding-functions-9",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate area of overlap\n\n\noverlap.areas &lt;- st_area(hospital.int.overlaps)\n\narea_m2 &lt;- sum(overlap.areas) + units::set_units(pi*50000^2, m^2)\n\nunits::set_units(area_m2, km^2)\n\n32664.78 [km^2]"
  },
  {
    "objectID": "slides/11-slides.html#what-do-we-need-to-know-2",
    "href": "slides/11-slides.html#what-do-we-need-to-know-2",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?"
  },
  {
    "objectID": "slides/11-slides.html#pseudocode-2",
    "href": "slides/11-slides.html#pseudocode-2",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode"
  },
  {
    "objectID": "slides/11-slides.html#adding-functions-10",
    "href": "slides/11-slides.html#adding-functions-10",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions"
  },
  {
    "objectID": "slides/11-slides.html#plotting-the-results",
    "href": "slides/11-slides.html#plotting-the-results",
    "title": "Operations With Vector Data II",
    "section": "Plotting the Results",
    "text": "Plotting the Results"
  },
  {
    "objectID": "slides/10-slides.html#announcements",
    "href": "slides/10-slides.html#announcements",
    "title": "Operations With Vector Data I",
    "section": "Announcements",
    "text": "Announcements\nDue this week: assignment revision 1"
  },
  {
    "objectID": "slides/10-slides.html#objectives",
    "href": "slides/10-slides.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data"
  },
  {
    "objectID": "slides/10-slides.html#revisiting-predicates-and-measures",
    "href": "slides/10-slides.html#revisiting-predicates-and-measures",
    "title": "Operations With Vector Data I",
    "section": "Revisiting predicates and measures",
    "text": "Revisiting predicates and measures\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nUnary, binary, and n-ary distinguish how many geometries each function accepts and returns"
  },
  {
    "objectID": "slides/10-slides.html#transformations",
    "href": "slides/10-slides.html#transformations",
    "title": "Operations With Vector Data I",
    "section": "Transformations",
    "text": "Transformations\n\n\nTransformations: create new geometries based on input geometries"
  },
  {
    "objectID": "slides/10-slides.html#unary-transformations",
    "href": "slides/10-slides.html#unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Unary Transformations",
    "text": "Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that do no longer cover the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith an arbitrary point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries",
    "href": "slides/10-slides.html#fixing-geometries",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries",
    "text": "Fixing geometries\n\nWhen all(st_is_valid(your.shapefile)) returns FALSE\n\n\n\n\n\nst_make_valid has two methods:\n\noriginal converts rings into noded lines and extracts polygons\nstructured makes rings valid first then merges/subtracts from existing polgyons\nVerify that the output is what you expect!!\n\n\n\n\n\n```{r}\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(0.5,0),c(0.5,0.5),c(0.5,0),c(1,0),c(1,1),c(0,1),c(0,0)))))\nst_is_valid(x)\n```\n\n[1] FALSE"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries-with-st_make_valid",
    "href": "slides/10-slides.html#fixing-geometries-with-st_make_valid",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries with st_make_valid",
    "text": "Fixing geometries with st_make_valid\n\n\n\n\n\n\n\n\n\n\n\n\n\n```{r}\ny &lt;- x %&gt;% st_make_valid()\nst_is_valid(y)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-slides.html#fixing-geometries-with-st_buffer",
    "href": "slides/10-slides.html#fixing-geometries-with-st_buffer",
    "title": "Operations With Vector Data I",
    "section": "Fixing Geometries with st_buffer",
    "text": "Fixing Geometries with st_buffer\n\n\n\nst_buffer enforces valid geometries as an output\nSetting a 0 distance buffer leaves most geometries unchanged\nNot all transformations do this\n\n\n\n```{r}\nz &lt;- x %&gt;% st_buffer(., dist=0)\n\nst_is_valid(z)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-slides.html#changing-crs-with-st_transform",
    "href": "slides/10-slides.html#changing-crs-with-st_transform",
    "title": "Operations With Vector Data I",
    "section": "Changing CRS with st_transform",
    "text": "Changing CRS with st_transform\n\nYou’ve already been using this!!\nDoes not guarantee valid geometries (use check = TRUE if you want this)\nWe’ll try to keep things from getting too complicated"
  },
  {
    "objectID": "slides/10-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "href": "slides/10-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "title": "Operations With Vector Data I",
    "section": "Converting areas to points with st_centroid or st_point_on_surface",
    "text": "Converting areas to points with st_centroid or st_point_on_surface\n\n\n\n\nFor “sampling” other datasets\nTo simplify distance calculations\nTo construct networks\n\n\n\n\nid.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE)\nid.centroid &lt;- st_centroid(id.counties)\nid.pointonsurf &lt;- st_point_on_surface(id.counties)"
  },
  {
    "objectID": "slides/10-slides.html#creating-sampling-areas",
    "href": "slides/10-slides.html#creating-sampling-areas",
    "title": "Operations With Vector Data I",
    "section": "Creating “sampling areas”",
    "text": "Creating “sampling areas”\n\nUncertainty in your point locations\nIncorporate a fixed range around each point\nCombine multiple points into a single polygon\n\n\nhospitals.id &lt;- landmarks.id.csv %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4326"
  },
  {
    "objectID": "slides/10-slides.html#creating-sampling-areas-1",
    "href": "slides/10-slides.html#creating-sampling-areas-1",
    "title": "Operations With Vector Data I",
    "section": "Creating sampling areas",
    "text": "Creating sampling areas\n\nhospital.buf &lt;- hospitals.id %&gt;%\n  st_buffer(., dist=10000)\n\nhospital.mcp &lt;- hospitals.id %&gt;% \n  st_convex_hull(.)"
  },
  {
    "objectID": "slides/10-slides.html#other-unary-transformations",
    "href": "slides/10-slides.html#other-unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Other Unary Transformations",
    "text": "Other Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system\n\n\ntriangulate\nwith Delauney triangulated polygon(s)\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix"
  },
  {
    "objectID": "slides/10-slides.html#binary-transformers-1",
    "href": "slides/10-slides.html#binary-transformers-1",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers\n\n\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n%/%\n\n\ncrop\ncrop an sf object to a specific rectangle"
  },
  {
    "objectID": "slides/10-slides.html#binary-transformers-2",
    "href": "slides/10-slides.html#binary-transformers-2",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers"
  },
  {
    "objectID": "slides/10-slides.html#common-uses-of-binary-transformers",
    "href": "slides/10-slides.html#common-uses-of-binary-transformers",
    "title": "Operations With Vector Data I",
    "section": "Common Uses of Binary Transformers",
    "text": "Common Uses of Binary Transformers\n\nRelating partially overlapping datasets to each other\nReducing the extent of vector objects"
  },
  {
    "objectID": "slides/10-slides.html#n-ary-transformers",
    "href": "slides/10-slides.html#n-ary-transformers",
    "title": "Operations With Vector Data I",
    "section": "N-ary Transformers",
    "text": "N-ary Transformers\n\nSimilar to Binary (except st_crop)\nunion can be applied to a set of geometries to return its geometrical union\nintersection and difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples, etc."
  },
  {
    "objectID": "slides/10-slides.html#centroids-and-distances",
    "href": "slides/10-slides.html#centroids-and-distances",
    "title": "Operations With Vector Data I",
    "section": "Centroids and Distances",
    "text": "Centroids and Distances\nThe function system.time tells you how long a function takes to run:\n\nsystem.time(id.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE))\n\n   user  system elapsed \n   0.84    0.49    1.33 \n\n\nFind the counties that are the furthest distance from each other in Idaho using the polygons, centroids, and point on surface objects we created earlier. Which distance calculation is the fastest?\n(You may want to refer to our session 7 example code.)"
  },
  {
    "objectID": "slides/10-slides.html#intersections-and-buffers",
    "href": "slides/10-slides.html#intersections-and-buffers",
    "title": "Operations With Vector Data I",
    "section": "Intersections and Buffers",
    "text": "Intersections and Buffers\ntigris::primary_secondary_roads() retrieves shapefiles for major roads in each state of the US.\n\nPlot just Ada county and the major roads within.\nMap the portion of major roads that are within 50 km (as the crow flies) of the center of Ada county. (Remember to check the units of your CRS.)\nChallenge: Include county borders in your plot for part 2."
  },
  {
    "objectID": "slides/20-slides.html#spatial-autocorrelation",
    "href": "slides/20-slides.html#spatial-autocorrelation",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\n\n\nAttributes (features) are often non-randomly distributed\nEspecially true with aggregated data\nInterest is in the relationship between proximity and the feature\nDifference from kriging and semivariance\n\n\n\n\n\nFrom Manuel Gimond\n\n\n\n\n\nReminder of semivariance: \\(y = ax + by + \\epsilon\\), error term is variance, semivariance is the way the error changes over space\nKriging models semivariance to adjust interpolation values\n\n4 steps of kriging: 1. Is my process deterministic or stochastic? (deterministic - inverse distance weighting, stochastic - kriging) 2. How can I take the data I have and move it to spaces I don’t have data? 3. Remove broad geographic trends (1st, 2nd order) 4. How much residual autocorrelation is there (semivariance)?\nIt is different to be interested in autocorrelation in your error and autocorrelation in your covariates."
  },
  {
    "objectID": "slides/20-slides.html#morans-i",
    "href": "slides/20-slides.html#morans-i",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Moran’s I",
    "text": "Moran’s I\n\n\n\nMoran’s I\n\n\n\n\n\n\nThree components: difference between values at a location and mean value, weight for each observation, distance - how big are the differences and how much weight do you give to the observations"
  },
  {
    "objectID": "slides/20-slides.html#finding-neighbors---contiguity",
    "href": "slides/20-slides.html#finding-neighbors---contiguity",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Finding Neighbors - Contiguity",
    "text": "Finding Neighbors - Contiguity\n\nHow do we define \\(I(d)\\) for areal data?\nWhat about \\(w_{ij}\\)?\nWe can use spdep for that!!\n\n ::: :::"
  },
  {
    "objectID": "slides/20-slides.html#using-spdep",
    "href": "slides/20-slides.html#using-spdep",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Using spdep",
    "text": "Using spdep\n\ncdc &lt;- read_sf(\"data/opt/data/2023/vectorexample/cdc_nw.shp\") %&gt;% \n  select(stateabbr, countyname, countyfips, casthma_cr)\n\n\n::: :::"
  },
  {
    "objectID": "slides/20-slides.html#finding-neighbors",
    "href": "slides/20-slides.html#finding-neighbors",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Finding Neighbors",
    "text": "Finding Neighbors\n\nQueen, rook, (and bishop) cases impose neighbors by contiguity\nWeights calculated as a \\(1/ num. of neighbors\\)\n\n\nnb.qn &lt;- poly2nb(cdc, queen=TRUE)\nnb.rk &lt;- poly2nb(cdc, queen=FALSE)"
  },
  {
    "objectID": "slides/20-slides.html#finding-neighbors-1",
    "href": "slides/20-slides.html#finding-neighbors-1",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Finding Neighbors",
    "text": "Finding Neighbors"
  },
  {
    "objectID": "slides/20-slides.html#getting-weights-and-distance",
    "href": "slides/20-slides.html#getting-weights-and-distance",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Getting Weights and Distance",
    "text": "Getting Weights and Distance\n\n\n\n# get weights\nlw.qn &lt;- nb2listw(nb.qn, style=\"W\", zero.policy = TRUE)\nlw.qn$weights[1:5]\n\n[[1]]\n[1] 0.5 0.5\n\n[[2]]\n[1] 0.25 0.25 0.25 0.25\n\n[[3]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[4]]\n[1] 0.3333333 0.3333333 0.3333333\n\n[[5]]\n[1] 1\n\n# get average neighboring asthma values\nasthma.lag &lt;- lag.listw(lw.qn, cdc$casthma_cr)\n\n\n\n\n                         asthma.lag        \n[1,] \"Camas\"      \"9.9\"  \"10.3\"            \n[2,] \"Kootenai\"   \"10.4\" \"9.575\"           \n[3,] \"Kootenai\"   \"10\"   \"9.88\"            \n[4,] \"Kootenai\"   \"9.5\"  \"10.2666666666667\"\n[5,] \"Twin Falls\" \"10.2\" \"9.5\"             \n[6,] \"Twin Falls\" \"10.4\" \"9.9\""
  },
  {
    "objectID": "slides/20-slides.html#fit-a-model",
    "href": "slides/20-slides.html#fit-a-model",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Fit a model",
    "text": "Fit a model\n\nMoran’s I coefficient is the slope of the regression of the lagged asthma percentage vs. the asthma percentage in the tract\nMore generally it is the slope of the lagged average to the measurement\n\n\nM &lt;- lm(asthma.lag ~ cdc$casthma_cr)\n\n\n\n\ncdc$casthma_cr \n     0.6357449 \n\n\n\nThe slope of this line can be a covariate in your model as a way to account for spatial autocorrelation"
  },
  {
    "objectID": "slides/20-slides.html#comparing-observed-to-expected",
    "href": "slides/20-slides.html#comparing-observed-to-expected",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Comparing observed to expected",
    "text": "Comparing observed to expected\n\nWe can generate the expected distribution of Moran’s I coefficients under a Null hypothesis of no spatial autocorrelation\nUsing permutation and a loop to generate simulations of Moran’s I\n\n\nn &lt;- 400L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle income values\n  x &lt;- sample(cdc$casthma_cr, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag &lt;- lag.listw(lw.qn, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}"
  },
  {
    "objectID": "slides/20-slides.html#comparing-observed-to-expected-1",
    "href": "slides/20-slides.html#comparing-observed-to-expected-1",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Comparing observed to expected",
    "text": "Comparing observed to expected\n\n# manual p-value\n# hist is null hypothesis of no spatial autocorrelation\n# red line is our value\nhist(I.r, main=NULL, xlab=\"Moran's I\", las=1, xlim = c(-1, 1))\nabline(v=coef(M)[2], col=\"red\")"
  },
  {
    "objectID": "slides/20-slides.html#compare-to-morans-i-test",
    "href": "slides/20-slides.html#compare-to-morans-i-test",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Compare to Moran’s I test",
    "text": "Compare to Moran’s I test\n\nmoran.test(cdc$casthma_cr, lw.qn)\n\n\n    Moran I test under randomisation\n\ndata:  cdc$casthma_cr  \nweights: lw.qn  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 40.826, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6381428057     -0.0005037783      0.0002447034"
  },
  {
    "objectID": "slides/20-slides.html#finding-neighbors---distance",
    "href": "slides/20-slides.html#finding-neighbors---distance",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Finding Neighbors - Distance",
    "text": "Finding Neighbors - Distance\n\ncdc.pt &lt;- cdc %&gt;% st_point_on_surface(.)\n# get nearest neighbor\ngeog.nearnb &lt;- knn2nb(knearneigh(cdc.pt, k = 1), row.names = cdc.pt$GEOID, sym=TRUE) \n#estimate distance to first nearest neighbor\nnb.nearest &lt;- dnearneigh(cdc.pt,\n                         # minimum distance to search\n                         d1 = 0,  \n                         # maximum distance to search\n                         d2 = max( unlist(nbdists(geog.nearnb, cdc.pt))))\n\n\nSeattle has islands - no contiguity!"
  },
  {
    "objectID": "slides/20-slides.html#getting-weights",
    "href": "slides/20-slides.html#getting-weights",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Getting Weights",
    "text": "Getting Weights\n\n\nlw.nearest &lt;- nb2listw(nb.nearest, style=\"W\")\nasthma.lag &lt;- lag.listw(lw.nearest, cdc$casthma_cr)\n\n\nNotice difference between queen’s and rook’s case. Rooks is most conservative, queen’s is less conservative, neither guarantees a connected graph"
  },
  {
    "objectID": "slides/20-slides.html#fit-a-model-1",
    "href": "slides/20-slides.html#fit-a-model-1",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Fit a model",
    "text": "Fit a model\n\nMoran’s I coefficient is the slope of the regression of the lagged asthma percentage vs. the asthma percentage in the tract\nMore generally it is the slope of the lagged average to the measurement"
  },
  {
    "objectID": "slides/20-slides.html#fit-a-model-2",
    "href": "slides/20-slides.html#fit-a-model-2",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Fit a model",
    "text": "Fit a model\n\nM &lt;- lm(asthma.lag ~ cdc$casthma_cr)\n\n\n\n\ncdc$casthma_cr \n     0.1577524"
  },
  {
    "objectID": "slides/20-slides.html#comparing-observed-to-expected-2",
    "href": "slides/20-slides.html#comparing-observed-to-expected-2",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Comparing observed to expected",
    "text": "Comparing observed to expected\n\nWe can generate the expected distribution of Moran’s I coefficients under a Null hypothesis of no spatial autocorrelation\nUsing permutation and a loop to generate simulations of Moran’s I"
  },
  {
    "objectID": "slides/20-slides.html#comparing-observed-to-expected-3",
    "href": "slides/20-slides.html#comparing-observed-to-expected-3",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Comparing observed to expected",
    "text": "Comparing observed to expected\n\nn &lt;- 400L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle income values\n  x &lt;- sample(cdc$casthma_cr, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag &lt;- lag.listw(lw.nearest, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}"
  },
  {
    "objectID": "slides/20-slides.html#significance-testing",
    "href": "slides/20-slides.html#significance-testing",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Significance testing",
    "text": "Significance testing\n\nPseudo p-value (based on permutations)\nAnalytically (sensitive to deviations from assumptions)\nUsing Monte Carlo\n\n\n#Pseudo p-value\nN.greater &lt;- sum(coef(M)[2] &gt; I.r)\n# add modifiers to stay in -1 to 1 range\n(p &lt;- min(N.greater + 1, n + 1 - N.greater) / (n + 1))\n\n# Analytically\n# Based on a normal distribution, not the distribution of your data\nmoran.test(cdc$casthma_cr,lw.nearest, zero.policy = TRUE)\n\n# Monte Carlo\nmoran.mc(cdc$casthma_cr, lw.nearest, zero.policy = TRUE, nsim=400)"
  },
  {
    "objectID": "slides/20-slides.html#significance-testing-1",
    "href": "slides/20-slides.html#significance-testing-1",
    "title": "Spatial Autocorrelation and Areal Data",
    "section": "Significance testing",
    "text": "Significance testing\n\n\n[1] 0.002493766\n\n\n\n    Moran I test under randomisation\n\ndata:  cdc$casthma_cr  \nweights: lw.nearest    \n\nMoran I statistic standard deviate = 64.107, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     1.577524e-01     -4.990020e-04      6.093649e-06 \n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  cdc$casthma_cr \nweights: lw.nearest  \nnumber of simulations + 1: 401 \n\nstatistic = 0.15775, observed rank = 401, p-value = 0.002494\nalternative hypothesis: greater"
  },
  {
    "objectID": "slides/23-slides.html#best-model-for-what",
    "href": "slides/23-slides.html#best-model-for-what",
    "title": "Statistical Modelling III",
    "section": "Best Model for What?",
    "text": "Best Model for What?\n\n\n\n\n\nfrom Tradennick et al. 2021\n\n\n\n\n\nExploration: describe patterns in the data and generate hypotheses\nInference: evaluate the strength of evidence for some statement about the process\nPrediction: forecast outcomes at unsampled locations based on covariates"
  },
  {
    "objectID": "slides/23-slides.html#the-importance-of-model-fit",
    "href": "slides/23-slides.html#the-importance-of-model-fit",
    "title": "Statistical Modelling III",
    "section": "The Importance of Model Fit",
    "text": "The Importance of Model Fit\n\nThe general regression context:\n\n\\[\n\\begin{equation}\n\\hat{y} = \\mathbf{X}\\hat{\\beta}\n\\end{equation}\n\\]\n\nInference is focused on robust estimates of \\(\\hat{\\beta}\\) given the data we have\nPrediction is focused on accurate forecasts of \\(\\hat{y}\\) at locations where we have yet to collect the data"
  },
  {
    "objectID": "slides/23-slides.html#inference-and-presenceabsence-data",
    "href": "slides/23-slides.html#inference-and-presenceabsence-data",
    "title": "Statistical Modelling III",
    "section": "Inference and Presence/Absence Data",
    "text": "Inference and Presence/Absence Data\n\n\\(\\hat{\\beta}\\) is conditional on variables in the model and those not in the model\n\n\nnsamp &lt;- 1000\ndf &lt;- data.frame(x1 = rnorm(nsamp,0,1),\n                 x2 = rnorm(nsamp,0,1),\n                 x3 = rnorm(nsamp,0,1))\n\nlinpred &lt;- 1 + 2*df$x1 -0.18*df$x2 -3.5*df$x3\ny &lt;- rbinom(nsamp, 1, plogis(linpred))\ndf &lt;- cbind(df, y)\n\nmod1 &lt;- glm(y~x1 +x2, data=df, family=\"binomial\")\nmod2 &lt;- glm(y~x1 +x2 + x3, data=df, family=\"binomial\")"
  },
  {
    "objectID": "slides/23-slides.html#inference-presenceabsence-data",
    "href": "slides/23-slides.html#inference-presenceabsence-data",
    "title": "Statistical Modelling III",
    "section": "Inference & Presence/Absence Data",
    "text": "Inference & Presence/Absence Data\n\n\n\ncoef(mod1)\n\n(Intercept)          x1          x2 \n 0.37557056  0.83794428 -0.06453936 \n\ncoef(mod2)\n\n(Intercept)          x1          x2          x3 \n 0.96456401  1.85379897 -0.09317298 -3.34416357 \n\n\n\n\nprd1 &lt;- predict(mod1, df, \"response\")\ndif1 &lt;- plogis(linpred) - prd1\nprd2 &lt;- predict(mod2, df, \"response\")\ndif2 &lt;- plogis(linpred) - prd2\n\n\n\n\n\n\n\n\n\n\n\n\nInferring coefficient effects requires that your model fit the data well"
  },
  {
    "objectID": "slides/23-slides.html#back-to-our-simulated-data",
    "href": "slides/23-slides.html#back-to-our-simulated-data",
    "title": "Statistical Modelling III",
    "section": "Back to our simulated data",
    "text": "Back to our simulated data"
  },
  {
    "objectID": "slides/23-slides.html#back-to-our-simulated-data-1",
    "href": "slides/23-slides.html#back-to-our-simulated-data-1",
    "title": "Statistical Modelling III",
    "section": "Back to our simulated data",
    "text": "Back to our simulated data\n\nbase.path &lt;- \"/opt/data/data/presabsexample/\" #sets the path to the root directory\n\npres.abs &lt;- st_read(paste0(base.path, \"presenceabsence.shp\"), quiet = TRUE) #read the points with presence absence data\npred.files &lt;- list.files(base.path,pattern='grd$', full.names=TRUE) #get the bioclim data\n\npred.stack &lt;- rast(pred.files) #read into a RasterStack\nnames(pred.stack) &lt;- c(\"MeanAnnTemp\", \"TotalPrecip\", \"PrecipWetQuarter\", \"PrecipDryQuarter\", \"MinTempCold\", \"TempRange\")\nplot(pred.stack)\npred.stack.scl &lt;- scale(pred.stack)\npts.df &lt;- terra::extract(pred.stack.scl, vect(pres.abs), df=TRUE)\npts.df &lt;- cbind(pts.df, pres.abs$y)\ncolnames(pts.df)[8] &lt;- \"y\""
  },
  {
    "objectID": "slides/23-slides.html#using-test-statistics",
    "href": "slides/23-slides.html#using-test-statistics",
    "title": "Statistical Modelling III",
    "section": "Using Test Statistics",
    "text": "Using Test Statistics\n\n\n\n\\(R^2\\) for linear regression:\n\n\\[\n\\begin{equation}\nR^2 = 1- \\frac{SS_{res}}{SS_{tot}}\\\\\nSS_{res} = \\sum_{i}(y_i- f_i)^2\\\\\nSS_{tot} = \\sum_{i}(y_i-\\bar{y})^2\n\\end{equation}\n\\]\n\n\nPerfect prediction (\\(f_i = y_i\\)); \\(SS_{res} = 0\\); and \\(R^2 = 1\\)\nNull prediction (Intercept only) (\\(f_i = \\bar{y}\\)); \\(SS_{res} = SS_{tot}\\); and \\(R^2 = 0\\)\nNo direct way of implementing for logistic regression"
  },
  {
    "objectID": "slides/23-slides.html#pseudo--r2",
    "href": "slides/23-slides.html#pseudo--r2",
    "title": "Statistical Modelling III",
    "section": "Pseudo- \\(R^2\\)",
    "text": "Pseudo- \\(R^2\\)\n\n\n\\[\n\\begin{equation}\nR^2_L = \\frac{D_{null} - D_{fitted}}{D_{null}}\n\\end{equation}\n\\]\n\n\nCohen’s Likelihood Ratio\nDeviance (\\(D\\)), the difference between the model and some hypothetical perfect model (lower is better)\nChallenge: Not monotonically related to \\(p\\)\nChallenge: How high is too high?"
  },
  {
    "objectID": "slides/23-slides.html#cohens-likelihood-ratio",
    "href": "slides/23-slides.html#cohens-likelihood-ratio",
    "title": "Statistical Modelling III",
    "section": "Cohen’s Likelihood Ratio",
    "text": "Cohen’s Likelihood Ratio\n\nlogistic.rich &lt;- glm(y ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\nlogistic.simple &lt;- glm(y ~ MeanAnnTemp + TotalPrecip, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\n# Pseudo-R^2\nwith(logistic.rich, \n     null.deviance - deviance)/with(logistic.rich,\n                                    null.deviance)\n\n[1] 0.4495966\n\nwith(logistic.simple, \n     null.deviance - deviance)/with(logistic.simple,\n                                    null.deviance)\n\n[1] 0.4567641"
  },
  {
    "objectID": "slides/23-slides.html#pseudo--r2-1",
    "href": "slides/23-slides.html#pseudo--r2-1",
    "title": "Statistical Modelling III",
    "section": "Pseudo- \\(R^2\\)",
    "text": "Pseudo- \\(R^2\\)\n\n\n\\[\n\\begin{equation}\n\\begin{aligned}\nR^2_{CS} &= 1 - \\left( \\frac{L_0}{L_M} \\right)^{(2/n)}\\\\\n&= 1 - \\exp^{2(ln(L_0)-ln(L_M))/n}\n\\end{aligned}\n\\end{equation}\n\\]\n\n\n\nCox and Snell \\(R^2\\)\nLikelihood (\\(L\\)), the probability of observing the sample given an assumed distribution\nChallenge: Maximum value is less than 1 and changes with \\(n\\)\nCorrection by Nagelkerke so that maximum is 1"
  },
  {
    "objectID": "slides/23-slides.html#cox-and-snell-r2",
    "href": "slides/23-slides.html#cox-and-snell-r2",
    "title": "Statistical Modelling III",
    "section": "Cox and Snell \\(R^2\\)",
    "text": "Cox and Snell \\(R^2\\)\n\nlogistic.null &lt;- glm(y ~ 1, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\n# Cox & Snell R^2 for logistic.rich\n1 - exp(2*(logLik(logistic.null)[1] - logLik(logistic.rich)[1])/nobs(logistic.rich))\n\n[1] 0.4308873\n\n# Cox & Snell R^2 for logistic.simple\n1 - exp(2*(logLik(logistic.null)[1] - logLik(logistic.simple)[1])/nobs(logistic.simple))\n\n[1] 0.4359785"
  },
  {
    "objectID": "slides/23-slides.html#using-test-statistics-1",
    "href": "slides/23-slides.html#using-test-statistics-1",
    "title": "Statistical Modelling III",
    "section": "Using Test Statistics",
    "text": "Using Test Statistics\n\nBased on the data used in the model (i.e., not prediction)\nLikelihood Ratio behaves most similarly to \\(R^2\\)\nCox and Snell (and Nagelkerke) increases with more presences\nOngoing debate over which is “best”\nDon’t defer to a single statistic"
  },
  {
    "objectID": "slides/23-slides.html#predictive-performance-and-fit",
    "href": "slides/23-slides.html#predictive-performance-and-fit",
    "title": "Statistical Modelling III",
    "section": "Predictive Performance and Fit",
    "text": "Predictive Performance and Fit\n\nPredictive performance can be an estimate of fit\nComparisons are often relative (better \\(\\neq\\) good)\nTheoretical and subsampling methods"
  },
  {
    "objectID": "slides/23-slides.html#theoretical-assessment-of-predictive-performance",
    "href": "slides/23-slides.html#theoretical-assessment-of-predictive-performance",
    "title": "Statistical Modelling III",
    "section": "Theoretical Assessment of Predictive Performance",
    "text": "Theoretical Assessment of Predictive Performance\n\n\n\n\n\nHirotugu Akaike of AIC\n\n\n\n\n\nInformation Criterion Methods\nMinimize the amount of information lost by using model to approximate true process\nTrade-off between fit and overfitting\nCan’t know the true process (so comparisons are relative) \\[\n\\begin{equation}\nAIC = -2ln(\\hat{L}) +2k\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/23-slides.html#aic-comparison",
    "href": "slides/23-slides.html#aic-comparison",
    "title": "Statistical Modelling III",
    "section": "AIC Comparison",
    "text": "AIC Comparison\n\nlogistic.null$formula\n\ny ~ 1\n\nlogistic.rich$formula\n\ny ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter\n\nlogistic.simple$formula\n\ny ~ MeanAnnTemp + TotalPrecip\n\nAIC(logistic.null, logistic.rich, logistic.simple)\n\n                df       AIC\nlogistic.null    1 127.37389\nlogistic.rich    4  77.00622\nlogistic.simple  3  74.10760"
  },
  {
    "objectID": "slides/23-slides.html#sub-sampling-methods",
    "href": "slides/23-slides.html#sub-sampling-methods",
    "title": "Statistical Modelling III",
    "section": "Sub-sampling Methods",
    "text": "Sub-sampling Methods\n\n\n\nSplit data into training and testing\nTesting set needs to be large enough for results to be statistically meaningful\nTest set should be representative of the data as a whole\nValidation data used to tune parameters (not always)"
  },
  {
    "objectID": "slides/23-slides.html#subsampling-your-data-with-caret",
    "href": "slides/23-slides.html#subsampling-your-data-with-caret",
    "title": "Statistical Modelling III",
    "section": "Subsampling your data with caret",
    "text": "Subsampling your data with caret\n\npts.df$y &lt;- factor(ifelse(pts.df$y == 1, \"Yes\", \"No\"), \n                      levels = c(\"Yes\", \"No\"))\nlibrary(caret)\nTrain &lt;- createDataPartition(pts.df$y, p=0.6, list=FALSE)\n\ntraining &lt;- pts.df[ Train, ]\ntesting &lt;- pts.df[ -Train, ]"
  },
  {
    "objectID": "slides/23-slides.html#misclassification",
    "href": "slides/23-slides.html#misclassification",
    "title": "Statistical Modelling III",
    "section": "Misclassification",
    "text": "Misclassification\n\n\n\nConfusion matrices compare actual values to predictions\n\n\n\nTrue Positive (TN) - This is correctly classified as the class if interest / target.\nTrue Negative (TN) - This is correctly classified as not a class of interest / target.\nFalse Positive (FP) - This is wrongly classified as the class of interest / target.\nFalse Negative (FN) - This is wrongly classified as not a class of interest / target."
  },
  {
    "objectID": "slides/23-slides.html#confusion-matrices-in-r",
    "href": "slides/23-slides.html#confusion-matrices-in-r",
    "title": "Statistical Modelling III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\ntrain.log &lt;- glm(y ~ ., \n                 family=\"binomial\", \n                 data=training[,2:8])\n\npredicted.log &lt;- predict(train.log, \n                         newdata=testing[,2:8], \n                         type=\"response\")\n\npred &lt;- factor(\n  ifelse(predicted.log &gt; 0.5, \n                         \"Yes\",\n                         \"No\"),\n  levels = c(\"Yes\", \"No\"))\n\n\n\n\nconfusionMatrix(testing$y, pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Yes No\n       Yes   4  8\n       No   26  1\n                                         \n               Accuracy : 0.1282         \n                 95% CI : (0.043, 0.2743)\n    No Information Rate : 0.7692         \n    P-Value [Acc &gt; NIR] : 1.000000       \n                                         \n                  Kappa : -0.4444        \n                                         \n Mcnemar's Test P-Value : 0.003551       \n                                         \n            Sensitivity : 0.13333        \n            Specificity : 0.11111        \n         Pos Pred Value : 0.33333        \n         Neg Pred Value : 0.03704        \n             Prevalence : 0.76923        \n         Detection Rate : 0.10256        \n   Detection Prevalence : 0.30769        \n      Balanced Accuracy : 0.12222        \n                                         \n       'Positive' Class : Yes"
  },
  {
    "objectID": "slides/23-slides.html#confusion-matrices",
    "href": "slides/23-slides.html#confusion-matrices",
    "title": "Statistical Modelling III",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\n\n\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Accuracy} &= \\frac{TP + TN}{TP + TN + FP + FN}\\\\\n\\text{Sensitivity (Recall)} &= \\frac{TP}{TP + FN}\\\\\n\\text{Specificity} &= \\frac{TN}{FP + TN}\\\\\n\\text{Precision} &= \\frac{TP}{TP + FP}\n\\end{aligned}\n\\end{equation}\n\\]\n\n\nDepends upon threshold!!"
  },
  {
    "objectID": "slides/23-slides.html#confusion-matrices-in-r-1",
    "href": "slides/23-slides.html#confusion-matrices-in-r-1",
    "title": "Statistical Modelling III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\nlibrary(tree)\ntree.model &lt;- tree(y ~ . , training[,2:8])\npredict.tree &lt;- predict(tree.model, newdata=testing[,2:8], type=\"class\")\n\n\n\n\nconfusionMatrix(testing$y, predict.tree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Yes No\n       Yes   7  5\n       No    2 25\n                                          \n               Accuracy : 0.8205          \n                 95% CI : (0.6647, 0.9246)\n    No Information Rate : 0.7692          \n    P-Value [Acc &gt; NIR] : 0.2928          \n                                          \n                  Kappa : 0.5473          \n                                          \n Mcnemar's Test P-Value : 0.4497          \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.5833          \n         Neg Pred Value : 0.9259          \n             Prevalence : 0.2308          \n         Detection Rate : 0.1795          \n   Detection Prevalence : 0.3077          \n      Balanced Accuracy : 0.8056          \n                                          \n       'Positive' Class : Yes"
  },
  {
    "objectID": "slides/23-slides.html#confusion-matrices-in-r-2",
    "href": "slides/23-slides.html#confusion-matrices-in-r-2",
    "title": "Statistical Modelling III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\nlibrary(randomForest, quietly = TRUE)\nclass.model &lt;- y ~ .\nrf &lt;- randomForest(class.model, data=training[,2:8])\npredict.rf &lt;- predict(rf, newdata=testing[,2:8], type=\"class\")\n\n\n\n\nconfusionMatrix(testing$y, predict.rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Yes No\n       Yes   7  5\n       No    1 26\n                                          \n               Accuracy : 0.8462          \n                 95% CI : (0.6947, 0.9414)\n    No Information Rate : 0.7949          \n    P-Value [Acc &gt; NIR] : 0.2851          \n                                          \n                  Kappa : 0.602           \n                                          \n Mcnemar's Test P-Value : 0.2207          \n                                          \n            Sensitivity : 0.8750          \n            Specificity : 0.8387          \n         Pos Pred Value : 0.5833          \n         Neg Pred Value : 0.9630          \n             Prevalence : 0.2051          \n         Detection Rate : 0.1795          \n   Detection Prevalence : 0.3077          \n      Balanced Accuracy : 0.8569          \n                                          \n       'Positive' Class : Yes"
  },
  {
    "objectID": "slides/23-slides.html#threshold-free-methods",
    "href": "slides/23-slides.html#threshold-free-methods",
    "title": "Statistical Modelling III",
    "section": "Threshold-Free Methods",
    "text": "Threshold-Free Methods\n\n\n\n\n\nReceiver Operating Characteristic Curves\nIllustrates discrimination of binary classifier as the threshold is varied\nArea Under the Curve (AUC) provides an estimate of classification ability"
  },
  {
    "objectID": "slides/23-slides.html#criticisms-of-rocauc",
    "href": "slides/23-slides.html#criticisms-of-rocauc",
    "title": "Statistical Modelling III",
    "section": "Criticisms of ROC/AUC",
    "text": "Criticisms of ROC/AUC\n\nTreats false positives and false negatives equally\nUndervalues models that predict across smaller geographies\nFocus on discrimination and not calibration\nNew methods for presence-only data"
  },
  {
    "objectID": "slides/23-slides.html#roc-in-r-using-proc",
    "href": "slides/23-slides.html#roc-in-r-using-proc",
    "title": "Statistical Modelling III",
    "section": "ROC in R (using pROC)",
    "text": "ROC in R (using pROC)\n\nGenerate predictions (note the difference for tree and rf)\n\n\nlibrary(pROC, quietly = TRUE)\ntrain.log &lt;- glm(y ~ ., \n                 family=\"binomial\", \n                 data=training[,2:8])\n\npredicted.log &lt;- predict(train.log, \n                         newdata=testing[,2:8], \n                         type=\"response\")\n\npredict.tree &lt;- predict(tree.model, newdata=testing[,2:8], type=\"vector\")[,2]\n\npredict.rf &lt;- predict(rf, newdata=testing[,2:8], type=\"prob\")[,2]"
  },
  {
    "objectID": "slides/23-slides.html#roc-in-r-using-proc-1",
    "href": "slides/23-slides.html#roc-in-r-using-proc-1",
    "title": "Statistical Modelling III",
    "section": "ROC in R (using pROC)",
    "text": "ROC in R (using pROC)\n\nplot(roc(testing$y, predicted.log), print.auc=TRUE)\n\nplot(roc(testing$y, predict.tree), print.auc=TRUE, print.auc.y = 0.45, col=\"green\", add=TRUE)\n\nplot(roc(testing$y, predict.rf), print.auc=TRUE, print.auc.y = 0.4, col=\"blue\", add=TRUE)"
  },
  {
    "objectID": "slides/23-slides.html#cross-validation",
    "href": "slides/23-slides.html#cross-validation",
    "title": "Statistical Modelling III",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nOften want to make sure that fit/accuracy not a function of partition choice\nCross-validation allows resampling of data (multiple times)\nK-fold - Data are split into K datasets of ~ equal size, model fit to \\((K-1)(\\frac{n}{K})\\) observations to predict held-out set\nLeave One Out (LOO) - Model fit to n-1 observations to predict the held out observation"
  },
  {
    "objectID": "slides/23-slides.html#crossvalidation-in-r-using-caret",
    "href": "slides/23-slides.html#crossvalidation-in-r-using-caret",
    "title": "Statistical Modelling III",
    "section": "Crossvalidation in R using caret",
    "text": "Crossvalidation in R using caret\n\nfitControl &lt;- trainControl(method = \"repeatedcv\",\n                           number = 10,\n                           repeats = 10,\n                           classProbs = TRUE,\n                           summaryFunction = twoClassSummary)\n\nlog.model &lt;- train(y ~., data = pts.df[,2:8],\n               method = \"glm\",\n               trControl = fitControl)\npred.log &lt;- predict(log.model, newdata = testing[,2:8], type=\"prob\")[,2]\n\ntree.model &lt;- train(y ~., data = pts.df[,2:8],\n               method = \"rpart\",\n               trControl = fitControl)\n\npred.tree &lt;- predict(tree.model, newdata=testing[,2:8], type=\"prob\")[,2]\n\nrf.model &lt;- train(y ~., data = pts.df[,2:8],\n               method = \"rf\",\n               trControl = fitControl)\npred.rf &lt;- predict(rf.model, newdata=testing[,2:8], type=\"prob\")[,2]"
  },
  {
    "objectID": "slides/23-slides.html#crossvalidation-in-r-using-caret-1",
    "href": "slides/23-slides.html#crossvalidation-in-r-using-caret-1",
    "title": "Statistical Modelling III",
    "section": "Crossvalidation in R using caret",
    "text": "Crossvalidation in R using caret\n\nplot(roc(testing$y, pred.log), print.auc=TRUE)\n\nplot(roc(testing$y, pred.tree), print.auc=TRUE, print.auc.y = 0.45, col=\"green\", add=TRUE)\n\nplot(roc(testing$y, pred.rf), print.auc=TRUE, print.auc.y = 0.4, col=\"blue\", add=TRUE)"
  },
  {
    "objectID": "slides/23-slides.html#crossvalidation-in-r-using-caret-2",
    "href": "slides/23-slides.html#crossvalidation-in-r-using-caret-2",
    "title": "Statistical Modelling III",
    "section": "Crossvalidation in R using caret",
    "text": "Crossvalidation in R using caret\n\nresamps &lt;- resamples(list(GLM = log.model,\n                          TREE = tree.model,\n                          RF = rf.model))\ndotplot(resamps)"
  },
  {
    "objectID": "slides/23-slides.html#spatial-predictions",
    "href": "slides/23-slides.html#spatial-predictions",
    "title": "Statistical Modelling III",
    "section": "Spatial predictions",
    "text": "Spatial predictions\n\nbest.rf &lt;- rf.model$finalModel\nbest.glm &lt;- log.model$finalModel\n\nrf.spatial &lt;- terra::predict(pred.stack.scl, best.rf, type=\"prob\")\n\n\nglm.spatial &lt;- terra::predict(pred.stack.scl, best.glm,type=\"response\" )"
  },
  {
    "objectID": "slides/23-slides.html#spatial-predictions-1",
    "href": "slides/23-slides.html#spatial-predictions-1",
    "title": "Statistical Modelling III",
    "section": "Spatial predictions",
    "text": "Spatial predictions"
  },
  {
    "objectID": "slides/23-slides.html#practice",
    "href": "slides/23-slides.html#practice",
    "title": "Statistical Modelling III",
    "section": "Practice",
    "text": "Practice\n\n\nCreate a training/testing split of the cejst_mod data from last class.\nFit the logistic, tree, and random forest models with your training data, generate predictions for the testing data, and generate confusion matrices for each model. Which one seems best?\nMake an ROC plot for the three models (make sure to generate the correct type of predictions for the tree and rf models, code is in the slides). Which model seems best?\nUse the cross-validation code from the slides to create cross-validated versions of the three models (remember to use cejst_mod as the train data here). Generate ROC curves for these models. Which model seems best?\n\nChallenge: Generate predictions from the best model and create a map of predicted CFLRP probability with the CFLRP boundaries overlayed on top. You will need to join the cejst_sub geometries back to the cejst_mod data."
  },
  {
    "objectID": "example/session-12-example.html",
    "href": "example/session-12-example.html",
    "title": "Session 12 Code",
    "section": "",
    "text": "Code\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nlibrary(spDataLarge)\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#read-in-packages",
    "href": "example/session-12-example.html#read-in-packages",
    "title": "Session 12 Code",
    "section": "",
    "text": "Code\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(terra)\n\n\nterra 1.7.78\n\n\nCode\nlibrary(spDataLarge)\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#changing-resolution",
    "href": "example/session-12-example.html#changing-resolution",
    "title": "Session 12 Code",
    "section": "Changing resolution:",
    "text": "Changing resolution:\n\n\nCode\nr &lt;- rast()\nr\n\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n\n\n\nCode\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\n\nCode\nra &lt;- aggregate(r, 20)\nra\n\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\n\n\n\nCode\n# rarely used\nrd &lt;- disagg(r, 20)\n\n\n\n|---------|---------|---------|---------|\n=\n                                          \n\n\nCode\nplot(rd)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#crop-and-mask",
    "href": "example/session-12-example.html#crop-and-mask",
    "title": "Session 12 Code",
    "section": "Crop and Mask",
    "text": "Crop and Mask\n\n\nCode\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n\n[1] TRUE\n\n\n\n\nCode\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")\n\n\n\n\nCode\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk2 &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=-1000)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#extend",
    "href": "example/session-12-example.html#extend",
    "title": "Session 12 Code",
    "section": "Extend",
    "text": "Extend\n\n\nCode\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\n\nCode\nplot(srtm.ext)\nplot(st_geometry(zion.buff), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#practice",
    "href": "example/session-12-example.html#practice",
    "title": "Session 12 Code",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\n\nCode\nid &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\nor &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_OR.tif\")\n\n\n\nAggregate\n\n\nCode\nid_agg &lt;- aggregate(id, fact = 30)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nor_agg &lt;- aggregate(or, fact = 30)\n\n\n\n\nCheck for alignment\n\n\nCode\ncrs(id_agg) == crs(or_agg)\n\n\n[1] TRUE\n\n\nCode\norigin(id_agg) == origin(or_agg)\n\n\n[1] FALSE FALSE\n\n\nCode\next(id_agg) == ext(or_agg)\n\n\n[1] FALSE\n\n\n\n\nAlign the origins\nBoth project and resample don’t give the intended results.\n\n\nCode\nid_proj &lt;- project(id_agg, or_agg)\n\nid_resamp &lt;- resample(id_agg, or_agg)\n\npar(mfrow = c(1,2))\nplot(id_proj)\nplot(id_resamp)\n\n\n\n\n\n\n\n\n\nWe need to extend, then resample. Resample is the faster choice because the CRS’s of the the two rasters already match.\n\n\nCode\nid_ext &lt;- extend(id_agg, or_agg)\nor_ext &lt;- extend(or_agg, id_ext)\n\nid_resamp &lt;- resample(id_ext, or_ext)\n\n\n\n\nCode\ncrs(id_resamp) == crs(or_ext)\n\n\n[1] TRUE\n\n\nCode\norigin(id_resamp) == origin(or_ext)\n\n\n[1] TRUE TRUE\n\n\nCode\next(id_resamp) == ext(or_ext)\n\n\n[1] TRUE\n\n\n\n\nMosaic\n\n\nCode\nidor &lt;- mosaic(id_resamp, or_ext)\n\n\n\n\nCrops and Masks\n\n\nCode\nidor_counties &lt;- counties(state = c(\"ID\", \"OR\"))\n\n\nRetrieving data for the year 2022\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\neast_id &lt;- idor_counties %&gt;%\n  filter(NAME %in% c(\"Teton\", \"Jefferson\", \"Madison\"))\nplot(st_geometry(east_id))\n\n\n\n\n\n\n\n\n\nmask without crop leaves a lot of white space.\n\n\nCode\neast_id_proj &lt;- st_transform(east_id, crs(idor))\n\neast_id_fire &lt;- mask(x = idor, mask = east_id_proj)\n\neast_id_fire2 &lt;- crop(idor, east_id_proj, mask=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#read-in-packages-1",
    "href": "example/session-12-example.html#read-in-packages-1",
    "title": "Session 12 Code",
    "section": "Read in packages:",
    "text": "Read in packages:\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nlibrary(tigris)\nlibrary(tidyverse)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#changing-resolution-1",
    "href": "example/session-12-example.html#changing-resolution-1",
    "title": "Session 12 Code",
    "section": "Changing resolution:",
    "text": "Changing resolution:\n\n\nCode\nr &lt;- rast()\nr\n\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n\n\n\n\nCode\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\n\n\n\n\n\nCode\nra &lt;- aggregate(r, 20)\nra\n\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\n\n\n\nCode\n# rarely used\nrd &lt;- disagg(r, 20)\n\n\n\n|---------|---------|---------|---------|\n=\n                                          \n\n\nCode\nplot(rd)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#crop-and-mask-1",
    "href": "example/session-12-example.html#crop-and-mask-1",
    "title": "Session 12 Code",
    "section": "Crop and Mask",
    "text": "Crop and Mask\n\n\nCode\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n\n[1] TRUE\n\n\n\n\nCode\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")\n\n\n\n\nCode\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsrtm.msk2 &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=-1000)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#extend-1",
    "href": "example/session-12-example.html#extend-1",
    "title": "Session 12 Code",
    "section": "Extend",
    "text": "Extend\n\n\nCode\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\n\nSpatExtent : -113.343749879444, -112.74541654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\n\nCode\nplot(srtm.ext)\nplot(st_geometry(zion.buff), add=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "example/session-12-example.html#practice-1",
    "href": "example/session-12-example.html#practice-1",
    "title": "Session 12 Code",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\n\nCode\nid &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_ID.tif\")\nor &lt;- rast(\"/opt/data/data/rasterexample/Copy of CRPS_OR.tif\")\n\n\n\nAggregate\n\n\nCode\nid_agg &lt;- aggregate(id, fact = 30)\n\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\nCode\nor_agg &lt;- aggregate(or, fact = 30)\n\n\n\n\nCheck for alignment\n\n\nCode\ncrs(id_agg) == crs(or_agg)\n\n\n[1] TRUE\n\n\nCode\norigin(id_agg) == origin(or_agg)\n\n\n[1] FALSE FALSE\n\n\nCode\next(id_agg) == ext(or_agg)\n\n\n[1] FALSE\n\n\n\n\nAlign the origins\nBoth project and resample don’t give the intended results.\n\n\nCode\nid_proj &lt;- project(id_agg, or_agg)\n\nid_resamp &lt;- resample(id_agg, or_agg)\n\npar(mfrow = c(1,2))\nplot(id_proj)\nplot(id_resamp)\n\n\n\n\n\n\n\n\n\nWe need to extend, then resample. Resample is the faster choice because the CRS’s of the the two rasters already match.\n\n\nCode\nid_ext &lt;- extend(id_agg, or_agg)\nor_ext &lt;- extend(or_agg, id_ext)\n\nid_resamp &lt;- resample(id_ext, or_ext)\n\n\n\n\nCode\ncrs(id_resamp) == crs(or_ext)\n\n\n[1] TRUE\n\n\nCode\norigin(id_resamp) == origin(or_ext)\n\n\n[1] TRUE TRUE\n\n\nCode\next(id_resamp) == ext(or_ext)\n\n\n[1] TRUE\n\n\n\n\nMosaic\n\n\nCode\nidor &lt;- mosaic(id_resamp, or_ext)\n\n\n\n\nCrops and Masks\n\n\nCode\nidor_counties &lt;- counties(state = c(\"ID\", \"OR\"))\n\n\nRetrieving data for the year 2022\n\n\nCode\neast_id &lt;- idor_counties %&gt;%\n  filter(NAME %in% c(\"Teton\", \"Jefferson\", \"Madison\"))\nplot(st_geometry(east_id))\n\n\n\n\n\n\n\n\n\nmask without crop leaves a lot of white space.\n\n\nCode\neast_id_proj &lt;- st_transform(east_id, crs(idor))\n\neast_id_fire &lt;- mask(x = idor, mask = east_id_proj)\n\neast_id_fire2 &lt;- crop(idor, east_id_proj, mask=TRUE)",
    "crumbs": [
      "Examples",
      "Spatial operations in R",
      "Raster Operations I"
    ]
  },
  {
    "objectID": "slides/16-slides.html#objectives",
    "href": "slides/16-slides.html#objectives",
    "title": "Combining Raster and Vector Data",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nConvert between raster and vector datasets\nGenerate new rasters describing the spatial arrangement of vector data\nExtract raster values as attributes of vector data"
  },
  {
    "objectID": "slides/16-slides.html#converting-between-formats-1",
    "href": "slides/16-slides.html#converting-between-formats-1",
    "title": "Combining Raster and Vector Data",
    "section": "Converting Between Formats",
    "text": "Converting Between Formats\n\nUsing coercion (as, rast, vect) can change class, but not data model\nSometimes we need to actually change the data model"
  },
  {
    "objectID": "slides/16-slides.html#converting-vectors-to-rasters-using-rasterize",
    "href": "slides/16-slides.html#converting-vectors-to-rasters-using-rasterize",
    "title": "Combining Raster and Vector Data",
    "section": "Converting Vectors to Rasters Using rasterize",
    "text": "Converting Vectors to Rasters Using rasterize\n\nA special kind of data aggregation\nx is your SpatVector object\ny is a template raster with the appropriate CRS, resolution, and extent\nfun allows you to specify the value of the resulting raster"
  },
  {
    "objectID": "slides/16-slides.html#using-rasterize",
    "href": "slides/16-slides.html#using-rasterize",
    "title": "Combining Raster and Vector Data",
    "section": "Using rasterize",
    "text": "Using rasterize\n\n\nPresence/Absence\nfield specifies which value should be returned to non-empty cells\n\n\n\n\n\n\nhospitals_pnw &lt;- read_csv(\"/opt/data/data/assignment06/landmarks_pnw.csv\") %&gt;%\n  filter(., MTFCC == \"K2543\") %&gt;%\n  st_as_sf(., coords = c(\"longitude\", \"latitude\"), crs=4269) %&gt;%\n  st_transform(crs = 5070)\n\n\nraster_template = rast(ext(hospitals_pnw), resolution = 10000,\n                       crs = st_crs(hospitals_pnw)$wkt)\n\nhosp_raster1 = rasterize(hospitals_pnw, raster_template,\n                         field = 1)"
  },
  {
    "objectID": "slides/16-slides.html#using-rasterize-1",
    "href": "slides/16-slides.html#using-rasterize-1",
    "title": "Combining Raster and Vector Data",
    "section": "Using rasterize",
    "text": "Using rasterize\n\n\nThe fun argument specifies how we aggregate the data\nUseful for counting occurrences (using length)\n\n\n\n\n\n\nhosp_raster2 = rasterize(hospitals_pnw, raster_template, \n                         fun = \"length\")"
  },
  {
    "objectID": "slides/16-slides.html#using-rasterize-2",
    "href": "slides/16-slides.html#using-rasterize-2",
    "title": "Combining Raster and Vector Data",
    "section": "Using rasterize",
    "text": "Using rasterize\n\n\nThe fun argument specifies how we aggregate the data\nCan use a variety of functions\n\n\n\n\n\n\nhospitals_pnw$rand_capacity &lt;- rnorm(n = nrow(hospitals_pnw),\n                                     mean = 5000,\n                                     sd = 2000)\n\nhosp_raster3 = rasterize(hospitals_pnw, raster_template, \n                         field = \"rand_capacity\", fun = sum)"
  },
  {
    "objectID": "slides/16-slides.html#lines-and-polygons",
    "href": "slides/16-slides.html#lines-and-polygons",
    "title": "Combining Raster and Vector Data",
    "section": "Lines and Polygons",
    "text": "Lines and Polygons\n\nCan use rasterize or stars::st_rasterize\nResult depends on the touches argument"
  },
  {
    "objectID": "slides/16-slides.html#converting-rasters-to-vectors",
    "href": "slides/16-slides.html#converting-rasters-to-vectors",
    "title": "Combining Raster and Vector Data",
    "section": "Converting rasters to vectors",
    "text": "Converting rasters to vectors\n\nLess common, but can convert to vector data\nas.points, as.countour, and polygonize\n\n\n\n\n\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ncl = as.contour(dem)"
  },
  {
    "objectID": "slides/16-slides.html#generating-new-data-1",
    "href": "slides/16-slides.html#generating-new-data-1",
    "title": "Combining Raster and Vector Data",
    "section": "Generating New Data",
    "text": "Generating New Data\n\nSometimes we want a raster describing the spatial context of vector data\ndistance is a simple method\nWe’ll use interpolation in the next few weeks"
  },
  {
    "objectID": "slides/16-slides.html#generating-distance-rasters",
    "href": "slides/16-slides.html#generating-distance-rasters",
    "title": "Combining Raster and Vector Data",
    "section": "Generating Distance Rasters",
    "text": "Generating Distance Rasters\n\nreturns a distance matrix or SpatRaster\n\n\nhosp_dist &lt;- distance(vect(hospitals_pnw))\nhead(as.matrix(hosp_dist))[1:5, 1:5]\n\n           1        2        3          4        5\n1      0.000 209100.3 474603.9   5731.844 422252.6\n2 209100.275      0.0 401284.9 204972.864 281571.2\n3 474603.876 401284.9      0.0 469036.193 171252.0\n4   5731.844 204972.9 469036.2      0.000 416568.2\n5 422252.623 281571.2 171252.0 416568.171      0.0"
  },
  {
    "objectID": "slides/16-slides.html#generating-distance-rasters-1",
    "href": "slides/16-slides.html#generating-distance-rasters-1",
    "title": "Combining Raster and Vector Data",
    "section": "Generating Distance Rasters",
    "text": "Generating Distance Rasters\n\nreturns a distance matrix or SpatRaster\n\n\nraster_template = rast(ext(hospitals_pnw), resolution = 1000,\n                       crs = st_crs(hospitals_pnw)$wkt)\nhosp_raster1 = rasterize(hospitals_pnw, raster_template,\n                       field = 1)\n\nhosp_dist_rast &lt;- distance(hosp_raster1)\nplot(hosp_dist_rast)"
  },
  {
    "objectID": "slides/16-slides.html#creating-vector-data-by-extraction",
    "href": "slides/16-slides.html#creating-vector-data-by-extraction",
    "title": "Combining Raster and Vector Data",
    "section": "Creating Vector Data by Extraction",
    "text": "Creating Vector Data by Extraction\n\nSometimes we want to use rasters to create new attributes\nfun controls how the cells are aggregated\n\n\nwildfire_haz &lt;- rast(\"/opt/data/data/assignment07/wildfire_hazard_agg.tif\")\n\n\nhospitals_pnw_proj &lt;- st_transform(hospitals_pnw, crs(wildfire_haz))\n\nhosp_fire_haz &lt;- terra::extract(wildfire_haz, hospitals_pnw_proj)\nhead(hosp_fire_haz)\n\n  ID    WHP_ID\n1  1 1952.8750\n2  2    0.0000\n3  3  741.4531\n4  4  200.2812\n5  5    0.0000\n6  6  150.5938"
  },
  {
    "objectID": "slides/16-slides.html#creating-vector-data-by-extraction-1",
    "href": "slides/16-slides.html#creating-vector-data-by-extraction-1",
    "title": "Combining Raster and Vector Data",
    "section": "Creating Vector Data by Extraction",
    "text": "Creating Vector Data by Extraction\n\nCan use zonal for one summary statistic for polygons\n\n\ncejst &lt;- st_read(\"/opt/data/data/assignment06/cejst_pnw.shp\") %&gt;%\n  st_transform(crs = crs(wildfire_haz)) %&gt;%\n  filter(!st_is_empty(.))\n\n\nwildfire.zones &lt;- terra::zonal(wildfire_haz, vect(cejst), fun=\"mean\", na.rm=TRUE)\n\nhead(wildfire.zones)\n\n       WHP_ID\n1    3.053172\n2 2997.795051\n3    6.647930\n4   85.971309\n5   34.706535\n6   17.306250"
  },
  {
    "objectID": "slides/16-slides.html#ways-to-extract-raster-data-for-polygons",
    "href": "slides/16-slides.html#ways-to-extract-raster-data-for-polygons",
    "title": "Combining Raster and Vector Data",
    "section": "3 ways to extract raster data for polygons",
    "text": "3 ways to extract raster data for polygons\n\nsystem.time(wildfire.zones &lt;- terra::zonal(wildfire_haz, vect(cejst), fun=\"mean\", na.rm=TRUE))\n\n   user  system elapsed \n  31.39    1.21   32.89 \n\nsystem.time(wildfire.zones2 &lt;- terra::extract(wildfire_haz, vect(cejst), fun=mean, na.rm=TRUE))\n\n   user  system elapsed \n  31.50    1.05   32.73 \n\nsystem.time(wildfire.zones3 &lt;- exactextractr::exact_extract(wildfire_haz, cejst, fun=\"mean\", progress = FALSE))\n\n   user  system elapsed \n   3.02    0.12    3.14 \n\n\n\n\n\n\n       WHP_ID\n1    3.053172\n2 2997.795051\n3    6.647930\n4   85.971309\n5   34.706535\n6   17.306250\n\n\n\n\n\n  ID      WHP_ID\n1  1    3.053172\n2  2 2997.795051\n3  3    6.647930\n4  4   85.971309\n5  5   34.706535\n6  6   17.306250\n\n\n\n\n\n[1]    3.230088 2997.102783    6.464695   86.015327   34.672573   16.559727"
  },
  {
    "objectID": "slides/16-slides.html#thinking-about-the-data",
    "href": "slides/16-slides.html#thinking-about-the-data",
    "title": "Combining Raster and Vector Data",
    "section": "Thinking about the data",
    "text": "Thinking about the data\n\nDatasets - Forest Service Boundaries, CFLRP Boundaries, Wildfire Risk Raster, CEJST shapefile\nDependent Variable - CFLRP (T or F)\nIndependent Variables - Wildfire hazard, income, education, housing burden"
  },
  {
    "objectID": "slides/16-slides.html#building-some-pseudocode",
    "href": "slides/16-slides.html#building-some-pseudocode",
    "title": "Combining Raster and Vector Data",
    "section": "Building some Pseudocode",
    "text": "Building some Pseudocode\n\n1. Load libraries\n2. Load data\n3. Check validity and alignment\n4. Subset to relevant geographies\n5. Select relevant attributes\n6. Extract wildfire risk\n7. CFLRP T or F\n8. Compare risks"
  },
  {
    "objectID": "slides/16-slides.html#load-libraries",
    "href": "slides/16-slides.html#load-libraries",
    "title": "Combining Raster and Vector Data",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)"
  },
  {
    "objectID": "slides/16-slides.html#load-the-data",
    "href": "slides/16-slides.html#load-the-data",
    "title": "Combining Raster and Vector Data",
    "section": "Load the data",
    "text": "Load the data\n\nDownloading USFS data using the function in the code folder\n\n\ndownload_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n}\n\n### FS Boundaries\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\nfs.bdry &lt;- download_unzip_read(link = fs.url)\n\n### CFLRP Data\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ncflrp.bdry &lt;- download_unzip_read(link = cflrp.url)"
  },
  {
    "objectID": "slides/26-slides.html#tidycensus-package",
    "href": "slides/26-slides.html#tidycensus-package",
    "title": "Data Visualization and Maps II",
    "section": "tidycensus package",
    "text": "tidycensus package\nhttps://walker-data.com/tidycensus/articles/basic-usage.html"
  },
  {
    "objectID": "slides/26-slides.html#using-ggplot2",
    "href": "slides/26-slides.html#using-ggplot2",
    "title": "Data Visualization and Maps II",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\ncty.info &lt;- get_acs(geography = \"county\", \n                      variables = c(pop=\"B01003_001\", \n                                    medincome = \"B19013_001\"),\n                      survey=\"acs5\",\n                      state = c(\"WA\", \"OR\", \"ID\", \"MT\", \"WY\"),\n                      geometry = TRUE, key = censkey, progress_bar=FALSE) %&gt;% \n  select(., -moe) %&gt;% \n  pivot_wider(\n    names_from = \"variable\",\n    values_from = \"estimate\"\n  )\n\np &lt;- ggplot(data=cty.info) +\n  geom_sf(mapping=aes(fill=medincome))"
  },
  {
    "objectID": "slides/26-slides.html#static-maps-with-ggplot2",
    "href": "slides/26-slides.html#static-maps-with-ggplot2",
    "title": "Data Visualization and Maps II",
    "section": "Static Maps with ggplot2",
    "text": "Static Maps with ggplot2"
  },
  {
    "objectID": "slides/26-slides.html#changing-aesthetics",
    "href": "slides/26-slides.html#changing-aesthetics",
    "title": "Data Visualization and Maps II",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics\n\np &lt;- ggplot(data=cty.info) +\n  geom_sf(mapping=aes(fill=pop), color=\"white\") +\n  scale_fill_viridis()"
  },
  {
    "objectID": "slides/26-slides.html#changing-aesthetics-1",
    "href": "slides/26-slides.html#changing-aesthetics-1",
    "title": "Data Visualization and Maps II",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics"
  },
  {
    "objectID": "slides/26-slides.html#adding-layers",
    "href": "slides/26-slides.html#adding-layers",
    "title": "Data Visualization and Maps II",
    "section": "Adding layers",
    "text": "Adding layers\n\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% \n  filter(., STUSPS %in% c(\"WA\", \"OR\", \"ID\", \"MT\", \"WY\"))\n\np &lt;- ggplot(data=cty.info) +\n  geom_sf(mapping=aes(fill=pop), color=\"white\") +\n  geom_sf(data=st, fill=NA, color=\"red\") +\n  scale_fill_viridis()"
  },
  {
    "objectID": "slides/26-slides.html#adding-layers-1",
    "href": "slides/26-slides.html#adding-layers-1",
    "title": "Data Visualization and Maps II",
    "section": "Adding layers",
    "text": "Adding layers"
  },
  {
    "objectID": "slides/26-slides.html#using-tmap",
    "href": "slides/26-slides.html#using-tmap",
    "title": "Data Visualization and Maps II",
    "section": "Using tmap",
    "text": "Using tmap\n\npt &lt;- tm_shape(cty.info) + \n  tm_polygons(col = \"pop\",\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/26-slides.html#using-tmap-1",
    "href": "slides/26-slides.html#using-tmap-1",
    "title": "Data Visualization and Maps II",
    "section": "Using tmap",
    "text": "Using tmap"
  },
  {
    "objectID": "slides/26-slides.html#changing-aesthetics-2",
    "href": "slides/26-slides.html#changing-aesthetics-2",
    "title": "Data Visualization and Maps II",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics\n\npt &lt;- tm_shape(cty.info) + \n  tm_polygons(col = \"pop\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/26-slides.html#changing-aesthetics-3",
    "href": "slides/26-slides.html#changing-aesthetics-3",
    "title": "Data Visualization and Maps II",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics"
  },
  {
    "objectID": "slides/26-slides.html#adding-layers-2",
    "href": "slides/26-slides.html#adding-layers-2",
    "title": "Data Visualization and Maps II",
    "section": "Adding layers",
    "text": "Adding layers\n\npt &lt;- tm_shape(cty.info) + \n  tm_polygons(col = \"pop\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(st) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/26-slides.html#adding-layers-3",
    "href": "slides/26-slides.html#adding-layers-3",
    "title": "Data Visualization and Maps II",
    "section": "Adding layers",
    "text": "Adding layers"
  },
  {
    "objectID": "slides/26-slides.html#themes",
    "href": "slides/26-slides.html#themes",
    "title": "Data Visualization and Maps II",
    "section": "Themes",
    "text": "Themes\n\np"
  },
  {
    "objectID": "slides/26-slides.html#themes-1",
    "href": "slides/26-slides.html#themes-1",
    "title": "Data Visualization and Maps II",
    "section": "Themes",
    "text": "Themes\n\np + \n  theme_void()"
  },
  {
    "objectID": "slides/26-slides.html#themes-2",
    "href": "slides/26-slides.html#themes-2",
    "title": "Data Visualization and Maps II",
    "section": "Themes",
    "text": "Themes\n\np + \n  theme_void() +\n  theme(panel.background = element_rect(fill=\"white\", color=\"black\"))"
  },
  {
    "objectID": "slides/26-slides.html#labels",
    "href": "slides/26-slides.html#labels",
    "title": "Data Visualization and Maps II",
    "section": "Labels",
    "text": "Labels\n\np +\n  labs(title = \"County Populations\")"
  },
  {
    "objectID": "slides/26-slides.html#legend-customization",
    "href": "slides/26-slides.html#legend-customization",
    "title": "Data Visualization and Maps II",
    "section": "Legend customization",
    "text": "Legend customization\n\nggplot(data=cty.info) +\n  geom_sf(mapping=aes(fill=pop), color=\"white\") +\n  geom_sf(data=st, fill=NA, color=\"red\") +\n  scale_fill_viridis(name = \"Population\\n(millions)\",\n                     breaks = seq(from=0, to=3e6, by=1e6),\n                     labels = seq(from=0, to=3e6, by=1e6)/1e6)"
  },
  {
    "objectID": "slides/26-slides.html#legend-customization-1",
    "href": "slides/26-slides.html#legend-customization-1",
    "title": "Data Visualization and Maps II",
    "section": "Legend customization",
    "text": "Legend customization\n\nggplot(data=cty.info) +\n  geom_sf(mapping=aes(fill=pop), color=\"white\") +\n  geom_sf(data=st, fill=NA, color=\"red\") +\n  scale_fill_viridis(name = \"Population\\n(millions)\",\n                     limits = c(0, 3e6),\n                     breaks = seq(from=0, to=3e6, by=1e6),\n                     labels = seq(from=0, to=3e6, by=1e6)/1e6)"
  },
  {
    "objectID": "slides/26-slides.html#convert-raster-to-tidy-format",
    "href": "slides/26-slides.html#convert-raster-to-tidy-format",
    "title": "Data Visualization and Maps II",
    "section": "Convert raster to tidy format",
    "text": "Convert raster to tidy format\nTo visualize rasters in ggplot2, they need to be converted to a dataframe.\n\nfire.haz &lt;- rast(\"/opt/data/data/assignment01/wildfire_hazard_agg.tif\")\n\nfire.haz_df &lt;- as.data.frame(fire.haz)\n\nstr(fire.haz_df)\n\n\n\n'data.frame':   11152346 obs. of  3 variables:\n $ x     : num  -1979745 -1979505 -1979265 -1979745 -1979505 ...\n $ y     : num  3172215 3172215 3172215 3171975 3171975 ...\n $ WHP_ID: num  3.5 3.48 2.95 4.97 3.47 ..."
  },
  {
    "objectID": "slides/26-slides.html#use-geom_raster",
    "href": "slides/26-slides.html#use-geom_raster",
    "title": "Data Visualization and Maps II",
    "section": "Use geom_raster",
    "text": "Use geom_raster\n\nggplot(data = fire.haz_df, aes(x=x, y=y, fill = WHP_ID)) +\n  geom_raster()"
  },
  {
    "objectID": "slides/26-slides.html#coordinate-system",
    "href": "slides/26-slides.html#coordinate-system",
    "title": "Data Visualization and Maps II",
    "section": "Coordinate system",
    "text": "Coordinate system\n\nggplot(data = fire.haz_df, aes(x=x, y=y, fill = WHP_ID)) +\n  geom_raster() +\n  coord_sf(default_crs = crs(fire.haz))"
  },
  {
    "objectID": "slides/26-slides.html#layering-rasters-and-vectors",
    "href": "slides/26-slides.html#layering-rasters-and-vectors",
    "title": "Data Visualization and Maps II",
    "section": "Layering rasters and vectors",
    "text": "Layering rasters and vectors\n\n# get data\nstates &lt;- tigris::states(progress_bar=FALSE) %&gt;% \n  filter(., STUSPS %in% c(\"WA\", \"OR\", \"ID\")) %&gt;%\n  st_transform(crs = st_crs(fire.haz))\n\n# method 1\n# first dataset in ggplot() and use inherit.aes=FALSE\np2 &lt;- ggplot(data = fire.haz_df, aes(x=x, y=y, fill = WHP_ID)) +\n  geom_raster() +\n  geom_sf(data = states, fill=NA, color=\"red\", inherit.aes = FALSE) +\n  coord_sf(default_crs = crs(fire.haz))\n\n# method 2\n# first dataset in geom so inherit.aes isn't necessary\np3 &lt;- ggplot() +\n  geom_raster(data = fire.haz_df, aes(x=x, y=y, fill = WHP_ID)) +\n  geom_sf(data = states, fill=NA, color=\"red\") +\n  coord_sf(default_crs = crs(fire.haz))"
  },
  {
    "objectID": "slides/26-slides.html#layering-rasters-and-vectors-1",
    "href": "slides/26-slides.html#layering-rasters-and-vectors-1",
    "title": "Data Visualization and Maps II",
    "section": "Layering rasters and vectors",
    "text": "Layering rasters and vectors"
  },
  {
    "objectID": "slides/26-slides.html#subplots",
    "href": "slides/26-slides.html#subplots",
    "title": "Data Visualization and Maps II",
    "section": "Subplots",
    "text": "Subplots\n\np2+p3"
  },
  {
    "objectID": "slides/26-slides.html#combine-legends",
    "href": "slides/26-slides.html#combine-legends",
    "title": "Data Visualization and Maps II",
    "section": "Combine legends",
    "text": "Combine legends\n\np2+p3 + plot_layout(guides=\"collect\")"
  },
  {
    "objectID": "slides/26-slides.html#change-layout",
    "href": "slides/26-slides.html#change-layout",
    "title": "Data Visualization and Maps II",
    "section": "Change layout",
    "text": "Change layout\n\np+p3 + plot_layout(nrow=2)"
  },
  {
    "objectID": "slides/26-slides.html#map-insets",
    "href": "slides/26-slides.html#map-insets",
    "title": "Data Visualization and Maps II",
    "section": "Map insets",
    "text": "Map insets\n\n# get data (filter to continuous US)\nconus &lt;- tigris::states() %&gt;%\n  filter(!(DIVISION == 0 | STUSPS %in% c(\"HI\", \"AK\"))) %&gt;%\n  st_transform(st_crs(fire.haz))\n\n# create inset map\ninset &lt;- ggplot(data = conus) +\n  geom_sf(fill=\"white\") +\n  # fill study area states by filtering to those states and setting color parameters\n  geom_sf(data = filter(conus, STUSPS %in% c(\"ID\", \"OR\", \"WA\")),\n          fill = \"gray70\", color=\"red\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill=\"white\", color=\"black\")) +\n  coord_sf()"
  },
  {
    "objectID": "slides/26-slides.html#map-insets-1",
    "href": "slides/26-slides.html#map-insets-1",
    "title": "Data Visualization and Maps II",
    "section": "Map insets",
    "text": "Map insets\n\nlayout &lt;- c(\n  patchwork::area(t = 1, l = 1, b = 5, r = 4),\n  patchwork::area(t = 1, l = 4, b = 2, r = 5),\n  patchwork::area(t = 3, l = 4, b = 5, r = 5)\n)\n\np2 + inset + guide_area() + plot_layout(design=layout, guides='collect')"
  }
]